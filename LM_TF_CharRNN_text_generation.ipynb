{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline - tutorial\n",
        "https://www.tensorflow.org/text/tutorials/text_generation"
      ],
      "metadata": {
        "id": "C9vni-CO1OxE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GCCk8_dHpuNf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcD2nPQvPOFM"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_generation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "Note: Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware accelerator > GPU*.\n",
        "\n",
        "This tutorial includes runnable code implemented using [tf.keras](https://www.tensorflow.org/guide/keras/sequential_model) and [eager execution](https://www.tensorflow.org/guide/eager). The following is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt \"Q\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcygKkEVZBaa"
      },
      "source": [
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGsCP9DZFQ5"
      },
      "source": [
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the Shakespeare dataset\n",
        "\n",
        "Change the following line to run this code on your own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD_55cOxLkAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a707d0d2-6bad-4296-920b-43511800bc7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d079ee3c-791e-401a-bf60-9775a2ca352f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b9a345-5e77-41bc-9126-ce599249f014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4529442e-d3bb-4155-8d21-c393ddc52d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiSiQBa-yJju",
        "outputId": "f3262e65-1315-469a-be39-ee1161e6d618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation. \n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a296353-75de-403f-a3f6-01da5fdc26f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "Now create the `tf.keras.layers.StringLookup` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "It converts from tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18685735-3cda-42b4-c1fa-01f48ce526e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `tf.keras.layers.StringLookup(..., invert=True)`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd699b9-720f-475b-8961-e3ec8b9c6a9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "You can `tf.strings.reduce_join` to join the characters back into strings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba490a71-60ad-4c35-b164-69fc5775fda7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3481ea-f65b-4fce-bb9a-d3ce3ce5dd94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjH5v45-yqqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0f19c3-2b9a-46c9-fe9c-d5bdd0d617ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51597428-c201-416c-c7ba-c4f129ed0198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO32cMWu4a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31455cb4-ba35-450f-80de-c8753b5a5352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970ec810-392e-475e-f7f3-19b8116718bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f273aacf-e9f0-4eb1-94c4-001a045c43cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fca5c70-5d88-4999-d0c8-446877c1b38e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHT8cLh7EAsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3cc7176-17ac-4e5e-bcf3-a07e38a4f493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 66\n"
          ]
        }
      ],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "print('vocab_size:', vocab_size)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKbfm04amhXk"
      },
      "source": [
        "Note: For training you could use a `keras.Sequential` model here. To  generate text later you'll need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7371a271-25b1-404e-e87d-a8979d4d8584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47e0279-4467-4bc8-f0cc-128c3f57cc8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45f90ea-f1ee-419b-c713-52ce0dbacff3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([25, 62, 22,  9, 14, 38, 38, 22,  1, 14, 49, 14, 60, 53, 56, 61, 50,\n",
              "       49, 42, 59, 27, 30, 49, 57, 21, 42, 10, 33,  3, 50, 16,  5, 46, 60,\n",
              "        2, 52, 18, 50, 10, 24, 16, 36, 38, 31, 29, 61, 22, 47, 11, 57, 49,\n",
              "       41, 61, 54, 30, 43, 17, 32, 60, 21, 24, 45,  1, 12, 33, 61, 51, 63,\n",
              "       39,  0,  6, 59, 65, 21, 50, 57,  8,  7, 46, 42, 32,  9, 31, 30, 51,\n",
              "       65, 28, 21, 36, 24, 64, 51, 50,  1, 40, 55, 23, 10, 19, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb7cfe9-d42c-45ec-c5e1-2368a20457ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"d world's exile is death: then banished,\\nIs death mis-term'd: calling death banishment,\\nThou cutt'st\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"LwI.AYYI\\nAjAunqvkjctNQjrHc3T!kC&gu mEk3KCWYRPvIh:rjbvoQdDSuHKf\\n;TvlxZ[UNK]'tzHkr-,gcS.RQlzOHWKylk\\napJ3FN\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a719eb-4b35-4007-c44e-3c3846125d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1893163, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Init Perplexity"
      ],
      "metadata": {
        "id": "D5PWKHS6CF96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53939d66-b67d-4227-d363-a474e82034cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.97766"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()\n",
        "# this is Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc56f5b-5603-4426-cff3-43a5714450fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 2.7004\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.9821\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.7045\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.5434\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.4459\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.3782\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.3269\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2819\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2413\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.2009\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1606\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1192\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0763\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0292\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9795\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.9283\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 59ms/step - loss: 0.8746\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8221\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7721\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7225\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681d079e-f841-4b8b-f8ea-9ef8008b826f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Doly, calm Peter upon the rest:\n",
            "And so, we'll follow, this is too helph again\n",
            "In some monamarisoms as my father's bounds.\n",
            "It,--dayour form reqoires; for Lord Antil\n",
            "That she's your wife, think nothing but obey:\n",
            "the duke would have my headds the end of manys of waxes,\n",
            "Like a lord: but yet had not desert it,\n",
            "And stand upon me. How never made him\n",
            "ere I already, my master's heir?\n",
            "\n",
            "KING EDWARD IV:\n",
            "Then suffer now straw accusations and that thou\n",
            "shoulkst shift and shrick-place, dirst myself will keep you woo'd;\n",
            "The Letters of the deposion's cherusio's threats,\n",
            "To chide awhile to neither reach the pail;\n",
            "For even is of labour we accused with her by-fouldest,\n",
            "'Fore the reason that will rain and play on,\n",
            "But only things fitting on her life; let it be put\n",
            "To stand in wakery now; the way\n",
            "betreed.\n",
            "\n",
            "Provost:\n",
            "God save your life! take the sepulchre?\n",
            "\n",
            "POLIXENES:\n",
            "A Edgur charge him for her protector\n",
            "Of strengthens, and supposed tongue both grief,\n",
            "The nobles for the rest,\n",
            "As Grumio's fled innocent and si \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.128346681594849\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkLu7Y8UCMT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5af53b8-cec6-49e8-fa1e-01e35f4de8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThe best of is not best.\\nI'll think what hand doth send the way\\nTo think about his hate; and have it us,\\nSides you your soldier, thus never honour'd our own;\\nAnd, to be smiled and joy out, visits death,\\n'Romeo me, nor no man I did:\\nAnother becomes a crack. You should, good forence;\\nHow could he stands for the fialeman? the subject strait musician!\\nO, no! for such a due! and all the world\\nreproud you! where is Charbey?' 'Myost termness,\\nI have a soul a very rating nece\\nIn drops than my report: but, for,\\nHave counterfeit dog that hath begun to me well, poor lord,\\nTwo in the swallow'd gates for the jest shall\\nneed. Fare you me drught, and do thank those when\\nI love his hate, and leave the son.\\n\\nDUKE OF AUMERLE:\\nPray now, sweet lords, Leave me, and left with thee.\\n\\nLord:\\nExetience, noble Corioli, curse\\nAnd swiftly, underneath the room:\\nIn humble arms in all allethy hour,\\nFrom wine change prepared for hurtiert equally;\\nNo speech be flattered to the Antiapise,\\nTo occupy the people did not b\"\n",
            " b\"ROMEO:\\nBy sickly we hear us done.\\n\\nKING EDWARD IV:\\nBrothers, a visor much in your owf\\nThe fruil of windest sick in comfort\\nThat I should sing.\\n\\nHENRY BOLINGBROKE:\\nOne being, and Edward's skilling violators,\\nA listing panish he shall prove a true and prince:\\nLet us to vive in mine honey food,\\nWhose colding buckle and extire my back\\nWould buy the old man broke from sour and most\\nI beried you with it: it is not him;\\nAnd till we confine the ground, and the nurse cries out\\nWhose heir of that with woman.\\n\\nVOLUMNIA:\\nNever burns of increasion:\\nHold, why, he's dead to seek you all;\\nAnd that my present baseness through immetting a\\nvery glory in the hope reverse earth as this.\\n\\nLADY CAPULET:\\nMarry, but all to look on!\\n\\nCAMILLO:\\nHe lies,\\nDid not my tent to one it may,\\nOr if not fit, and heaven and tears they do die to-day;\\nWhich knows you why sain is fair deliver'd,\\nOr else you will make a parties Romeo, to make him\\nand\\ngiven us with anread heppos' to such show:\\nWithin the poor cursed endloss whose pers\"\n",
            " b\"ROMEO:\\nI shall, my lord.\\n\\nFLORIZEL:\\nI not amend my sons and princely garden:\\nmusic assurance and my intelligence,\\nNor to his nature, which they do lie. I am no very hat\\nAnd Warwick's nothing eremity on ere they are\\nlecherous suffer'd.\\n\\nMERCUTIO:\\nWhy, so his gift slaughter--to fled his partness will we\\nVile, and every chamber-moody; blows was thrown,\\nThe longest tesemory.\\n\\nANGELO:\\nThe lying so night' quick Peteer'd, and I will ha't-sadute,\\nEven as such as I, pale amazing citizen,\\nDivine being queen, to make my tron\\ncrabbed Richard; we wiph their steads upon the less;\\nWhere he shall proceed: and whether they did make\\nWhat case that had before mine enemy Coriolanus.\\nBesides, sweet master him ere justice!\\n\\nDUKE VINCENTIO:\\nHis nature is, in love's spiners.'\\n\\nPitisphers:\\nCall hither to grieve condemned with an unspeak'st man;\\nAnd by the rebels which lay aside his scruple,\\nComproble and undertake to me:\\nAnd, gentle corfes, except the midst\\nof your eyes amen. ELIZABETH:\\nThe lords and now.\\n\\nCOMINIUS:\"\n",
            " b\"ROMEO:\\nThen, for the golden prince both, and quite away,\\nRegain these skins all neglect than mad\\nquenching her letters. Hang 't doth;\\n'Tis meet upon myself, as I bife for\\nsink is not well I liven'd and not an inch, I am\\npour'd to a neat-have needful, nurse; I'll to me.\\n\\nMENENIUS:\\nOn Thumberland, sir! here, sir!\\n\\nPOMPEY:\\nOnce, purget him, for 'tis as his lent here.\\n\\nLord:\\nO monstrous law, a recrown'd blood\\nIs three chance to mend her temptless fire!\\nThou drowns that ne'er then haply prove a sat by false.\\n\\nWARWICK:\\nWhy, therefore then?\\nOn what it true!--\\nHow! Grumio! God save Kine Henry! and Paris?\\n\\nTRANIO:\\nBut to her love about it. I come;\\nIf I must not; I live with Viennant:\\nRichmond, farewell, our neighbours of an unstance,\\nTears then enough for some hour instinctingly.\\n\\nNurse:\\nWhat would you have my pain's body\\nAnd better prove either favourry.\\n\\nPETRUCHIO:\\nThe more better honour.\\n\\nANGELO:\\nDear bloody!\\n\\nAUFIDIUS:\\nAy, it\\nmads, we'll so your most virtuous.\\n\\nMARCIUS:\\nSir, the sun seems well as\"\n",
            " b\"ROMEO:\\nWho, light us not, night, we'll attain our heads and thee,\\nAnd give away my slaughter and my fleshman.\\nCome on, brother, drunk, I pray.\\n\\nSecond Wapper? And then must I contemp\\nThan the strict oracle and external person\\nMay overbo thee, his face thoughts,\\nThat thou respect'st our soldiers use to pass,\\nBoyally, this can all force and thrust himself\\nBlocks are drinking it you: then curts himself,\\nAnd did you not receive their subsuains, whispering and\\nto the wantongs would be plain, I have ta'en you name,\\nSuch is of all the wisest haste;\\nAnd, if thou mean a penuriel than in that\\nselmed very gracious the furnising his exile:\\nIf I be noble and such as anlewless. Is he would\\nMake us our betters in the field.\\n\\nKING EDWARD IA:\\nRichard, in any cause! Come hither, here are nexed.\\n\\nGLOUCESTER:\\nTowards Cherbst? what of that?\\n\\nCOMINIUS:\\nEven so?\\n\\nSecond Capulet;\\nPrepare you, sir, her conscience she pletses;\\nAnd here remains marriage, rud marved-moning sound,\\nAnd, with purpose, him than thou art: I\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.106008768081665\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Export the generator\n",
        "\n",
        "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Grk32H_CzsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e852add-1163-41a7-a6c0-2b9f2fe5c4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f1fd068d550>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f34f40c-49e8-4281-b843-ee0059203ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Are there not Richard; here comes the air,\n",
            "Had take the field and sold himself will go rin\n",
            "Beholdin\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but does not give you much control.\n",
        "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement _curriculum  learning_ to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and apply them to the model using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0pZ101hjwW0"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oc-eJALcK8B"
      },
      "source": [
        "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows you to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKyWiZ_Lj7w5"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U817KUm7knlm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o694aoBPnEi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fb0a46-4098-45c1-ba5d-f228f89a1c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 12s 52ms/step - loss: 2.7252\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1f58651dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8nAtKHVoInR"
      },
      "source": [
        "Or if you need more control, you can write your own complete custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4tSNwymzf-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ea51e8-8864-45c4-9855-44cf47a84112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1906\n",
            "Epoch 1 Batch 50 Loss 2.0683\n",
            "Epoch 1 Batch 100 Loss 1.9509\n",
            "Epoch 1 Batch 150 Loss 1.8527\n",
            "\n",
            "Epoch 1 Loss: 1.9905\n",
            "Time taken for 1 epoch 10.51 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8174\n",
            "Epoch 2 Batch 50 Loss 1.7344\n",
            "Epoch 2 Batch 100 Loss 1.7154\n",
            "Epoch 2 Batch 150 Loss 1.6844\n",
            "\n",
            "Epoch 2 Loss: 1.7090\n",
            "Time taken for 1 epoch 9.88 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5668\n",
            "Epoch 3 Batch 50 Loss 1.5477\n",
            "Epoch 3 Batch 100 Loss 1.5293\n",
            "Epoch 3 Batch 150 Loss 1.5203\n",
            "\n",
            "Epoch 3 Loss: 1.5459\n",
            "Time taken for 1 epoch 10.05 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4497\n",
            "Epoch 4 Batch 50 Loss 1.4571\n",
            "Epoch 4 Batch 100 Loss 1.3938\n",
            "Epoch 4 Batch 150 Loss 1.5038\n",
            "\n",
            "Epoch 4 Loss: 1.4470\n",
            "Time taken for 1 epoch 10.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3819\n",
            "Epoch 5 Batch 50 Loss 1.4153\n",
            "Epoch 5 Batch 100 Loss 1.3199\n",
            "Epoch 5 Batch 150 Loss 1.3404\n",
            "\n",
            "Epoch 5 Loss: 1.3790\n",
            "Time taken for 1 epoch 10.34 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.2889\n",
            "Epoch 6 Batch 50 Loss 1.3085\n",
            "Epoch 6 Batch 100 Loss 1.2966\n",
            "Epoch 6 Batch 150 Loss 1.3579\n",
            "\n",
            "Epoch 6 Loss: 1.3278\n",
            "Time taken for 1 epoch 10.23 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2487\n",
            "Epoch 7 Batch 50 Loss 1.2615\n",
            "Epoch 7 Batch 100 Loss 1.2471\n",
            "Epoch 7 Batch 150 Loss 1.2456\n",
            "\n",
            "Epoch 7 Loss: 1.2833\n",
            "Time taken for 1 epoch 10.22 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2263\n",
            "Epoch 8 Batch 50 Loss 1.2272\n",
            "Epoch 8 Batch 100 Loss 1.2266\n",
            "Epoch 8 Batch 150 Loss 1.2376\n",
            "\n",
            "Epoch 8 Loss: 1.2417\n",
            "Time taken for 1 epoch 10.31 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1769\n",
            "Epoch 9 Batch 50 Loss 1.1658\n",
            "Epoch 9 Batch 100 Loss 1.1917\n",
            "Epoch 9 Batch 150 Loss 1.1895\n",
            "\n",
            "Epoch 9 Loss: 1.2031\n",
            "Time taken for 1 epoch 10.39 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1680\n",
            "Epoch 10 Batch 50 Loss 1.1532\n",
            "Epoch 10 Batch 100 Loss 1.1562\n",
            "Epoch 10 Batch 150 Loss 1.1812\n",
            "\n",
            "Epoch 10 Loss: 1.1633\n",
            "Time taken for 1 epoch 10.74 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f} Perplexity: {tf.exp(mean.result().numpy()}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2 - Char RNN LM with medium-data"
      ],
      "metadata": {
        "id": "GpjAW70Q4-fv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Setup & Import**"
      ],
      "metadata": {
        "id": "gCy2D22w5HQq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lvT8FSx5eLo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Prepare Data**"
      ],
      "metadata": {
        "id": "IMorxeqKBtKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data"
      ],
      "metadata": {
        "id": "PmR2Cq5ROgX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "data_link = \"https://drive.google.com/file/d/1vQbu1XmenVtNYSvqZmiwAo2jzFWZCsI8/view?usp=sharing\"\n",
        "file_id= data_link.split('/')[-2]\n",
        "fn = 'medium_data.csv'\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=''$file_id' -O '$fn'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f681a09-7ee5-47ba-9659-e51d7d258b2b",
        "id": "ieLAx_WUOgX0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2022-06-09 14:37:42--  https://docs.google.com/uc?export=download&id=1vQbu1XmenVtNYSvqZmiwAo2jzFWZCsI8\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.45.14, 2607:f8b0:4004:83e::200e\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.45.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-14-b0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p832sgv87l8i4t94ucvfgrucv0scgre7/1654785450000/17304979608001708681/*/1vQbu1XmenVtNYSvqZmiwAo2jzFWZCsI8?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-06-09 14:37:42--  https://doc-14-b0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p832sgv87l8i4t94ucvfgrucv0scgre7/1654785450000/17304979608001708681/*/1vQbu1XmenVtNYSvqZmiwAo2jzFWZCsI8?e=download\n",
            "Resolving doc-14-b0-docs.googleusercontent.com (doc-14-b0-docs.googleusercontent.com)... 172.253.115.132, 2607:f8b0:4004:c06::84\n",
            "Connecting to doc-14-b0-docs.googleusercontent.com (doc-14-b0-docs.googleusercontent.com)|172.253.115.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1428382 (1.4M) [text/csv]\n",
            "Saving to: ‘medium_data.csv’\n",
            "\n",
            "medium_data.csv     100%[===================>]   1.36M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-06-09 14:37:43 (107 MB/s) - ‘medium_data.csv’ saved [1428382/1428382]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#medium_data = pd.read_csv('../input/medium-articles-dataset/medium_data.csv') # Kaggle\n",
        "medium_data = pd.read_csv('medium_data.csv') # Colab\n",
        "medium_data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-31T14:50:49.294905Z",
          "iopub.execute_input": "2022-05-31T14:50:49.295657Z",
          "iopub.status.idle": "2022-05-31T14:50:49.349944Z",
          "shell.execute_reply.started": "2022-05-31T14:50:49.295603Z",
          "shell.execute_reply": "2022-05-31T14:50:49.348937Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "4e0c25a8-f5e4-42dd-b2f8-cf7788a7ed98",
        "id": "ZlYbLrrTOgX0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                                url  \\\n",
              "0   1  https://towardsdatascience.com/a-beginners-gui...   \n",
              "1   2  https://towardsdatascience.com/hands-on-graph-...   \n",
              "2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n",
              "3   4  https://towardsdatascience.com/databricks-how-...   \n",
              "4   5  https://towardsdatascience.com/a-step-by-step-...   \n",
              "\n",
              "                                               title  \\\n",
              "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
              "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
              "2                       How to Use ggplot2 in Python   \n",
              "3  Databricks: How to Save Files in CSV on Your L...   \n",
              "4  A Step-by-Step Implementation of Gradient Desc...   \n",
              "\n",
              "                                  subtitle   image  claps responses  \\\n",
              "0                                      NaN   1.png    850         8   \n",
              "1                                      NaN   2.png   1100        11   \n",
              "2         A Grammar of Graphics for Python   3.png    767         1   \n",
              "3  When I work on Python projects dealing…  4.jpeg    354         0   \n",
              "4          One example of building neural…  5.jpeg    211         3   \n",
              "\n",
              "   reading_time           publication        date  \n",
              "0             8  Towards Data Science  2019-05-30  \n",
              "1             9  Towards Data Science  2019-05-30  \n",
              "2             5  Towards Data Science  2019-05-30  \n",
              "3             4  Towards Data Science  2019-05-30  \n",
              "4             4  Towards Data Science  2019-05-30  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f71dac8c-8898-4593-99e7-9585e489614d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>subtitle</th>\n",
              "      <th>image</th>\n",
              "      <th>claps</th>\n",
              "      <th>responses</th>\n",
              "      <th>reading_time</th>\n",
              "      <th>publication</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
              "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.png</td>\n",
              "      <td>850</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>Towards Data Science</td>\n",
              "      <td>2019-05-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
              "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.png</td>\n",
              "      <td>1100</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>Towards Data Science</td>\n",
              "      <td>2019-05-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
              "      <td>How to Use ggplot2 in Python</td>\n",
              "      <td>A Grammar of Graphics for Python</td>\n",
              "      <td>3.png</td>\n",
              "      <td>767</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Towards Data Science</td>\n",
              "      <td>2019-05-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
              "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
              "      <td>When I work on Python projects dealing…</td>\n",
              "      <td>4.jpeg</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>Towards Data Science</td>\n",
              "      <td>2019-05-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n",
              "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
              "      <td>One example of building neural…</td>\n",
              "      <td>5.jpeg</td>\n",
              "      <td>211</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>Towards Data Science</td>\n",
              "      <td>2019-05-30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f71dac8c-8898-4593-99e7-9585e489614d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f71dac8c-8898-4593-99e7-9585e489614d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f71dac8c-8898-4593-99e7-9585e489614d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"remv\"></a>\n",
        "#### Preprocess Data\n",
        "Removing unwanted characters and words in titles\n",
        "\n",
        "Looking at titles, we can see there are some of unwanted characters and words in it which can not be useful for us to predict infact it might decrease our model accuracy so we have to remove it."
      ],
      "metadata": {
        "id": "HKMg1EZsOgX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocess(line):\n",
        "    # line: string\n",
        "    preprocess_line = line.lower()\n",
        "    preprocess_line = preprocess_line.replace(u'\\xa0',u' ')\n",
        "    preprocess_line = preprocess_line.replace('\\u200a',' ')\n",
        "    preprocess_line = re.sub(\"[^A-Za-zА-Яа-я0-9\\'\\s]\",'',preprocess_line)\n",
        "    return preprocess_line\n",
        "text_preprocess(\"Hands-on Graph Neural 'Network's with PyTorch & .\")    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-OBH7lN2Et4c",
        "outputId": "76d2a8a4-f6f8-49c0-ea21-5c5506847f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"handson graph neural 'network's with pytorch  \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "medium_data['title'] = medium_data['title'].apply(lambda x: text_preprocess(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-31T14:51:04.377174Z",
          "iopub.execute_input": "2022-05-31T14:51:04.377769Z",
          "iopub.status.idle": "2022-05-31T14:51:04.38892Z",
          "shell.execute_reply.started": "2022-05-31T14:51:04.377717Z",
          "shell.execute_reply": "2022-05-31T14:51:04.387839Z"
        },
        "trusted": true,
        "id": "lWmGjq3sOgX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medium_data['title'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-Q_uUetGyMv",
        "outputId": "48d0e8e4-93aa-41a8-ea68-f61d19dd6464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    a beginners guide to word embedding with gensi...\n",
              "1    handson graph neural networks with pytorch  py...\n",
              "2                         how to use ggplot2 in python\n",
              "3    databricks how to save files in csv on your lo...\n",
              "4    a stepbystep implementation of gradient descen...\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "medium_data['title'].to_csv('medium_data.txt', index=False, header=None )"
      ],
      "metadata": {
        "id": "hS8uenWEOgX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlBDLYb86cx7"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f256be3f-eadf-46ba-8f5a-222c2c473abe",
        "id": "Pn_5dFun6cx7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 324742 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "path_to_file = 'medium_data.txt'\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5d088f-43a3-474a-e21b-22f41634bebb",
        "id": "7xlqVXCG6cx8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a beginners guide to word embedding with gensim word2vec model\n",
            "handson graph neural networks with pytorch  pytorch geometric\n",
            "how to use ggplot2 in python\n",
            "databricks how to save files in csv on your local computer\n",
            "a stepbystep implementation of gradie\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a70300-9adc-4247-b55c-b2eea544b310",
        "id": "FUvizK5C6cx9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38881550-af8e-4fb7-8df6-13e00952dbea",
        "id": "9vUb7RPc6cx9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', \"'\", '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPc5g-1g6cx9"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation. \n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b46792-f900-4beb-9612-38bc21d097b0",
        "id": "eEWQCh_26cx-"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6vraweA6cx-"
      },
      "source": [
        "Now create the `tf.keras.layers.StringLookup` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59sI6Vql6cx-"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzVLJaPL6cx-"
      },
      "source": [
        "It converts from tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d5e629-3fda-403d-c0d0-40ed97cc8aec",
        "id": "cas7Zt-Y6cx-"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[14, 15, 16, 17, 18, 19, 20], [37, 38, 39]]>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COw1Dyln6cx-"
      },
      "source": [
        "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtLWWnEg6cx_"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TLZOFoe6cx_"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0cbd3b-709b-4b39-ff61-2854abe97a2e",
        "id": "k5mJDlsK6cx_"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vwY_Nvv6cx_"
      },
      "source": [
        "You can `tf.strings.reduce_join` to join the characters back into strings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb9fd27-0d4a-4958-9fd2-f78ac39d98e1",
        "id": "vhShPZ9F6cx_"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv-rwk-B6cx_"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhrMJRUs7DxU"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic7CaJgU7DxU"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi45DYah7DxU"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc763dc7-eacb-4156-a4b9-fb1deff60379",
        "id": "6HxcNIRq7DxV"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(324742,), dtype=int64, numpy=array([14,  2, 15, ..., 32, 33,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hmT3U5D7DxV"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5a8a24-8c43-41ce-a064-a3713e954c52",
        "id": "rLOj4d2R7DxV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n",
            " \n",
            "b\n",
            "e\n",
            "g\n",
            "i\n",
            "n\n",
            "n\n",
            "e\n",
            "r\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrdMvOmA7DxV"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsTyiuHw7DxV"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd86c91-2eec-4400-9c91-3f28051dd2d8",
        "id": "3T-VjWwG7DxW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'a' b' ' b'b' b'e' b'g' b'i' b'n' b'n' b'e' b'r' b's' b' ' b'g' b'u'\n",
            " b'i' b'd' b'e' b' ' b't' b'o' b' ' b'w' b'o' b'r' b'd' b' ' b'e' b'm'\n",
            " b'b' b'e' b'd' b'd' b'i' b'n' b'g' b' ' b'w' b'i' b't' b'h' b' ' b'g'\n",
            " b'e' b'n' b's' b'i' b'm' b' ' b'w' b'o' b'r' b'd' b'2' b'v' b'e' b'c'\n",
            " b' ' b'm' b'o' b'd' b'e' b'l' b'\\n' b'h' b'a' b'n' b'd' b's' b'o' b'n'\n",
            " b' ' b'g' b'r' b'a' b'p' b'h' b' ' b'n' b'e' b'u' b'r' b'a' b'l' b' '\n",
            " b'n' b'e' b't' b'w' b'o' b'r' b'k' b's' b' ' b'w' b'i' b't' b'h' b' '\n",
            " b'p' b'y' b't'], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9_xsP_17DxW"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5e70cb-a5d7-4b31-8f19-dad23d8e34d3",
        "id": "ag1cE9Qg7DxW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'a beginners guide to word embedding with gensim word2vec model\\nhandson graph neural networks with pyt'\n",
            "b'orch  pytorch geometric\\nhow to use ggplot2 in python\\ndatabricks how to save files in csv on your loca'\n",
            "b'l computer\\na stepbystep implementation of gradient descent and backpropagation\\nan easy introduction t'\n",
            "b'o sql for data scientists\\nhypothesis testing visualized\\nintroduction to latent matrix factorization r'\n",
            "b'ecommender systems\\nwhich 2020 candidate is the best at twitter\\nwhat if ai model understanding were ea'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wElR6X3u7DxW"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_h46xqF7DxW"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26b91d4-7392-4205-f548-f478a3d0c26c",
        "id": "HFy-GPg77DxW"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UpzUKvj7DxW"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ecdde3-3f5a-47e5-e64d-572ce8a7bef3",
        "id": "mzgZbnve7DxX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'a beginners guide to word embedding with gensim word2vec model\\nhandson graph neural networks with py'\n",
            "Target: b' beginners guide to word embedding with gensim word2vec model\\nhandson graph neural networks with pyt'\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sifARRnl7DxX"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8281d5f-a69b-49e8-d9fe-99ccde032c4b",
        "id": "DS_UqBQY7DxX"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0lQEz4u7DxX"
      },
      "source": [
        "## Step 3: Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V92aw10Z7DxX"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8410595a-bd02-4f02-fac1-d77af9f53ea0",
        "id": "iAr1zby_7DxX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 40\n"
          ]
        }
      ],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "print('vocab_size:', vocab_size)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fbPQv0Q7DxY"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIDMKKna7DxY"
      },
      "outputs": [],
      "source": [
        "model_2 = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwY_m-lx7DxY"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3oVRhNr7DxY"
      },
      "source": [
        "Note: For training you could use a `keras.Sequential` model here. To  generate text later you'll need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG5hwNLO7DxZ"
      },
      "source": [
        "### Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90641e24-221a-4d0b-b46a-2c189c5f4bb3",
        "id": "NJ_pRGdE7DxZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 40) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model_2(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQRzJ4RX7DxZ"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3bc67d9-be91-4784-8166-3710eabeb14c",
        "id": "2bTfoj0g7DxZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  10240     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  41000     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,989,544\n",
            "Trainable params: 3,989,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcbPtI_L7DxZ"
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apdo3yZR7DxZ"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgOJmdjw7Dxa"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717b8f63-25f5-4f31-b291-e571e938d9e0",
        "id": "Fi4iPSkc7Dxa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([29,  6, 20, 16, 35, 37, 10, 22, 34, 26, 26, 31,  5,  7, 22,  9, 32,\n",
              "       14, 18,  9, 22, 27,  3,  8, 20, 33, 33, 31, 10, 36,  5,  4,  5, 28,\n",
              "       10,  5, 32,  6, 12, 14, 36, 36, 25,  3,  0, 19, 17, 12, 18, 33, 36,\n",
              "       32, 34,  2,  7,  5,  7, 37, 15, 20, 16,  7, 18, 13, 30, 22,  4,  9,\n",
              "       39, 16, 22, 23, 37,  4,  8, 39, 34, 10,  7, 38,  6, 39, 25,  6, 28,\n",
              "       18,  3,  6, 17, 34,  3,  3, 26, 11, 23, 32, 31,  1, 37, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-_hnmm87Dxa"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae744d1c-38ab-4ef3-cc49-4d8e1dbb639a",
        "id": "bSBve9JT7Dxa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'krcnn\\npractical graph neural networks for molecular machine learning\\nthe little known ogrid function'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"p2gcvx6iummr13i5sae5in'4gttr6w101o61s28awwl'[UNK]fd8etwsu 313xbgc3e9qi05zcijx04zu63y2zl2oe'2du''m7jsr\\nxf\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWRHWman7Dxa"
      },
      "source": [
        "## Step 4: Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRVY7yE67Dxa"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "due5cwq37Dxa"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPTzUQgm7Dxb"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JMkBM_w7Dxb"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b124f2-9b84-48cc-ddc0-3e21f910130e",
        "id": "J9nWm3r57Dxb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 40)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(3.6884894, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnIgpf-x7Dxb"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac0aaf77-fb3a-49d2-a20c-d4af686fed6d",
        "id": "Ehg4eKlM7Dxb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39.9844"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()\n",
        "# this is Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-YhdCyN7Dxb"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObX0KNu27Dxb"
      },
      "outputs": [],
      "source": [
        "model_2.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4AA6kH77Dxc"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj7PSLhu7Dxc"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U2tj5Uk7Dxc"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8VQLNDF7Dxc"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smHSf4BG7Dxc"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8vWSZlo7Dxc"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61ae9be-87b4-42e6-b484-208b80243a05",
        "id": "dK5aHt507Dxc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "50/50 [==============================] - 8s 63ms/step - loss: 3.1762\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 4s 66ms/step - loss: 2.5036\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 4s 60ms/step - loss: 2.2990\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - 4s 58ms/step - loss: 2.1212\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 4s 58ms/step - loss: 1.9525\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 1.8025\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 1.6715\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - 3s 59ms/step - loss: 1.5641\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 3s 59ms/step - loss: 1.4759\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - 3s 59ms/step - loss: 1.4011\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.3354\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.2758\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.2214\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.1668\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.1122\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - 3s 59ms/step - loss: 1.0583\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 1.0017\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.9401\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.8774\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.8102\n"
          ]
        }
      ],
      "source": [
        "history = model_2.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9oGvnuq7Dxd"
      },
      "source": [
        "## Step5: Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBGfknYj7Dxd"
      },
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3M7rGqa7Dxd"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0EZiWKR7Dxd"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuAyuuTP7Dxd"
      },
      "outputs": [],
      "source": [
        "one_step_model_2 = OneStep(model_2, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJH_pVbf7Dxd"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "575182ab-2832-4c51-b8e5-dc063ed9e317",
        "id": "gvoXE0Nt7Dxd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a begginers\n",
            "how to find your road lend on your life times client\n",
            "managring on baranwer laborm developer startsing\n",
            "hears the most important skillstrong\n",
            "tod assumptions of neurotics in hightigning for a science o \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 0.8176467418670654\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['a begginer'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(200):\n",
        "  next_char, states = one_step_model_2.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAa-gSFY7Dxe"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SkpSNS87Dxe"
      },
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d360becb-ed1e-41d6-b15a-499e9b60bdc1",
        "id": "5jRSnH917Dxe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beginerative easy made et\n",
            "3 musstrong\n",
            "the biggest spock privility using deep learning\n",
            "coffer living on the key to get the beginners\n",
            "created my reality colombundent\n",
            "solving the first me usage intentionald eve \n",
            "\n",
            "________________________________________________________________________________\n",
            "beginers\n",
            "solved backfrolt choices that level whech you find your and spe you hure\n",
            "what i learned from its stay get a coaly\n",
            "how erongens is all what sell swilk simple\n",
            "keep can hode\n",
            "theres and ker you dont nee \n",
            "\n",
            "________________________________________________________________________________\n",
            "beginers images\n",
            "what to write web apps and ecrovers excression ta know be of a melations\n",
            "deep learning from 4 minutes app classification with python and deing in deepent make great coming sources\n",
            "413 in a se \n",
            "\n",
            "________________________________________________________________________________\n",
            "beginers getting readersan stories are process innovating science is meetings\n",
            "automated me opportunity het its the worstem attacks  overvitection reviews\n",
            "the tixting over ldss this 1 things time\n",
            "how dath dec \n",
            "\n",
            "________________________________________________________________________________\n",
            "beginerative meets keep your change\n",
            "y are day stagementstrong\n",
            "five betters for data scientist next servery\n",
            "designer is an empertake\n",
            "you might not overlook  designed in one future for rations in python interm \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 0.7339882850646973\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"beginer\", \"beginer\", \"beginer\", \"beginer\", \"beginer\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(200):\n",
        "  next_char, states = one_step_model_2.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "result_str = [a.decode() for a in result.numpy()]\n",
        "for result in result_str:\n",
        "    print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bRIDn2piDEBT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-aiJpD1DEht"
      },
      "source": [
        "## Step 4a: Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but does not give you much control.\n",
        "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement _curriculum  learning_ to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and apply them to the model using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnG2VnOCDEhu"
      },
      "outputs": [],
      "source": [
        "class CustomTraining_3(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "          #predictions2 = tf.keras.activations.softmax(predictions)\n",
        "          #loss = self.loss(labels, predictions2)\n",
        "          \n",
        "          #loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=activation) #'softmax'\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GapNxNScDEhu"
      },
      "source": [
        "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows you to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRjzIl0DDEhu"
      },
      "outputs": [],
      "source": [
        "model_3 = CustomTraining_3(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrgtLY7QDEhv"
      },
      "outputs": [],
      "source": [
        "model_3.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5-1uV5rDEhv"
      },
      "outputs": [],
      "source": [
        "#model_3.fit(dataset, epochs=1)\n",
        "#fail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H024trzYDEhv"
      },
      "source": [
        "Or if you need more control, you can write your own complete custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bda2f0e-c830-4c3b-9680-4df2d64d27ec",
        "id": "ly4ZwX7UDEhv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.7554\n",
            "\n",
            "Epoch 1 Loss: 0.7365 Perplexity: 2.09\n",
            "Time taken for 1 epoch 5.12 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 0.7237\n",
            "\n",
            "Epoch 2 Loss: 0.7045 Perplexity: 2.02\n",
            "Time taken for 1 epoch 5.12 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 0.6898\n",
            "\n",
            "Epoch 3 Loss: 0.6710 Perplexity: 1.96\n",
            "Time taken for 1 epoch 3.89 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 0.6555\n",
            "\n",
            "Epoch 4 Loss: 0.6373 Perplexity: 1.89\n",
            "Time taken for 1 epoch 3.36 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 0.6216\n",
            "\n",
            "Epoch 5 Loss: 0.6042 Perplexity: 1.83\n",
            "Time taken for 1 epoch 3.46 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 0.5888\n",
            "\n",
            "Epoch 6 Loss: 0.5725 Perplexity: 1.77\n",
            "Time taken for 1 epoch 3.30 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 0.5577\n",
            "\n",
            "Epoch 7 Loss: 0.5426 Perplexity: 1.72\n",
            "Time taken for 1 epoch 3.30 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 0.5286\n",
            "\n",
            "Epoch 8 Loss: 0.5146 Perplexity: 1.67\n",
            "Time taken for 1 epoch 3.30 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 0.5014\n",
            "\n",
            "Epoch 9 Loss: 0.4885 Perplexity: 1.63\n",
            "Time taken for 1 epoch 3.27 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 0.4763\n",
            "\n",
            "Epoch 10 Loss: 0.4645 Perplexity: 1.59\n",
            "Time taken for 1 epoch 3.36 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model_2.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f} Perplexity: {tf.exp(mean.result().numpy()):.2f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model_2.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5a: Generate Text**"
      ],
      "metadata": {
        "id": "1orN6vynIe4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "defd25d4-f300-4aaf-f41c-fee58ad386bd",
        "id": "kwaP5RP1IhQa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a begginers\n",
            "5 questions to ask we generative vs discriminative modelsstrong\n",
            "why i decided to get abote web app jest project managementstrong\n",
            "question for all data contemplative\n",
            "twe analysis for building your jus \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 0.5653448104858398\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['a begginers'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(200):\n",
        "  next_char, states = one_step_model_2.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusions**"
      ],
      "metadata": {
        "id": "p0qdp0jw9zmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Быстрее полчается обучение с Char RNN (GRU) - без Bi-Directional\n",
        "2. Без val и test datasets - зазубривает хорошо, получается похоже\n",
        "3. Результат получается красивый, разнообразный (diverse)\n",
        "4. Но не связный с подаваемым на вход словом (тще coherent) "
      ],
      "metadata": {
        "id": "Xfqp3x86927T"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hgsVvVxnymwf",
        "MJdfPmdqzf-R",
        "kKkD5M6eoSiN"
      ],
      "name": "LM_TF_CharRNN_text_generation",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}