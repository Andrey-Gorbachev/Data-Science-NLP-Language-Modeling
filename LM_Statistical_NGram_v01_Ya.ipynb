{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram language models\n",
        "By [LenaVoita lecture Language Modeling](https://lena-voita.github.io/nlp_course/language_modeling.html)\n",
        "\n",
        "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_"
      ],
      "metadata": {
        "id": "zon31yanWmvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup & Import"
      ],
      "metadata": {
        "id": "6OyDNaTmJuEq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RirUA7Jhq4R2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "VXK4weIHJaWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### v1 dropbox"
      ],
      "metadata": {
        "id": "w-RysNkcJdxj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4IpNUMoqq4R4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "f4f803cc-dc6c-432d-d5bf-a7aba8fa30da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-24 18:11:07--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n",
            "--2022-06-24 18:11:07--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucc9b3ae8ed066d3110179abbf3c.dl.dropboxusercontent.com/cd/0/get/Bn14T5hLSMEIzJ0jE8NkT-6DKwHZ64c2T5q6UZDubFLEGIEgyGSGEXHdyYahOdMjlRy9EFrQXJCzggkpumEaB8m06aVG7DRRvlEHfl2rwl2eYcVEAd73EdBajhAJu4KbRLdfymYBwkIy3rs5-U1F_KyKkpwgYFVdNlN0vk9qcCvcbw/file?dl=1# [following]\n",
            "--2022-06-24 18:11:08--  https://ucc9b3ae8ed066d3110179abbf3c.dl.dropboxusercontent.com/cd/0/get/Bn14T5hLSMEIzJ0jE8NkT-6DKwHZ64c2T5q6UZDubFLEGIEgyGSGEXHdyYahOdMjlRy9EFrQXJCzggkpumEaB8m06aVG7DRRvlEHfl2rwl2eYcVEAd73EdBajhAJu4KbRLdfymYBwkIy3rs5-U1F_KyKkpwgYFVdNlN0vk9qcCvcbw/file?dl=1\n",
            "Resolving ucc9b3ae8ed066d3110179abbf3c.dl.dropboxusercontent.com (ucc9b3ae8ed066d3110179abbf3c.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucc9b3ae8ed066d3110179abbf3c.dl.dropboxusercontent.com (ucc9b3ae8ed066d3110179abbf3c.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18933283 (18M) [application/binary]\n",
            "Saving to: ‚ÄòarxivData.json.tar.gz‚Äô\n",
            "\n",
            "arxivData.json.tar. 100%[===================>]  18.06M  34.4MB/s    in 0.5s    \n",
            "\n",
            "2022-06-24 18:11:09 (34.4 MB/s) - ‚ÄòarxivData.json.tar.gz‚Äô saved [18933283/18933283]\n",
            "\n",
            "arxivData.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "6826   [{'name': 'Twan van Laarhoven'}, {'name': 'Ele...   16  1706.05335v2   \n",
              "23698  [{'name': 'Ju Sun'}, {'name': 'Qiang Chen'}, {...   27   1010.5610v1   \n",
              "2013   [{'name': 'Kamil Nar'}, {'name': 'Shankar Sast...   22  1803.08203v1   \n",
              "24037  [{'name': 'Adri√† Recasens'}, {'name': 'Carl Vo...    9  1612.03094v1   \n",
              "40743  [{'name': 'Chunxiao Jiang'}, {'name': 'Yan Che...    6   1212.1245v2   \n",
              "\n",
              "                                                    link  month  \\\n",
              "6826   [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
              "23698  [{'rel': 'alternate', 'href': 'http://arxiv.or...     10   \n",
              "2013   [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
              "24037  [{'rel': 'alternate', 'href': 'http://arxiv.or...     12   \n",
              "40743  [{'rel': 'related', 'href': 'http://dx.doi.org...     12   \n",
              "\n",
              "                                                 summary  \\\n",
              "6826   Unsupervised Domain Adaptation (DA) is used to...   \n",
              "23698  In this paper we propose a vision system that ...   \n",
              "2013   While training error of most deep neural netwo...   \n",
              "24037  Following the gaze of people inside videos is ...   \n",
              "40743  Distributed adaptive filtering has been consid...   \n",
              "\n",
              "                                                     tag  \\\n",
              "6826   [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "23698  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "2013   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "24037  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "40743  [{'term': 'cs.GT', 'scheme': 'http://arxiv.org...   \n",
              "\n",
              "                                                   title  year  \n",
              "6826   Unsupervised Domain Adaptation with Random Wal...  2017  \n",
              "23698                   Selective Image Super-Resolution  2010  \n",
              "2013   Residual Networks: Lyapunov Stability and Conv...  2018  \n",
              "24037                        Following Gaze Across Views  2016  \n",
              "40743  Distributed Adaptive Networks: A Graphical Evo...  2012  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2217f779-8f01-4e26-b37b-00e20cc2a18f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6826</th>\n",
              "      <td>[{'name': 'Twan van Laarhoven'}, {'name': 'Ele...</td>\n",
              "      <td>16</td>\n",
              "      <td>1706.05335v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>Unsupervised Domain Adaptation (DA) is used to...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Unsupervised Domain Adaptation with Random Wal...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23698</th>\n",
              "      <td>[{'name': 'Ju Sun'}, {'name': 'Qiang Chen'}, {...</td>\n",
              "      <td>27</td>\n",
              "      <td>1010.5610v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>10</td>\n",
              "      <td>In this paper we propose a vision system that ...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Selective Image Super-Resolution</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013</th>\n",
              "      <td>[{'name': 'Kamil Nar'}, {'name': 'Shankar Sast...</td>\n",
              "      <td>22</td>\n",
              "      <td>1803.08203v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>3</td>\n",
              "      <td>While training error of most deep neural netwo...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Residual Networks: Lyapunov Stability and Conv...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24037</th>\n",
              "      <td>[{'name': 'Adri√† Recasens'}, {'name': 'Carl Vo...</td>\n",
              "      <td>9</td>\n",
              "      <td>1612.03094v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>12</td>\n",
              "      <td>Following the gaze of people inside videos is ...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Following Gaze Across Views</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40743</th>\n",
              "      <td>[{'name': 'Chunxiao Jiang'}, {'name': 'Yan Che...</td>\n",
              "      <td>6</td>\n",
              "      <td>1212.1245v2</td>\n",
              "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org...</td>\n",
              "      <td>12</td>\n",
              "      <td>Distributed adaptive filtering has been consid...</td>\n",
              "      <td>[{'term': 'cs.GT', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Distributed Adaptive Networks: A Graphical Evo...</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2217f779-8f01-4e26-b37b-00e20cc2a18f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2217f779-8f01-4e26-b37b-00e20cc2a18f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2217f779-8f01-4e26-b37b-00e20cc2a18f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
        "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
        "!tar -xvzf arxivData.json.tar.gz\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### v2 from Kaggle \n",
        "\n",
        "Easiest way to download kaggle data in Google Colab https://www.kaggle.com/general/74235\n",
        "\n",
        "\n",
        "Please follow the steps below to download and use kaggle data within Google Colab:\n",
        "1. Go to your account, Scroll to API section and Click Expire API Token to remove previous tokens\n",
        "2. Click on Create New API Token - It will download kaggle.json file on your machine.\n",
        "\n",
        "last. Use unzip command to unzip the data:\n",
        "For example,\n",
        "Create a directory named train,\n",
        "```\n",
        "    ! mkdir train\n",
        "    ! unzip train.zip -d train\n",
        "```"
      ],
      "metadata": {
        "id": "IGMKrSXsEsIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "Li6soLxEEvBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload() # kaggle.json"
      ],
      "metadata": {
        "id": "NmJgm2GJHRsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "8ywVaSMiHfmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBAGMosTIGjL",
        "outputId": "83a2c2cf-8eee-438d-e8b7-75702308d09d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                                   title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "victorsoeiro/netflix-tv-shows-and-movies                              Netflix TV Shows and Movies                           2MB  2022-05-15 00:01:23           5848        185  1.0              \n",
            "surajjha101/stores-area-and-sales-data                                Supermarket store branches sales analysis            10KB  2022-04-29 11:10:16           5274        156  1.0              \n",
            "devansodariya/student-performance-data                                Student Performance Dataset                           7KB  2022-05-26 13:55:09           2236        111  0.9705882        \n",
            "paradisejoy/top-hits-spotify-from-20002019                            Top Hits Spotify from 2000-2019                      94KB  2022-05-31 07:20:57           4930        124  1.0              \n",
            "odins0n/amex-parquet                                                  Amex Competition Data in Parquet Format               9GB  2022-05-25 23:20:19            610         84  1.0              \n",
            "muratkokludataset/date-fruit-datasets                                 Date Fruit Datasets                                 408KB  2022-04-03 09:25:39          12497       1551  0.9375           \n",
            "ruchi798/parquet-files-amexdefault-prediction                         Feather & Parquet Files : AMEX-Default Prediction    22GB  2022-05-26 05:46:53            472         69  1.0              \n",
            "iamsouravbanerjee/software-professional-salaries-2022                 Salary Dataset - 2022                               526KB  2022-06-08 08:19:34            459         20  1.0              \n",
            "muhammedtausif/best-selling-mobile-phones                             Best Selling Mobile Phones                            2KB  2022-05-22 17:31:44            787         28  1.0              \n",
            "mdmahmudulhasansuzan/students-adaptability-level-in-online-education  Students Adaptability Level in Online Education       6KB  2022-04-16 04:46:28           8869        220  1.0              \n",
            "shariful07/student-mental-health                                      Student Mental health                                 2KB  2022-05-11 17:18:55           2664         52  0.8235294        \n",
            "dansbecker/melbourne-housing-snapshot                                 Melbourne Housing Snapshot                          451KB  2018-06-05 12:52:24          88292       1079  0.7058824        \n",
            "prasertk/cities-with-the-best-worklife-balance-2022                   Cities with the Best Work-Life Balance 2022           5KB  2022-06-01 09:15:52            395         17  0.9411765        \n",
            "mysarahmadbhat/airline-passenger-satisfaction                         Airline Passenger Satisfaction                        2MB  2022-05-19 11:46:02           1637         37  1.0              \n",
            "timmofeyy/-subway-locations-in-us                                     üçî Subway Restaurants Locations in US                778KB  2022-06-03 13:07:01            287         19  1.0              \n",
            "datasnaek/youtube-new                                                 Trending YouTube Video Statistics                   201MB  2019-06-03 00:56:47         176472       4554  0.7941176        \n",
            "ahmedshahriarsakib/usa-real-estate-dataset                            USA Real Estate Dataset                               5MB  2022-06-04 21:40:25            452         19  1.0              \n",
            "zynicide/wine-reviews                                                 Wine Reviews                                         51MB  2017-11-27 17:08:04         161164       3320  0.7941176        \n",
            "kkhandekar/cost-of-living-index-by-city-2022                          Cost of Living Index by City 2022                    15KB  2022-06-03 02:50:24            444         20  0.9411765        \n",
            "azminetoushikwasi/ucl-202122-uefa-champions-league                    UCL ‚öΩ 2021-22 ‚≠ê Players Data | Champions League      55KB  2022-06-04 08:31:45            679         33  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! kaggle competitions download -c 'name-of-competition'"
      ],
      "metadata": {
        "id": "EF3uAf6tIrd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kaggle datasets download <name-of-dataset>\n",
        "# https://www.kaggle.com/datasets/neelshah18/arxivdataset\n",
        "!kaggle datasets download neelshah18/arxivdataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZSNod1IGX6Z",
        "outputId": "aaf6decd-1442-4bef-e3ed-533ac57b4eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading arxivdataset.zip to /content\n",
            " 87% 16.0M/18.3M [00:00<00:00, 47.9MB/s]\n",
            "100% 18.3M/18.3M [00:00<00:00, 57.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip arxivdataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cla7JtIJ-eG",
        "outputId": "5a00143d-410a-4d42-ed70-de9a9ab325c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  arxivdataset.zip\n",
            "  inflating: arxivData.json          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ELtxye05KHEW",
        "outputId": "8587c731-8afc-4fc5-f21c-7190ac654c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "2515   [{'name': 'Pooria Joulani'}, {'name': 'Andr√°s ...   30  1507.00066v1   \n",
              "30919  [{'name': 'Umit Kacar'}, {'name': 'Murvet Kirc...   27  1801.09054v1   \n",
              "26618  [{'name': 'Yu-An Chung'}, {'name': 'Hsuan-Tien...   16  1611.05134v1   \n",
              "29737  [{'name': 'Sachin Mehta'}, {'name': 'Amar P. A...   10  1710.03811v2   \n",
              "23022  [{'name': 'Ilya Soloveychik'}, {'name': 'Vahid...   12  1802.03848v6   \n",
              "\n",
              "                                                    link  month  \\\n",
              "2515   [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
              "30919  [{'rel': 'alternate', 'href': 'http://arxiv.or...      1   \n",
              "26618  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
              "29737  [{'rel': 'alternate', 'href': 'http://arxiv.or...     10   \n",
              "23022  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "\n",
              "                                                 summary  \\\n",
              "2515   Cross-validation (CV) is one of the main tools...   \n",
              "30919  Only a few studies have been reported regardin...   \n",
              "26618  While deep neural networks have succeeded in s...   \n",
              "29737  The impact of soiling on solar panels is an im...   \n",
              "23022  In this work we consider the problem of model ...   \n",
              "\n",
              "                                                     tag  \\\n",
              "2515   [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "30919  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "26618  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "29737  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "23022  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "\n",
              "                                                   title  year  \n",
              "2515      Fast Cross-Validation for Incremental Learning  2015  \n",
              "30919  Ear Recognition With Score-Level Fusion Based ...  2018  \n",
              "26618  Cost-Sensitive Deep Learning with Layer-Wise C...  2016  \n",
              "29737  DeepSolarEye: Power Loss Prediction and Weakly...  2017  \n",
              "23022  Region Detection in Markov Random Fields: Gaus...  2018  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6f349644-03a5-4be8-b0f6-43fc4db25d1c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2515</th>\n",
              "      <td>[{'name': 'Pooria Joulani'}, {'name': 'Andr√°s ...</td>\n",
              "      <td>30</td>\n",
              "      <td>1507.00066v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>Cross-validation (CV) is one of the main tools...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Fast Cross-Validation for Incremental Learning</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30919</th>\n",
              "      <td>[{'name': 'Umit Kacar'}, {'name': 'Murvet Kirc...</td>\n",
              "      <td>27</td>\n",
              "      <td>1801.09054v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>1</td>\n",
              "      <td>Only a few studies have been reported regardin...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Ear Recognition With Score-Level Fusion Based ...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26618</th>\n",
              "      <td>[{'name': 'Yu-An Chung'}, {'name': 'Hsuan-Tien...</td>\n",
              "      <td>16</td>\n",
              "      <td>1611.05134v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>11</td>\n",
              "      <td>While deep neural networks have succeeded in s...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Cost-Sensitive Deep Learning with Layer-Wise C...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29737</th>\n",
              "      <td>[{'name': 'Sachin Mehta'}, {'name': 'Amar P. A...</td>\n",
              "      <td>10</td>\n",
              "      <td>1710.03811v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>10</td>\n",
              "      <td>The impact of soiling on solar panels is an im...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>DeepSolarEye: Power Loss Prediction and Weakly...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23022</th>\n",
              "      <td>[{'name': 'Ilya Soloveychik'}, {'name': 'Vahid...</td>\n",
              "      <td>12</td>\n",
              "      <td>1802.03848v6</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>In this work we consider the problem of model ...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Region Detection in Markov Random Fields: Gaus...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f349644-03a5-4be8-b0f6-43fc4db25d1c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6f349644-03a5-4be8-b0f6-43fc4db25d1c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6f349644-03a5-4be8-b0f6-43fc4db25d1c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "sdlbtu8AExSD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8_qnAN3kq4R5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff8b583-4899-42c1-e1d3-9529edc0bf3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
              " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
              " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# assemble lines: concatenate title and description\n",
        "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'], axis=1).tolist()\n",
        "\n",
        "sorted(lines, key=len)[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzPlKaYfq4R5"
      },
      "source": [
        "## Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3G2pFzk5q4R5"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "     \n",
        "tokenizer = WordPunctTokenizer()\n",
        "lines = [\n",
        "         ' '.join(tokenizer.tokenize(line.lower())) for line in lines\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZqCl2JAQq4R6"
      },
      "outputs": [],
      "source": [
        "assert sorted(lines, key=len)[0] == \\\n",
        "    'differential contrastive divergence ; this paper has been retracted .'\n",
        "assert sorted(lines, key=len)[2] == \\\n",
        "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1anfRs-Dq4R7"
      },
      "source": [
        "## N-Gram Language Model \n",
        "\n",
        "A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
        "\n",
        "It can do so by following the chain rule:\n",
        "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$ \n",
        "\n",
        "The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n",
        "\n",
        "One popular approximation is to assume that next word only depends on a finite amount of previous words:\n",
        "\n",
        "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n",
        "\n",
        "Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words. \n",
        "\n",
        "$$\n",
        "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
        "$$\n",
        "\n",
        "You can also sometimes see such approximation under the name of _n-th order markov assumption_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W87debD0q4R8"
      },
      "source": [
        "The first stage to building such a model is counting all word occurences given N-1 previous words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### count_ngrams v01"
      ],
      "metadata": {
        "id": "F5Yrboguk9CC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "arDFe0XFq4R9"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# special tokens: \n",
        "# - unk represents absent tokens, \n",
        "# - eos is a special token after the end of sequence\n",
        "\n",
        "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
        "\n",
        "def count_ngrams(lines, n):\n",
        "    \"\"\"\n",
        "    Count how many times each word occured after (n - 1) previous words\n",
        "    :param lines: an iterable of strings with space-separated tokens\n",
        "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
        "\n",
        "    When building counts, please consider the following two edge cases\n",
        "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
        "      empty prefix: \"\" -> (UNK, UNK)\n",
        "      short prefix: \"the\" -> (UNK, the)\n",
        "      long prefix: \"the new approach\" -> (new, approach)\n",
        "    - you should add a special token, EOS, at the end of each sequence\n",
        "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
        "      count the probability of this token just like all others.\n",
        "    \"\"\"\n",
        "    counts = defaultdict(Counter)\n",
        "    \n",
        "    cmd1 = \"counts[v[{i}], v[{i} + 1]][v[{i} + 2]] += 1\"\n",
        "    for line in lines:\n",
        "      pfx = '_UNK_ ' * (n-1)\n",
        "      header = f\"v = '''{pfx}{line} _EOS_'''.split()\\n\" \n",
        "      ops = '\\n'.join(cmd1.format(i=i) for i in range(len(header.split()[:-4])))\n",
        "      exec(header + ops)\n",
        "    \n",
        "    return counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### test  It"
      ],
      "metadata": {
        "id": "VJopuXrk2X7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xr_zZLWCq4R9"
      },
      "outputs": [],
      "source": [
        "# let's test it\n",
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
        "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
        "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
        "assert dummy_counts['p', '=']['np'] == 2\n",
        "assert dummy_counts['author', '.']['_EOS_'] == 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dummy_counts[('_UNK_', '_UNK_')])\n",
        "print(dummy_counts[('p', '=')])\n",
        "print(dummy_counts['p', '=']['np'])\n",
        "print(dummy_counts.keys())\n",
        "print(dummy_counts.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QefoUulvlMvz",
        "outputId": "8c5271cb-79bb-4a14-f874-cbd152425d91"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'a': 13, 'the': 3, 'on': 3, 'using': 2, 'learning': 2, 'automatic': 2, 'why': 2, 'proceedings': 2, 'piecewise': 2, 'differential': 1, 'what': 1, 'p': 1, 'computational': 1, 'weak': 1, 'creating': 1, 'defeasible': 1, 'essence': 1, 'deep': 1, 'statistical': 1, 'complex': 1, 'serious': 1, 'preprocessing': 1, 'liquid': 1, 'mining': 1, 'towards': 1, 'icon': 1, 'recognition': 1, 'glottochronologic': 1, 'utility': 1, 'temporized': 1, 'backpropagation': 1, 'random': 1, 'network': 1, 'glottochronology': 1, 'time': 1, 'convolutional': 1, 'fitness': 1, 'flip': 1, 'autonomous': 1, 'activitynet': 1, 'decision': 1, 'text': 1, 'discrimination': 1, 'are': 1, 'extraction': 1, 'comments': 1, 'resource': 1, 'advances': 1, 'exploration': 1, 'quantified': 1, 'in': 1, 'introduction': 1, 'beyond': 1, 'norm': 1, 'about': 1, 'unary': 1, 'some': 1, 'convex': 1, 'neurocontrol': 1, 'philosophy': 1, 'parallels': 1, 'an': 1, 'calculate': 1, 'group': 1, 'entropy': 1, 'word': 1, 'guarded': 1, 'cornell': 1, 'semistability': 1, 'attack': 1, 'standardization': 1, 'defensive': 1, 'how': 1, 'technical': 1, 'sat': 1, 'approximated': 1, 'solving': 1, 'agent': 1})\n",
            "Counter({'np': 2})\n",
            "2\n",
            "dict_keys([('_UNK_', '_UNK_'), ('_UNK_', 'differential'), ('differential', 'contrastive'), ('contrastive', 'divergence'), ('divergence', ';'), (';', 'this'), ('this', 'paper'), ('paper', 'has'), ('has', 'been'), ('been', 'retracted'), ('retracted', '.'), ('_UNK_', 'what'), ('what', 'does'), ('does', 'artificial'), ('artificial', 'life'), ('life', 'tell'), ('tell', 'us'), ('us', 'about'), ('about', 'death'), ('death', '?'), ('?', ';'), (';', 'short'), ('short', 'philosophical'), ('philosophical', 'essay'), ('_UNK_', 'p'), ('p', '='), ('=', 'np'), ('np', ';'), (';', 'we'), ('we', 'claim'), ('claim', 'to'), ('to', 'resolve'), ('resolve', 'the'), ('the', 'p'), ('p', '=?'), ('=?', 'np'), ('np', 'problem'), ('problem', 'via'), ('via', 'a'), ('a', 'formal'), ('formal', 'argument'), ('argument', 'for'), ('for', 'p'), ('np', '.'), ('_UNK_', 'computational'), ('computational', 'geometry'), ('geometry', 'column'), ('column', '38'), ('38', ';'), (';', 'recent'), ('recent', 'results'), ('results', 'on'), ('on', 'curve'), ('curve', 'reconstruction'), ('reconstruction', 'are'), ('are', 'described'), ('described', '.'), ('_UNK_', 'weak'), ('weak', 'evolvability'), ('evolvability', 'equals'), ('equals', 'strong'), ('strong', 'evolvability'), ('evolvability', ';'), (';', 'an'), ('an', 'updated'), ('updated', 'version'), ('version', 'will'), ('will', 'be'), ('be', 'uploaded'), ('uploaded', 'later'), ('later', '.'), ('_UNK_', 'creating'), ('creating', 'a'), ('a', 'new'), ('new', 'ontology'), ('ontology', ':'), (':', 'a'), ('a', 'modular'), ('modular', 'approach'), ('approach', ';'), (';', 'creating'), ('_UNK_', 'defeasible'), ('defeasible', 'reasoning'), ('reasoning', 'in'), ('in', 'oscar'), ('oscar', ';'), ('this', 'is'), ('is', 'a'), ('a', 'system'), ('system', 'description'), ('description', 'for'), ('for', 'the'), ('the', 'oscar'), ('oscar', 'defeasible'), ('defeasible', 'reasoner'), ('reasoner', '.'), ('_UNK_', 'essence'), ('essence', \"'\"), (\"'\", 'description'), ('description', ';'), (';', 'a'), ('a', 'description'), ('description', 'of'), ('of', 'the'), ('the', 'essence'), (\"'\", 'language'), ('language', 'as'), ('as', 'used'), ('used', 'by'), ('by', 'the'), ('the', 'tool'), ('tool', 'savile'), ('savile', 'row'), ('row', '.'), ('_UNK_', 'deep'), ('deep', 'neural'), ('neural', 'networks'), ('networks', '-'), ('-', 'a'), ('a', 'brief'), ('brief', 'history'), ('history', ';'), (';', 'introduction'), ('introduction', 'to'), ('to', 'deep'), ('networks', 'and'), ('and', 'their'), ('their', 'history'), ('history', '.'), ('_UNK_', 'statistical'), ('statistical', 'physics'), ('physics', 'for'), ('for', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', ';'), ('been', 'withdrawn'), ('withdrawn', 'by'), ('the', 'author'), ('author', '.'), ('_UNK_', 'complex'), ('complex', 'networks'), ('networks', ';'), ('to', 'the'), ('the', 'special'), ('special', 'issue'), ('issue', 'on'), ('on', 'complex'), ('networks', ','), (',', 'artificial'), ('life', 'journal'), ('journal', '.'), ('_UNK_', 'serious'), ('serious', 'flaws'), ('flaws', 'in'), ('in', 'korf'), ('korf', 'et'), ('et', 'al'), ('al', \".'\"), (\".'\", 's'), ('s', 'analysis'), ('analysis', 'on'), ('on', 'time'), ('time', 'complexity'), ('complexity', 'of'), ('of', 'a'), ('a', '*'), ('*', ';'), ('withdrawn', '.'), ('_UNK_', 'preprocessing'), ('preprocessing', ':'), ('a', 'step'), ('step', 'in'), ('in', 'automating'), ('automating', 'early'), ('early', 'detection'), ('detection', 'of'), ('of', 'cervical'), ('cervical', 'cancer'), ('cancer', ';'), ('_UNK_', 'liquid'), ('liquid', 'state'), ('state', 'machines'), ('machines', 'in'), ('in', 'adbiatic'), ('adbiatic', 'quantum'), ('quantum', 'computers'), ('computers', 'for'), ('for', 'general'), ('general', 'computation'), ('computation', ';'), (';', 'major'), ('major', 'mistakes'), ('mistakes', 'do'), ('do', 'not'), ('not', 'read'), ('_UNK_', 'mining'), ('mining', 'for'), ('for', 'trees'), ('trees', 'in'), ('in', 'a'), ('a', 'graph'), ('graph', 'is'), ('is', 'np'), ('np', '-'), ('-', 'complete'), ('complete', ';'), (';', 'mining'), ('is', 'shown'), ('shown', 'to'), ('to', 'be'), ('be', 'np'), ('complete', '.'), ('_UNK_', 'towards'), ('towards', 'a'), ('a', 'hierarchical'), ('hierarchical', 'model'), ('model', 'of'), ('of', 'consciousness'), ('consciousness', ','), (',', 'intelligence'), ('intelligence', ','), (',', 'mind'), ('mind', 'and'), ('and', 'body'), ('body', ';'), ('this', 'article'), ('article', 'is'), ('is', 'taken'), ('taken', 'out'), ('out', '.'), ('_UNK_', 'a'), ('a', 'notation'), ('notation', 'for'), ('for', 'markov'), ('markov', 'decision'), ('decision', 'processes'), ('processes', ';'), ('paper', 'specifies'), ('specifies', 'a'), ('processes', '.'), ('_UNK_', 'icon'), ('icon', 'challenge'), ('challenge', 'on'), ('on', 'algorithm'), ('algorithm', 'selection'), ('selection', ';'), ('we', 'present'), ('present', 'the'), ('the', 'results'), ('results', 'of'), ('the', 'icon'), ('selection', '.'), ('_UNK_', 'recognition'), ('recognition', 'of'), ('of', 'regular'), ('regular', 'shapes'), ('shapes', 'in'), ('in', 'satelite'), ('satelite', 'images'), ('images', ';'), ('author', 'ali'), ('ali', 'pourmohammad'), ('pourmohammad', '.'), ('_UNK_', 'glottochronologic'), ('glottochronologic', 'retrognostic'), ('retrognostic', 'of'), ('of', 'language'), ('language', 'system'), ('system', ';'), ('a', 'glottochronologic'), ('system', 'is'), ('is', 'proposed'), ('_UNK_', 'the'), ('the', 'model'), ('of', 'quantum'), ('quantum', 'evolution'), ('evolution', ';'), ('author', 'due'), ('due', 'to'), ('to', 'extremely'), ('extremely', 'unscientific'), ('unscientific', 'errors'), ('errors', '.'), ('_UNK_', 'utility'), ('utility', '-'), ('-', 'probability'), ('probability', 'duality'), ('duality', ';'), ('paper', 'presents'), ('presents', 'duality'), ('duality', 'between'), ('between', 'probability'), ('probability', 'distributions'), ('distributions', 'and'), ('and', 'utility'), ('utility', 'functions'), ('functions', '.'), ('_UNK_', 'temporized'), ('temporized', 'equilibria'), ('equilibria', ';'), ('to', 'a'), ('a', 'crucial'), ('crucial', 'error'), ('error', 'in'), ('in', 'the'), ('the', 'submission'), ('submission', 'action'), ('action', '.'), ('_UNK_', 'backpropagation'), ('backpropagation', 'in'), ('in', 'matrix'), ('matrix', 'notation'), ('notation', ';'), (';', 'in'), ('in', 'this'), ('this', 'note'), ('note', 'we'), ('we', 'calculate'), ('calculate', 'the'), ('the', 'gradient'), ('gradient', 'of'), ('the', 'network'), ('network', 'function'), ('function', 'in'), ('notation', '.'), ('_UNK_', 'random'), ('random', 'dfas'), ('dfas', 'are'), ('are', 'efficiently'), ('efficiently', 'pac'), ('pac', 'learnable'), ('learnable', ';'), ('withdrawn', 'due'), ('to', 'an'), ('an', 'error'), ('error', 'found'), ('found', 'by'), ('by', 'dana'), ('dana', 'angluin'), ('angluin', 'and'), ('and', 'lev'), ('lev', 'reyzin'), ('reyzin', '.'), ('_UNK_', 'network'), ('network', 'motifs'), ('motifs', 'in'), ('in', 'music'), ('music', 'sequences'), ('sequences', ';'), ('author', 'because'), ('because', 'it'), ('it', 'needs'), ('needs', 'a'), ('a', 'deep'), ('deep', 'methodological'), ('methodological', 'revision'), ('revision', '.'), ('_UNK_', 'glottochronology'), ('glottochronology', 'and'), ('and', 'problems'), ('problems', 'of'), ('of', 'protolanguage'), ('protolanguage', 'reconstruction'), ('reconstruction', ';'), ('a', 'method'), ('method', 'of'), ('of', 'languages'), ('languages', 'genealogical'), ('genealogical', 'trees'), ('trees', 'construction'), ('construction', 'is'), ('proposed', '.'), ('_UNK_', 'using'), ('using', 'slp'), ('slp', 'neural'), ('neural', 'network'), ('network', 'to'), ('to', 'persian'), ('persian', 'handwritten'), ('handwritten', 'digits'), ('digits', 'recognition'), ('recognition', ';'), ('_UNK_', 'time'), ('time', 'hopping'), ('hopping', 'technique'), ('technique', 'for'), ('for', 'faster'), ('faster', 'reinforcement'), ('reinforcement', 'learning'), ('learning', 'in'), ('in', 'simulations'), ('simulations', ';'), ('this', 'preprint'), ('preprint', 'has'), ('author', 'for'), ('for', 'revision'), ('_UNK_', 'convolutional'), ('convolutional', 'matching'), ('matching', 'pursuit'), ('pursuit', 'and'), ('and', 'dictionary'), ('dictionary', 'training'), ('training', ';'), (';', 'matching'), ('and', 'k'), ('k', '-'), ('-', 'svd'), ('svd', 'is'), ('is', 'demonstrated'), ('demonstrated', 'in'), ('the', 'translation'), ('translation', 'invariant'), ('invariant', 'setting'), ('_UNK_', 'fitness'), ('fitness', 'landscape'), ('landscape', 'analysis'), ('analysis', 'for'), ('for', 'dynamic'), ('dynamic', 'resource'), ('resource', 'allocation'), ('allocation', 'in'), ('in', 'multiuser'), ('multiuser', 'ofdm'), ('ofdm', 'based'), ('based', 'cognitive'), ('cognitive', 'radio'), ('radio', 'systems'), ('systems', ';'), ('a', 'note'), ('note', 'on'), ('on', 'darwiche'), ('darwiche', 'and'), ('and', 'pearl'), ('pearl', ';'), (';', 'it'), ('it', 'is'), ('shown', 'that'), ('that', 'darwiche'), ('pearl', \"'\"), (\"'\", 's'), ('s', 'postulates'), ('postulates', 'imply'), ('imply', 'an'), ('an', 'interesting'), ('interesting', 'property'), ('property', ','), (',', 'not'), ('not', 'noticed'), ('noticed', 'by'), ('the', 'authors'), ('authors', '.'), ('_UNK_', 'flip'), ('flip', '-'), ('-', 'flop'), ('flop', 'sublinear'), ('sublinear', 'models'), ('models', 'for'), ('for', 'graphs'), ('graphs', ':'), (':', 'proof'), ('proof', 'of'), ('of', 'theorem'), ('theorem', '1'), ('1', ';'), ('we', 'prove'), ('prove', 'that'), ('that', 'there'), ('there', 'is'), ('is', 'no'), ('no', 'class'), ('class', '-'), ('-', 'dual'), ('dual', 'for'), ('for', 'almost'), ('almost', 'all'), ('all', 'sublinear'), ('models', 'on'), ('on', 'graphs'), ('graphs', '.'), ('_UNK_', 'autonomous'), ('autonomous', 'perceptron'), ('perceptron', 'neural'), ('network', 'inspired'), ('inspired', 'from'), ('from', 'quantum'), ('quantum', 'computing'), ('computing', ';'), ('this', 'abstract'), ('abstract', 'will'), ('be', 'modified'), ('modified', 'after'), ('after', 'correcting'), ('correcting', 'the'), ('the', 'minor'), ('minor', 'error'), ('in', 'eq'), ('eq', '.('), ('.(', '2'), ('2', ')'), ('using', 'sets'), ('sets', 'of'), ('of', 'probability'), ('probability', 'measures'), ('measures', 'to'), ('to', 'represent'), ('represent', 'uncertainty'), ('uncertainty', ';'), (';', 'i'), ('i', 'explore'), ('explore', 'the'), ('the', 'use'), ('use', 'of'), ('of', 'sets'), ('measures', 'as'), ('as', 'a'), ('a', 'representation'), ('representation', 'of'), ('of', 'uncertainty'), ('uncertainty', '.'), ('a', 'comment'), ('comment', 'on'), ('on', 'argumentation'), ('argumentation', ';'), ('we', 'use'), ('use', 'the'), ('the', 'theory'), ('theory', 'of'), ('of', 'defaults'), ('defaults', 'and'), ('their', 'meaning'), ('meaning', 'of'), ('of', '['), ('[', 'gs16'), ('gs16', ']'), (']', 'to'), ('to', 'develop'), ('develop', '('), ('(', 'the'), ('the', 'outline'), ('outline', 'of'), ('a', ')'), (')', 'new'), ('new', 'theory'), ('of', 'argumentation'), ('argumentation', '.'), ('_UNK_', 'activitynet'), ('activitynet', 'challenge'), ('challenge', '2017'), ('2017', 'summary'), ('summary', ';'), (';', 'the'), ('the', 'activitynet'), ('activitynet', 'large'), ('large', 'scale'), ('scale', 'activity'), ('activity', 'recognition'), ('recognition', 'challenge'), ('summary', ':'), (':', 'results'), ('results', 'and'), ('and', 'challenge'), ('challenge', 'participants'), ('participants', 'papers'), ('papers', '.'), ('the', 'yahoo'), ('yahoo', 'query'), ('query', 'treebank'), ('treebank', ','), (',', 'v'), ('v', '.'), ('.', '1'), ('1', '.'), ('.', '0'), ('0', ';'), ('description', 'and'), ('and', 'annotation'), ('annotation', 'guidelines'), ('guidelines', 'for'), ('yahoo', 'webscope'), ('webscope', 'release'), ('release', 'of'), ('of', 'query'), (',', 'version'), ('version', '1'), ('0', ','), (',', 'may'), ('may', '2016'), ('2016', '.'), ('_UNK_', 'decision'), ('decision', 'under'), ('under', 'uncertainty'), ('we', 'derive'), ('derive', 'axiomatically'), ('axiomatically', 'the'), ('the', 'probability'), ('probability', 'function'), ('function', 'that'), ('that', 'should'), ('should', 'be'), ('be', 'used'), ('used', 'to'), ('to', 'make'), ('make', 'decisions'), ('decisions', 'given'), ('given', 'any'), ('any', 'form'), ('form', 'of'), ('of', 'underlying'), ('underlying', 'uncertainty'), ('_UNK_', 'text'), ('text', 'analysis'), ('analysis', 'tools'), ('tools', 'in'), ('in', 'spoken'), ('spoken', 'language'), ('this', 'submission'), ('submission', 'contains'), ('contains', 'the'), ('the', 'postscript'), ('postscript', 'of'), ('the', 'final'), ('final', 'version'), ('version', 'of'), ('the', 'slides'), ('slides', 'used'), ('used', 'in'), ('in', 'our'), ('our', 'acl'), ('acl', '-'), ('-', '94'), ('94', 'tutorial'), ('tutorial', '.'), ('_UNK_', 'discrimination'), ('discrimination', 'between'), ('between', 'arabic'), ('arabic', 'and'), ('and', 'latin'), ('latin', 'from'), ('from', 'bilingual'), ('bilingual', 'documents'), ('documents', ';'), (';', '2011'), ('2011', 'international'), ('international', 'conference'), ('conference', 'on'), ('on', 'communications'), ('communications', ','), (',', 'computing'), ('computing', 'and'), ('and', 'control'), ('control', 'applications'), ('applications', '('), ('(', 'ccca'), ('ccca', ')'), ('a', 'machine'), ('machine', '-'), ('-', 'learning'), ('learning', 'framework'), ('framework', 'for'), ('for', 'design'), ('design', 'for'), ('for', 'manufacturability'), ('manufacturability', ';'), ('a', 'duplicate'), ('duplicate', 'submission'), ('submission', '('), ('(', 'original'), ('original', 'is'), ('is', 'arxiv'), ('arxiv', ':'), (':', '1612'), ('1612', '.'), ('.', '02141'), ('02141', ').'), (').', 'hence'), ('hence', 'want'), ('want', 'to'), ('to', 'withdraw'), ('withdraw', 'it'), ('a', 'theory'), ('of', 'experiment'), ('experiment', ';'), ('article', 'aims'), ('aims', 'at'), ('at', 'clarifying'), ('clarifying', 'the'), ('the', 'language'), ('language', 'and'), ('and', 'practice'), ('practice', 'of'), ('of', 'scientific'), ('scientific', 'experiment'), ('experiment', ','), (',', 'mainly'), ('mainly', 'by'), ('by', 'hooking'), ('hooking', 'observability'), ('observability', 'on'), ('on', 'calculability'), ('calculability', '.'), ('_UNK_', 'are'), ('are', 'minds'), ('minds', 'computable'), ('computable', '?'), ('this', 'essay'), ('essay', 'explores'), ('explores', 'the'), ('the', 'limits'), ('limits', 'of'), ('of', 'turing'), ('turing', 'machines'), ('machines', 'concerning'), ('concerning', 'the'), ('the', 'modeling'), ('modeling', 'of'), ('of', 'minds'), ('minds', 'and'), ('and', 'suggests'), ('suggests', 'alternatives'), ('alternatives', 'to'), ('to', 'go'), ('go', 'beyond'), ('beyond', 'those'), ('those', 'limits'), ('limits', '.'), ('_UNK_', 'extraction'), ('extraction', 'de'), ('de', 'concepts'), ('concepts', 'sous'), ('sous', 'contraintes'), ('contraintes', 'dans'), ('dans', 'des'), ('des', 'donn√©es'), ('donn√©es', 'd'), ('d', \"'\"), (\"'\", 'expression'), ('expression', 'de'), ('de', 'g√®nes'), ('g√®nes', ';'), ('paper', ','), (',', 'we'), ('we', 'propose'), ('propose', 'a'), ('a', 'technique'), ('technique', 'to'), ('to', 'extract'), ('extract', 'constrained'), ('constrained', 'formal'), ('formal', 'concepts'), ('concepts', '.'), ('_UNK_', 'comments'), ('comments', 'on'), ('on', '\"'), ('\"', 'a'), ('new', 'combination'), ('combination', 'of'), ('of', 'evidence'), ('evidence', 'based'), ('based', 'on'), ('on', 'compromise'), ('compromise', '\"'), ('\"', 'by'), ('by', 'k'), ('k', '.'), ('.', 'yamada'), ('yamada', ';'), (';', 'comments'), ('on', '``'), ('``', 'a'), ('compromise', \"''\"), (\"''\", 'by'), ('_UNK_', 'learning'), ('learning', 'low'), ('low', '-'), ('-', 'shot'), ('shot', 'facial'), ('facial', 'representations'), ('representations', 'via'), ('via', '2d'), ('2d', 'warping'), ('warping', ';'), ('this', 'work'), ('work', ','), ('we', 'mainly'), ('mainly', 'study'), ('study', 'the'), ('the', 'influence'), ('influence', 'of'), ('the', '2d'), ('warping', 'module'), ('module', 'for'), ('for', 'one'), ('one', '-'), ('shot', 'face'), ('face', 'recognition'), ('recognition', '.'), ('_UNK_', 'automatic'), ('automatic', 'generation'), ('generation', 'of'), ('of', 'benchmarks'), ('benchmarks', 'for'), ('for', 'plagiarism'), ('plagiarism', 'detection'), ('detection', 'tools'), ('tools', 'using'), ('using', 'grammatical'), ('grammatical', 'evolution'), ('authors', 'due'), ('a', 'major'), ('major', 'rewriting'), ('rewriting', '.'), ('_UNK_', 'resource'), ('allocation', 'of'), ('of', 'mu'), ('mu', '-'), ('-', 'ofdm'), ('systems', 'under'), ('under', 'partial'), ('partial', 'channel'), ('channel', 'state'), ('state', 'information'), ('information', ';'), ('to', 'some'), ('some', 'errors'), ('_UNK_', 'advances'), ('advances', 'in'), ('in', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'require'), ('require', 'progress'), ('progress', 'across'), ('across', 'all'), ('all', 'of'), ('of', 'computer'), ('computer', 'science'), ('science', ';'), (';', 'advances'), ('science', '.'), ('_UNK_', 'exploration'), ('exploration', 'of'), ('of', 'object'), ('object', 'recognition'), ('recognition', 'from'), ('from', '3d'), ('3d', 'point'), ('point', 'cloud'), ('cloud', ';'), ('present', 'our'), ('our', 'latest'), ('latest', 'experiment'), ('experiment', 'results'), ('cloud', 'data'), ('data', 'collected'), ('collected', 'through'), ('through', 'moving'), ('moving', 'car'), ('car', '.'), ('_UNK_', 'quantified'), ('quantified', 'conditional'), ('conditional', 'logics'), ('logics', 'are'), ('are', 'fragments'), ('fragments', 'of'), ('of', 'hol'), ('hol', ';'), ('a', 'semantic'), ('semantic', 'embedding'), ('embedding', 'of'), ('of', '('), ('(', 'constant'), ('constant', 'domain'), ('domain', ')'), (')', 'quantified'), ('conditional', 'logic'), ('logic', 'in'), ('in', 'classical'), ('classical', 'higher'), ('higher', '-'), ('-', 'order'), ('order', 'logic'), ('logic', 'is'), ('is', 'presented'), ('presented', '.'), ('_UNK_', 'in'), ('in', 'memoriam'), ('memoriam', 'maurice'), ('maurice', 'gross'), ('gross', ';'), (';', 'maurice'), ('gross', '('), ('(', '1934'), ('1934', '-'), ('-', '2001'), ('2001', ')'), (')', 'was'), ('was', 'both'), ('both', 'a'), ('a', 'great'), ('great', 'linguist'), ('linguist', 'and'), ('and', 'a'), ('a', 'pioneer'), ('pioneer', 'in'), ('in', 'natural'), ('processing', '.'), ('.', 'this'), ('is', 'written'), ('written', 'in'), ('in', 'homage'), ('homage', 'to'), ('to', 'his'), ('his', 'memory'), ('_UNK_', 'introduction'), ('the', '26th'), ('26th', 'international'), ('on', 'logic'), ('logic', 'programming'), ('programming', 'special'), ('issue', ';'), ('is', 'the'), ('the', 'preface'), ('preface', 'to'), ('_UNK_', 'beyond'), ('beyond', 'description'), ('description', '.'), ('.', 'comment'), ('\"', 'approaching'), ('approaching', 'human'), ('human', 'language'), ('language', 'with'), ('with', 'complex'), ('networks', '\"'), ('by', 'cong'), ('cong', '&'), ('&', 'liu'), ('liu', ';'), (';', 'comment'), ('_UNK_', 'norm'), ('norm', '-'), ('-', 'based'), ('based', 'capacity'), ('capacity', 'control'), ('control', 'in'), ('in', 'neural'), ('we', 'investigate'), ('investigate', 'the'), ('the', 'capacity'), ('capacity', ','), (',', 'convexity'), ('convexity', 'and'), ('and', 'characterization'), ('characterization', 'of'), ('a', 'general'), ('general', 'family'), ('family', 'of'), ('of', 'norm'), ('-', 'constrained'), ('constrained', 'feed'), ('feed', '-'), ('-', 'forward'), ('forward', 'networks'), ('networks', '.'), ('_UNK_', 'about'), ('about', 'compression'), ('compression', 'of'), ('of', 'vocabulary'), ('vocabulary', 'in'), ('in', 'computer'), ('computer', 'oriented'), ('oriented', 'languages'), ('languages', ';'), ('author', 'uses'), ('uses', 'the'), ('the', 'entropy'), ('entropy', 'of'), ('the', 'ideal'), ('ideal', 'bose'), ('bose', '-'), ('-', 'einstein'), ('einstein', 'gas'), ('gas', 'to'), ('to', 'minimize'), ('minimize', 'losses'), ('losses', 'in'), ('computer', '-'), ('-', 'oriented'), ('languages', '.'), ('_UNK_', 'unary'), ('unary', 'coding'), ('coding', 'for'), ('for', 'neural'), ('network', 'learning'), ('learning', ';'), ('presents', 'some'), ('some', 'properties'), ('properties', 'of'), ('of', 'unary'), ('coding', 'of'), ('of', 'significance'), ('significance', 'for'), ('for', 'biological'), ('biological', 'learning'), ('learning', 'and'), ('and', 'instantaneously'), ('instantaneously', 'trained'), ('trained', 'neural'), ('_UNK_', 'some'), ('the', 'ukrainian'), ('ukrainian', 'writing'), ('writing', 'system'), ('the', 'grapheme'), ('grapheme', '-'), ('-', 'phoneme'), ('phoneme', 'relation'), ('relation', 'in'), ('in', 'ukrainian'), ('ukrainian', 'and'), ('and', 'some'), ('ukrainian', 'version'), ('the', 'cyrillic'), ('cyrillic', 'alphabet'), ('alphabet', '.'), ('_UNK_', 'convex'), ('convex', 'multiview'), ('multiview', 'fisher'), ('fisher', 'discriminant'), ('discriminant', 'analysis'), ('analysis', ';'), (';', 'section'), ('section', '1'), ('.', '3'), ('3', 'was'), ('was', 'incorrect'), ('incorrect', ','), (',', 'and'), ('and', '2'), ('2', '.'), ('1', 'will'), ('be', 'removed'), ('removed', 'from'), ('from', 'further'), ('further', 'submissions'), ('submissions', '.'), ('.', 'a'), ('a', 'rewritten'), ('rewritten', 'version'), ('be', 'posted'), ('posted', 'in'), ('the', 'future'), ('future', '.'), ('_UNK_', 'why'), ('why', 'bother'), ('bother', 'with'), ('with', 'syntax'), ('syntax', '?'), ('this', 'short'), ('short', 'note'), ('note', 'discusses'), ('discusses', 'the'), ('the', 'role'), ('role', 'of'), ('of', 'syntax'), ('syntax', 'vs'), ('vs', '.'), ('.', 'semantics'), ('semantics', 'and'), ('and', 'the'), ('the', 'interplay'), ('interplay', 'between'), ('between', 'logic'), ('logic', ','), (',', 'philosophy'), ('philosophy', ','), ('and', 'language'), ('language', 'in'), ('science', 'and'), ('and', 'game'), ('game', 'theory'), ('theory', '.'), ('why', \"'\"), (\"'\", 'gsa'), ('gsa', ':'), ('a', 'gravitational'), ('gravitational', 'search'), ('search', 'algorithm'), ('algorithm', \"'\"), (\"'\", 'is'), ('is', 'not'), ('not', 'genuinely'), ('genuinely', 'based'), ('on', 'the'), ('the', 'law'), ('law', 'of'), ('of', 'gravity'), ('gravity', ';'), (';', 'why'), ('_UNK_', 'neurocontrol'), ('neurocontrol', 'methods'), ('methods', 'review'), ('review', ';'), (';', 'methods'), ('methods', 'of'), ('of', 'applying'), ('applying', 'neural'), ('networks', 'to'), ('to', 'control'), ('control', 'plants'), ('plants', 'are'), ('are', 'considered'), ('considered', '.'), ('.', 'methods'), ('methods', 'and'), ('and', 'schemes'), ('schemes', 'are'), ('described', ','), (',', 'their'), ('their', 'advantages'), ('advantages', 'and'), ('and', 'disadvantages'), ('disadvantages', 'are'), ('are', 'discussed'), ('discussed', '.'), ('_UNK_', 'on'), ('the', 'universality'), ('universality', 'of'), ('of', 'online'), ('online', 'mirror'), ('mirror', 'descent'), ('descent', ';'), ('we', 'show'), ('show', 'that'), ('that', 'for'), ('for', 'a'), ('general', 'class'), ('class', 'of'), ('of', 'convex'), ('convex', 'online'), ('online', 'learning'), ('learning', 'problems'), ('problems', ','), (',', 'mirror'), ('descent', 'can'), ('can', 'always'), ('always', 'achieve'), ('achieve', 'a'), ('a', '('), ('(', 'nearly'), ('nearly', ')'), (')', 'optimal'), ('optimal', 'regret'), ('regret', 'guarantee'), ('guarantee', '.'), ('the', 'possibility'), ('possibility', 'of'), ('of', 'making'), ('making', 'the'), ('the', 'complete'), ('complete', 'computer'), ('computer', 'model'), ('a', 'human'), ('human', 'brain'), ('brain', ';'), ('the', 'development'), ('development', 'of'), ('the', 'algorithm'), ('algorithm', 'of'), ('a', 'neural'), ('network', 'building'), ('building', 'by'), ('the', 'corresponding'), ('corresponding', 'parts'), ('parts', 'of'), ('a', 'dna'), ('dna', 'code'), ('code', 'is'), ('is', 'discussed'), ('_UNK_', 'philosophy'), ('philosophy', 'in'), ('the', 'face'), ('face', 'of'), ('of', 'artificial'), ('intelligence', ';'), ('article', ','), (',', 'i'), ('i', 'discuss'), ('discuss', 'how'), ('how', 'the'), ('the', 'ai'), ('ai', 'community'), ('community', 'views'), ('views', 'concerns'), ('concerns', 'about'), ('about', 'the'), ('the', 'emergence'), ('emergence', 'of'), ('of', 'superintelligent'), ('superintelligent', 'ai'), ('ai', 'and'), ('and', 'related'), ('related', 'philosophical'), ('philosophical', 'issues'), ('issues', '.'), ('a', 'survey'), ('survey', 'on'), ('on', 'contextual'), ('contextual', 'multi'), ('multi', '-'), ('-', 'armed'), ('armed', 'bandits'), ('bandits', ';'), ('this', 'survey'), ('survey', 'we'), ('we', 'cover'), ('cover', 'a'), ('a', 'few'), ('few', 'stochastic'), ('stochastic', 'and'), ('and', 'adversarial'), ('adversarial', 'contextual'), ('contextual', 'bandit'), ('bandit', 'algorithms'), ('algorithms', '.'), ('.', 'we'), ('we', 'analyze'), ('analyze', 'each'), ('each', 'algorithm'), ('s', 'assumption'), ('assumption', 'and'), ('and', 'regret'), ('regret', 'bound'), ('bound', '.'), ('_UNK_', 'parallels'), ('parallels', 'of'), ('of', 'human'), ('the', 'behavior'), ('behavior', 'of'), ('of', 'bottlenose'), ('bottlenose', 'dolphins'), ('dolphins', ';'), ('a', 'short'), ('short', 'review'), ('review', 'of'), ('of', 'similarities'), ('similarities', 'between'), ('between', 'dolphins'), ('dolphins', 'and'), ('and', 'humans'), ('humans', 'with'), ('with', 'the'), ('the', 'help'), ('help', 'of'), ('of', 'quantitative'), ('quantitative', 'linguistics'), ('linguistics', 'and'), ('and', 'information'), ('information', 'theory'), ('a', 'history'), ('history', 'of'), ('of', 'metaheuristics'), ('metaheuristics', ';'), ('this', 'chapter'), ('chapter', 'describes'), ('describes', 'the'), ('the', 'history'), ('metaheuristics', 'in'), ('in', 'five'), ('five', 'distinct'), ('distinct', 'periods'), ('periods', ','), (',', 'starting'), ('starting', 'long'), ('long', 'before'), ('before', 'the'), ('the', 'first'), ('first', 'use'), ('the', 'term'), ('term', 'and'), ('and', 'ending'), ('ending', 'a'), ('a', 'long'), ('long', 'time'), ('time', 'in'), ('_UNK_', 'an'), ('an', 'energy'), ('energy', 'efficient'), ('efficient', 'scheme'), ('scheme', 'for'), ('for', 'data'), ('data', 'gathering'), ('gathering', 'in'), ('in', 'wireless'), ('wireless', 'sensor'), ('sensor', 'networks'), ('networks', 'using'), ('using', 'particle'), ('particle', 'swarm'), ('swarm', 'optimization'), ('optimization', ';'), ('crucial', 'sign'), ('sign', 'error'), ('in', 'equation'), ('equation', '1'), ('new', 'bengali'), ('bengali', 'readability'), ('readability', 'score'), ('score', ';'), ('paper', 'we'), ('we', 'have'), ('have', 'proposed'), ('proposed', 'methods'), ('methods', 'to'), ('to', 'analyze'), ('analyze', 'the'), ('the', 'readability'), ('readability', 'of'), ('of', 'bengali'), ('bengali', 'language'), ('language', 'texts'), ('texts', '.'), ('have', 'got'), ('got', 'some'), ('some', 'exceptionally'), ('exceptionally', 'good'), ('good', 'results'), ('results', 'out'), ('out', 'of'), ('the', 'experiments'), ('experiments', '.'), ('the', 'logic'), ('programming', 'paradigm'), ('paradigm', 'and'), ('and', 'prolog'), ('prolog', ';'), ('a', 'tutorial'), ('tutorial', 'on'), ('programming', 'and'), ('prolog', 'appropriate'), ('appropriate', 'for'), ('a', 'course'), ('course', 'on'), ('on', 'programming'), ('programming', 'languages'), ('languages', 'for'), ('for', 'students'), ('students', 'familiar'), ('familiar', 'with'), ('with', 'imperative'), ('imperative', 'programming'), ('programming', '.'), ('_UNK_', 'calculate'), ('calculate', 'distance'), ('distance', 'to'), ('to', 'object'), ('object', 'in'), ('the', 'area'), ('area', 'where'), ('where', 'car'), ('car', ','), (',', 'using'), ('using', 'video'), ('video', 'analysis'), ('the', 'method'), ('of', 'using'), ('video', 'cameras'), ('cameras', 'installed'), ('installed', 'on'), ('the', 'car'), (',', 'to'), ('to', 'calculate'), ('the', 'distance'), ('the', 'object'), ('in', 'its'), ('its', 'area'), ('area', 'of'), ('of', 'movement'), ('movement', '.'), ('_UNK_', 'group'), ('group', 'theory'), ('theory', ','), (',', 'group'), ('group', 'actions'), ('actions', ','), (',', 'evolutionary'), ('evolutionary', 'algorithms'), ('algorithms', ','), ('and', 'global'), ('global', 'optimization'), ('use', 'group'), ('group', ','), (',', 'action'), ('action', 'and'), ('and', 'orbit'), ('orbit', 'to'), ('to', 'understand'), ('understand', 'how'), ('how', 'evolutionary'), ('evolutionary', 'solve'), ('solve', 'nonconvex'), ('nonconvex', 'optimization'), ('optimization', 'problems'), ('problems', '.'), ('_UNK_', 'entropy'), ('of', 'telugu'), ('telugu', ';'), ('presents', 'an'), ('an', 'investigation'), ('investigation', 'of'), ('the', 'telugu'), ('telugu', 'script'), ('script', '.'), ('.', 'since'), ('since', 'this'), ('this', 'script'), ('script', 'is'), ('is', 'syllabic'), ('syllabic', ','), ('and', 'not'), ('not', 'alphabetic'), ('alphabetic', ','), (',', 'the'), ('the', 'computation'), ('computation', 'of'), ('of', 'entropy'), ('entropy', 'is'), ('is', 'somewhat'), ('somewhat', 'complicated'), ('complicated', '.'), ('_UNK_', 'word'), ('word', 'segmentation'), ('segmentation', 'on'), ('on', 'micro'), ('micro', '-'), ('-', 'blog'), ('blog', 'texts'), ('texts', 'with'), ('with', 'external'), ('external', 'lexicon'), ('lexicon', 'and'), ('and', 'heterogeneous'), ('heterogeneous', 'data'), ('data', ';'), ('paper', 'describes'), ('describes', 'our'), ('our', 'system'), ('system', 'designed'), ('designed', 'for'), ('the', 'nlpcc'), ('nlpcc', '2016'), ('2016', 'shared'), ('shared', 'task'), ('task', 'on'), ('on', 'word'), ('_UNK_', 'guarded'), ('guarded', 'resolution'), ('resolution', 'for'), ('for', 'answer'), ('answer', 'set'), ('set', 'programming'), ('programming', ';'), ('we', 'describe'), ('describe', 'a'), ('a', 'variant'), ('variant', 'of'), ('of', 'resolution'), ('resolution', 'rule'), ('rule', 'of'), ('of', 'proof'), ('proof', 'and'), ('and', 'show'), ('that', 'it'), ('is', 'complete'), ('complete', 'for'), ('for', 'stable'), ('stable', 'semantics'), ('semantics', 'of'), ('of', 'logic'), ('logic', 'programs'), ('programs', '.'), ('show', 'applications'), ('applications', 'of'), ('of', 'this'), ('this', 'result'), ('result', '.'), ('_UNK_', 'cornell'), ('cornell', 'spf'), ('spf', ':'), (':', 'cornell'), ('cornell', 'semantic'), ('semantic', 'parsing'), ('parsing', 'framework'), ('framework', ';'), ('the', 'cornell'), ('framework', '('), ('(', 'spf'), ('spf', ')'), (')', 'is'), ('a', 'learning'), ('and', 'inference'), ('inference', 'framework'), ('for', 'mapping'), ('mapping', 'natural'), ('language', 'to'), ('to', 'formal'), ('formal', 'representation'), ('of', 'its'), ('its', 'meaning'), ('meaning', '.'), ('_UNK_', 'semistability'), ('semistability', '-'), ('based', 'convergence'), ('convergence', 'analysis'), ('for', 'paracontracting'), ('paracontracting', 'multiagent'), ('multiagent', 'coordination'), ('coordination', 'optimization'), ('this', 'sequential'), ('sequential', 'technical'), ('technical', 'report'), ('report', 'extends'), ('extends', 'some'), ('some', 'of'), ('the', 'previous'), ('previous', 'results'), ('results', 'we'), ('we', 'posted'), ('posted', 'at'), ('at', 'arxiv'), (':', '1306'), ('1306', '.'), ('.', '0225'), ('0225', '.'), ('_UNK_', 'proceedings'), ('proceedings', 'of'), ('first', 'international'), ('international', 'workshop'), ('workshop', 'on'), ('on', 'deep'), ('deep', 'learning'), ('and', 'music'), ('music', ';'), (';', 'proceedings'), ('music', ','), (',', 'joint'), ('joint', 'with'), ('with', 'ijcnn'), ('ijcnn', ','), (',', 'anchorage'), ('anchorage', ','), (',', 'us'), ('us', ','), ('may', '17'), ('17', '-'), ('-', '18'), ('18', ','), (',', '2017'), ('_UNK_', 'attack'), ('attack', 'rmse'), ('rmse', 'leaderboard'), ('leaderboard', ':'), (':', 'an'), ('an', 'introduction'), ('introduction', 'and'), ('and', 'case'), ('case', 'study'), ('study', ';'), ('this', 'manuscript'), ('manuscript', ','), ('we', 'briefly'), ('briefly', 'introduce'), ('introduce', 'several'), ('several', 'tricks'), ('tricks', 'to'), ('to', 'climb'), ('climb', 'the'), ('the', 'leaderboards'), ('leaderboards', 'which'), ('which', 'use'), ('use', 'rmse'), ('rmse', 'for'), ('for', 'evaluation'), ('evaluation', 'without'), ('without', 'exploiting'), ('exploiting', 'any'), ('any', 'training'), ('training', 'data'), ('data', '.'), ('_UNK_', 'standardization'), ('standardization', 'of'), ('the', 'formal'), ('of', 'lexical'), ('lexical', 'information'), ('information', 'for'), ('for', 'nlp'), ('nlp', ';'), ('survey', 'of'), ('of', 'dictionary'), ('dictionary', 'models'), ('models', 'and'), ('and', 'formats'), ('formats', 'is'), ('presented', 'as'), ('as', 'well'), ('well', 'as'), ('a', 'presentation'), ('presentation', 'of'), ('of', 'corresponding'), ('corresponding', 'recent'), ('recent', 'standardisation'), ('standardisation', 'activities'), ('activities', '.'), ('machine', 'learning'), ('learning', 'model'), ('model', 'for'), ('for', 'stock'), ('stock', 'market'), ('market', 'prediction'), ('prediction', ';'), (';', 'stock'), ('prediction', 'is'), ('the', 'act'), ('act', 'of'), ('of', 'trying'), ('trying', 'to'), ('to', 'determine'), ('determine', 'the'), ('future', 'value'), ('value', 'of'), ('a', 'company'), ('company', 'stock'), ('stock', 'or'), ('or', 'other'), ('other', 'financial'), ('financial', 'instrument'), ('instrument', 'traded'), ('traded', 'on'), ('on', 'a'), ('a', 'financial'), ('financial', 'exchange'), ('exchange', '.'), ('_UNK_', 'defensive'), ('defensive', 'distillation'), ('distillation', 'is'), ('not', 'robust'), ('robust', 'to'), ('to', 'adversarial'), ('adversarial', 'examples'), ('examples', ';'), ('that', 'defensive'), ('not', 'secure'), ('secure', ':'), (':', 'it'), ('no', 'more'), ('more', 'resistant'), ('resistant', 'to'), ('to', 'targeted'), ('targeted', 'misclassification'), ('misclassification', 'attacks'), ('attacks', 'than'), ('than', 'unprotected'), ('unprotected', 'neural'), ('of', 'nips'), ('nips', '2017'), ('2017', 'symposium'), ('symposium', 'on'), ('on', 'interpretable'), ('interpretable', 'machine'), ('the', 'proceedings'), ('learning', ','), (',', 'held'), ('held', 'in'), ('in', 'long'), ('long', 'beach'), ('beach', ','), (',', 'california'), ('california', ','), (',', 'usa'), ('usa', 'on'), ('on', 'december'), ('december', '7'), ('7', ','), ('_UNK_', 'piecewise'), ('piecewise', 'linear'), ('linear', 'activation'), ('activation', 'functions'), ('functions', 'for'), ('for', 'more'), ('more', 'efficient'), ('efficient', 'deep'), ('deep', 'networks'), ('submission', 'has'), ('by', 'arxiv'), ('arxiv', 'administrators'), ('administrators', 'because'), ('is', 'intentionally'), ('intentionally', 'incomplete'), ('incomplete', ','), (',', 'which'), ('which', 'is'), ('is', 'in'), ('in', 'violation'), ('violation', 'of'), ('of', 'our'), ('our', 'policies'), ('policies', '.'), ('the', 'triangle'), ('triangle', 'inequality'), ('inequality', 'for'), ('the', 'jaccard'), ('jaccard', 'distance'), ('distance', ';'), (';', 'two'), ('two', 'simple'), ('simple', 'proofs'), ('proofs', 'of'), ('distance', 'in'), ('in', 'terms'), ('terms', 'of'), ('of', 'nonnegative'), ('nonnegative', ','), (',', 'monotone'), ('monotone', ','), (',', 'submodular'), ('submodular', 'functions'), ('functions', 'are'), ('are', 'given'), ('given', 'and'), ('and', 'discussed'), ('_UNK_', 'how'), ('how', 'to'), ('to', 'realize'), ('realize', '\"'), ('a', 'sense'), ('sense', 'of'), ('of', 'humour'), ('humour', '\"'), ('\"', 'in'), ('in', 'computers'), ('computers', '?'), (';', 'computer'), ('a', '\"'), ('\"', 'sense'), ('\"', 'suggested'), ('suggested', 'previously'), ('previously', '['), ('[', 'arxiv'), (':', '0711'), ('0711', '.'), ('.', '2058'), ('2058', ','), (',', '0711'), ('.', '2061'), ('2061', ','), ('.', '2270'), ('2270', ']'), (']', 'is'), ('is', 'raised'), ('raised', 'to'), ('the', 'level'), ('level', 'of'), ('a', 'realistic'), ('realistic', 'algorithm'), ('algorithm', '.'), ('linear', 'multilayer'), ('multilayer', 'perceptrons'), ('perceptrons', 'and'), ('and', 'dropout'), ('dropout', ';'), ('new', 'type'), ('type', 'of'), ('of', 'hidden'), ('hidden', 'layer'), ('layer', 'for'), ('a', 'multilayer'), ('multilayer', 'perceptron'), ('perceptron', ','), ('and', 'demonstrate'), ('demonstrate', 'that'), ('it', 'obtains'), ('obtains', 'the'), ('the', 'best'), ('best', 'reported'), ('reported', 'performance'), ('performance', 'for'), ('for', 'an'), ('an', 'mlp'), ('mlp', 'on'), ('the', 'mnist'), ('mnist', 'dataset'), ('dataset', '.'), ('_UNK_', 'technical'), ('report', ':'), ('a', 'tool'), ('tool', 'for'), ('for', 'measuring'), ('measuring', 'prosodic'), ('prosodic', 'accommodation'), ('accommodation', ';'), ('article', 'has'), ('because', 'the'), ('the', 'submitter'), ('submitter', 'did'), ('did', 'not'), ('not', 'have'), ('have', 'the'), ('the', 'legal'), ('legal', 'authority'), ('authority', 'to'), ('to', 'grant'), ('grant', 'the'), ('the', 'license'), ('license', 'applied'), ('applied', 'to'), ('the', 'work'), ('work', '.'), ('a', 'remark'), ('remark', 'on'), ('on', 'higher'), ('higher', 'order'), ('order', 'rue'), ('rue', '-'), ('-', 'resolution'), ('resolution', 'with'), ('with', 'extrue'), ('extrue', ';'), ('that', 'a'), ('a', 'prominent'), ('prominent', 'counterexample'), ('counterexample', 'for'), ('the', 'completeness'), ('completeness', 'of'), ('of', 'first'), ('first', 'order'), ('resolution', 'does'), ('does', 'not'), ('not', 'apply'), ('apply', 'to'), ('the', 'higher'), ('resolution', 'approach'), ('approach', 'extrue'), ('extrue', '.'), ('_UNK_', 'sat'), ('sat', 'as'), ('a', 'game'), ('game', ';'), ('a', 'funny'), ('funny', 'representation'), ('of', 'sat'), ('sat', '.'), ('.', 'while'), ('while', 'the'), ('the', 'primary'), ('primary', 'interest'), ('interest', 'is'), ('is', 'to'), ('to', 'present'), ('present', 'propositional'), ('propositional', 'satisfiability'), ('satisfiability', 'in'), ('a', 'playful'), ('playful', 'way'), ('way', 'for'), ('for', 'pedagogical'), ('pedagogical', 'purposes'), ('purposes', ','), (',', 'it'), ('it', 'could'), ('could', 'also'), ('also', 'inspire'), ('inspire', 'new'), ('new', 'search'), ('search', 'heuristics'), ('heuristics', '.'), ('on', 'adjusting'), ('adjusting', '$'), ('$', 'r'), ('r', '^'), ('^', '2'), ('2', '$'), ('$', 'for'), ('for', 'using'), ('using', 'with'), ('with', 'cross'), ('cross', '-'), ('-', 'validation'), ('validation', ';'), ('show', 'how'), ('to', 'adjust'), ('adjust', 'the'), ('the', 'coefficient'), ('coefficient', 'of'), ('of', 'determination'), ('determination', '($'), ('($', 'r'), ('2', '$)'), ('$)', 'when'), ('when', 'used'), ('used', 'for'), ('measuring', 'predictive'), ('predictive', 'accuracy'), ('accuracy', 'via'), ('via', 'leave'), ('leave', '-'), ('-', 'one'), ('-', 'out'), ('out', 'cross'), ('validation', '.'), ('_UNK_', 'approximated'), ('approximated', 'structured'), ('structured', 'prediction'), ('prediction', 'for'), ('for', 'learning'), ('learning', 'large'), ('scale', 'graphical'), ('graphical', 'models'), ('models', ';'), ('this', 'manuscripts'), ('manuscripts', 'contains'), ('the', 'proofs'), ('proofs', 'for'), ('for', '\"'), ('a', 'primal'), ('primal', '-'), ('dual', 'message'), ('message', '-'), ('-', 'passing'), ('passing', 'algorithm'), ('algorithm', 'for'), ('for', 'approximated'), ('approximated', 'large'), ('scale', 'structured'), ('prediction', '\".'), ('automatic', 'liver'), ('liver', 'segmentation'), ('segmentation', 'method'), ('method', 'in'), ('in', 'ct'), ('ct', 'images'), ('the', 'aim'), ('aim', 'of'), ('work', 'is'), ('develop', 'a'), ('method', 'for'), ('for', 'automatic'), ('automatic', 'segmentation'), ('segmentation', 'of'), ('the', 'liver'), ('liver', 'based'), ('a', 'priori'), ('priori', 'knowledge'), ('knowledge', 'of'), ('the', 'image'), ('image', ','), (',', 'such'), ('such', 'as'), ('as', 'location'), ('location', 'and'), ('and', 'shape'), ('shape', 'of'), ('liver', '.'), ('the', 'existence'), ('existence', 'of'), ('a', 'projective'), ('projective', 'reconstruction'), ('we', 'study'), ('the', 'connection'), ('connection', 'between'), ('between', 'the'), ('reconstruction', 'and'), ('a', 'fundamental'), ('fundamental', 'matrix'), ('matrix', 'satisfying'), ('satisfying', 'the'), ('the', 'epipolar'), ('epipolar', 'constraints'), ('constraints', '.'), ('_UNK_', 'solving'), ('solving', 'traveling'), ('traveling', 'salesman'), ('salesman', 'problem'), ('problem', 'by'), ('by', 'marker'), ('marker', 'method'), ('method', ';'), ('use', 'marker'), ('method', 'and'), ('and', 'propose'), ('new', 'mutation'), ('mutation', 'operator'), ('operator', 'that'), ('that', 'selects'), ('selects', 'the'), ('the', 'nearest'), ('nearest', 'neighbor'), ('neighbor', 'among'), ('among', 'all'), ('all', 'near'), ('near', 'neighbors'), ('neighbors', 'solving'), ('problem', '.'), ('a', 'primer'), ('primer', 'on'), ('on', 'answer'), ('a', 'introduction'), ('the', 'syntax'), ('syntax', 'and'), ('and', 'semantics'), ('of', 'answer'), ('programming', 'intended'), ('intended', 'as'), ('as', 'an'), ('an', 'handout'), ('handout', 'to'), ('to', '['), ('[', 'under'), ('under', ']'), (']', 'graduate'), ('graduate', 'students'), ('students', 'taking'), ('taking', 'artificial'), ('artificial', 'intlligence'), ('intlligence', 'or'), ('or', 'logic'), ('programming', 'classes'), ('classes', '.'), ('_UNK_', 'agent'), ('agent', 'models'), ('models', 'of'), ('of', 'political'), ('political', 'interactions'), ('interactions', ';'), (';', 'looks'), ('looks', 'at'), ('at', 'state'), ('state', 'interactions'), ('interactions', 'from'), ('from', 'an'), ('an', 'agent'), ('agent', 'based'), ('based', 'ai'), ('ai', 'perspective'), ('perspective', 'to'), ('to', 'see'), ('see', 'state'), ('interactions', 'as'), ('an', 'example'), ('example', 'of'), ('of', 'emergent'), ('emergent', 'intelligent'), ('intelligent', 'behavior'), ('behavior', '.'), ('.', 'exposes'), ('exposes', 'basic'), ('basic', 'principles'), ('principles', 'of'), ('of', 'game'), ('learning', 'states'), ('states', 'representations'), ('representations', 'in'), ('in', 'pomdp'), ('pomdp', ';'), ('propose', 'to'), ('to', 'deal'), ('deal', 'with'), ('with', 'sequential'), ('sequential', 'processes'), ('processes', 'where'), ('where', 'only'), ('only', 'partial'), ('partial', 'observations'), ('observations', 'are'), ('are', 'available'), ('available', 'by'), ('by', 'learning'), ('learning', 'a'), ('a', 'latent'), ('latent', 'representation'), ('representation', 'space'), ('space', 'on'), ('on', 'which'), ('which', 'policies'), ('policies', 'may'), ('may', 'be'), ('be', 'accurately'), ('accurately', 'learned'), ('learned', '.')])\n",
            "dict_items([(('_UNK_', '_UNK_'), Counter({'a': 13, 'the': 3, 'on': 3, 'using': 2, 'learning': 2, 'automatic': 2, 'why': 2, 'proceedings': 2, 'piecewise': 2, 'differential': 1, 'what': 1, 'p': 1, 'computational': 1, 'weak': 1, 'creating': 1, 'defeasible': 1, 'essence': 1, 'deep': 1, 'statistical': 1, 'complex': 1, 'serious': 1, 'preprocessing': 1, 'liquid': 1, 'mining': 1, 'towards': 1, 'icon': 1, 'recognition': 1, 'glottochronologic': 1, 'utility': 1, 'temporized': 1, 'backpropagation': 1, 'random': 1, 'network': 1, 'glottochronology': 1, 'time': 1, 'convolutional': 1, 'fitness': 1, 'flip': 1, 'autonomous': 1, 'activitynet': 1, 'decision': 1, 'text': 1, 'discrimination': 1, 'are': 1, 'extraction': 1, 'comments': 1, 'resource': 1, 'advances': 1, 'exploration': 1, 'quantified': 1, 'in': 1, 'introduction': 1, 'beyond': 1, 'norm': 1, 'about': 1, 'unary': 1, 'some': 1, 'convex': 1, 'neurocontrol': 1, 'philosophy': 1, 'parallels': 1, 'an': 1, 'calculate': 1, 'group': 1, 'entropy': 1, 'word': 1, 'guarded': 1, 'cornell': 1, 'semistability': 1, 'attack': 1, 'standardization': 1, 'defensive': 1, 'how': 1, 'technical': 1, 'sat': 1, 'approximated': 1, 'solving': 1, 'agent': 1})), (('_UNK_', 'differential'), Counter({'contrastive': 1})), (('differential', 'contrastive'), Counter({'divergence': 1})), (('contrastive', 'divergence'), Counter({';': 1})), (('divergence', ';'), Counter({'this': 1})), ((';', 'this'), Counter({'paper': 19, 'is': 5, 'article': 3, 'submission': 2, 'preprint': 1, 'abstract': 1, 'essay': 1, 'short': 1, 'chapter': 1, 'sequential': 1, 'manuscripts': 1})), (('this', 'paper'), Counter({'has': 14, 'presents': 3, 'we': 3, 'specifies': 1, ',': 1, 'describes': 1})), (('paper', 'has'), Counter({'been': 14})), (('has', 'been'), Counter({'withdrawn': 16, 'retracted': 1})), (('been', 'retracted'), Counter({'.': 1})), (('retracted', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'what'), Counter({'does': 1})), (('what', 'does'), Counter({'artificial': 1})), (('does', 'artificial'), Counter({'life': 1})), (('artificial', 'life'), Counter({'tell': 1, 'journal': 1})), (('life', 'tell'), Counter({'us': 1})), (('tell', 'us'), Counter({'about': 1})), (('us', 'about'), Counter({'death': 1})), (('about', 'death'), Counter({'?': 1})), (('death', '?'), Counter({';': 1})), (('?', ';'), Counter({'this': 2, 'short': 1, 'computer': 1})), ((';', 'short'), Counter({'philosophical': 1})), (('short', 'philosophical'), Counter({'essay': 1})), (('philosophical', 'essay'), Counter({'_EOS_': 1})), (('_UNK_', 'p'), Counter({'=': 1})), (('p', '='), Counter({'np': 2})), (('=', 'np'), Counter({';': 1, '.': 1})), (('np', ';'), Counter({'we': 1})), ((';', 'we'), Counter({'show': 4, 'propose': 3, 'present': 2, 'investigate': 2, 'claim': 1, 'prove': 1, 'use': 1, 'derive': 1, 'describe': 1})), (('we', 'claim'), Counter({'to': 1})), (('claim', 'to'), Counter({'resolve': 1})), (('to', 'resolve'), Counter({'the': 1})), (('resolve', 'the'), Counter({'p': 1})), (('the', 'p'), Counter({'=?': 1})), (('p', '=?'), Counter({'np': 1})), (('=?', 'np'), Counter({'problem': 1})), (('np', 'problem'), Counter({'via': 1})), (('problem', 'via'), Counter({'a': 1})), (('via', 'a'), Counter({'formal': 1})), (('a', 'formal'), Counter({'argument': 1})), (('formal', 'argument'), Counter({'for': 1})), (('argument', 'for'), Counter({'p': 1})), (('for', 'p'), Counter({'=': 1})), (('np', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'computational'), Counter({'geometry': 1})), (('computational', 'geometry'), Counter({'column': 1})), (('geometry', 'column'), Counter({'38': 1})), (('column', '38'), Counter({';': 1})), (('38', ';'), Counter({'recent': 1})), ((';', 'recent'), Counter({'results': 1})), (('recent', 'results'), Counter({'on': 1})), (('results', 'on'), Counter({'curve': 1})), (('on', 'curve'), Counter({'reconstruction': 1})), (('curve', 'reconstruction'), Counter({'are': 1})), (('reconstruction', 'are'), Counter({'described': 1})), (('are', 'described'), Counter({'.': 1, ',': 1})), (('described', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'weak'), Counter({'evolvability': 1})), (('weak', 'evolvability'), Counter({'equals': 1})), (('evolvability', 'equals'), Counter({'strong': 1})), (('equals', 'strong'), Counter({'evolvability': 1})), (('strong', 'evolvability'), Counter({';': 1})), (('evolvability', ';'), Counter({'an': 1})), ((';', 'an'), Counter({'updated': 1})), (('an', 'updated'), Counter({'version': 1})), (('updated', 'version'), Counter({'will': 1})), (('version', 'will'), Counter({'be': 2})), (('will', 'be'), Counter({'uploaded': 1, 'modified': 1, 'removed': 1, 'posted': 1})), (('be', 'uploaded'), Counter({'later': 1})), (('uploaded', 'later'), Counter({'.': 1})), (('later', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'creating'), Counter({'a': 1})), (('creating', 'a'), Counter({'new': 2})), (('a', 'new'), Counter({'ontology': 2, 'combination': 2, 'bengali': 1, 'type': 1, 'mutation': 1})), (('new', 'ontology'), Counter({':': 2})), (('ontology', ':'), Counter({'a': 2})), ((':', 'a'), Counter({'modular': 2, 'gravitational': 2, 'step': 1, 'tool': 1})), (('a', 'modular'), Counter({'approach': 2})), (('modular', 'approach'), Counter({';': 1, '_EOS_': 1})), (('approach', ';'), Counter({'creating': 1})), ((';', 'creating'), Counter({'a': 1})), (('_UNK_', 'defeasible'), Counter({'reasoning': 1})), (('defeasible', 'reasoning'), Counter({'in': 1})), (('reasoning', 'in'), Counter({'oscar': 1})), (('in', 'oscar'), Counter({';': 1})), (('oscar', ';'), Counter({'this': 1})), (('this', 'is'), Counter({'a': 3, 'the': 2})), (('is', 'a'), Counter({'system': 1, 'duplicate': 1, 'tutorial': 1, 'learning': 1})), (('a', 'system'), Counter({'description': 1})), (('system', 'description'), Counter({'for': 1})), (('description', 'for'), Counter({'the': 1})), (('for', 'the'), Counter({'jaccard': 2, 'oscar': 1, 'yahoo': 1, 'nlpcc': 1, 'completeness': 1})), (('the', 'oscar'), Counter({'defeasible': 1})), (('oscar', 'defeasible'), Counter({'reasoner': 1})), (('defeasible', 'reasoner'), Counter({'.': 1})), (('reasoner', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'essence'), Counter({\"'\": 1})), (('essence', \"'\"), Counter({'description': 1, 'language': 1})), ((\"'\", 'description'), Counter({';': 1})), (('description', ';'), Counter({'a': 1})), ((';', 'a'), Counter({'description': 2, 'glottochronologic': 1, 'method': 1, 'semantic': 1, 'short': 1, 'survey': 1, 'introduction': 1})), (('a', 'description'), Counter({'of': 1, 'and': 1})), (('description', 'of'), Counter({'the': 1})), (('of', 'the'), Counter({'ukrainian': 2, 'first': 2, 'liver': 2, 'essence': 1, 'icon': 1, 'network': 1, 'final': 1, 'slides': 1, '2d': 1, 'ideal': 1, 'cyrillic': 1, 'algorithm': 1, 'term': 1, 'experiments': 1, 'entropy': 1, 'telugu': 1, 'previous': 1, 'formal': 1, 'triangle': 1, 'image': 1})), (('the', 'essence'), Counter({\"'\": 1})), ((\"'\", 'language'), Counter({'as': 1})), (('language', 'as'), Counter({'used': 1})), (('as', 'used'), Counter({'by': 1})), (('used', 'by'), Counter({'the': 1})), (('by', 'the'), Counter({'author': 9, 'authors': 2, 'tool': 1, 'corresponding': 1})), (('the', 'tool'), Counter({'savile': 1})), (('tool', 'savile'), Counter({'row': 1})), (('savile', 'row'), Counter({'.': 1})), (('row', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'deep'), Counter({'neural': 1})), (('deep', 'neural'), Counter({'networks': 2})), (('neural', 'networks'), Counter({'.': 2, '-': 1, 'and': 1, ';': 1, 'to': 1})), (('networks', '-'), Counter({'a': 1})), (('-', 'a'), Counter({'brief': 1})), (('a', 'brief'), Counter({'history': 1})), (('brief', 'history'), Counter({';': 1})), (('history', ';'), Counter({'introduction': 1})), ((';', 'introduction'), Counter({'to': 2})), (('introduction', 'to'), Counter({'the': 3, 'deep': 1})), (('to', 'deep'), Counter({'neural': 1})), (('networks', 'and'), Counter({'their': 1})), (('and', 'their'), Counter({'history': 1, 'meaning': 1})), (('their', 'history'), Counter({'.': 1})), (('history', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'statistical'), Counter({'physics': 1})), (('statistical', 'physics'), Counter({'for': 1})), (('physics', 'for'), Counter({'natural': 1})), (('for', 'natural'), Counter({'language': 1})), (('natural', 'language'), Counter({'processing': 2, 'to': 1})), (('language', 'processing'), Counter({';': 2, '.': 1})), (('processing', ';'), Counter({'this': 2})), (('been', 'withdrawn'), Counter({'by': 12, '.': 2, '_EOS_': 1, 'due': 1})), (('withdrawn', 'by'), Counter({'the': 10, 'arxiv': 2})), (('the', 'author'), Counter({'due': 4, 'ali': 2, '.': 1, 'because': 1, 'for': 1, 'uses': 1})), (('author', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'complex'), Counter({'networks': 1})), (('complex', 'networks'), Counter({'\"': 2, ';': 1, ',': 1})), (('networks', ';'), Counter({'introduction': 1, 'we': 1, 'this': 1})), (('to', 'the'), Counter({'26th': 2, 'special': 1, 'object': 1, 'level': 1, 'work': 1, 'higher': 1, 'syntax': 1})), (('the', 'special'), Counter({'issue': 1})), (('special', 'issue'), Counter({'on': 1, ';': 1, '_EOS_': 1})), (('issue', 'on'), Counter({'complex': 1})), (('on', 'complex'), Counter({'networks': 1})), (('networks', ','), Counter({'artificial': 1})), ((',', 'artificial'), Counter({'life': 1})), (('life', 'journal'), Counter({'.': 1})), (('journal', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'serious'), Counter({'flaws': 1})), (('serious', 'flaws'), Counter({'in': 1})), (('flaws', 'in'), Counter({'korf': 1})), (('in', 'korf'), Counter({'et': 1})), (('korf', 'et'), Counter({'al': 1})), (('et', 'al'), Counter({\".'\": 1})), (('al', \".'\"), Counter({'s': 1})), ((\".'\", 's'), Counter({'analysis': 1})), (('s', 'analysis'), Counter({'on': 1})), (('analysis', 'on'), Counter({'time': 1})), (('on', 'time'), Counter({'complexity': 1})), (('time', 'complexity'), Counter({'of': 1})), (('complexity', 'of'), Counter({'a': 1})), (('of', 'a'), Counter({'projective': 2, '*': 1, ')': 1, 'general': 1, 'human': 1, 'neural': 1, 'dna': 1, 'company': 1, '\"': 1, 'realistic': 1, 'fundamental': 1})), (('a', '*'), Counter({';': 1})), (('*', ';'), Counter({'this': 1})), (('withdrawn', '.'), Counter({'_EOS_': 2})), (('_UNK_', 'preprocessing'), Counter({':': 1})), (('preprocessing', ':'), Counter({'a': 1})), (('a', 'step'), Counter({'in': 1})), (('step', 'in'), Counter({'automating': 1})), (('in', 'automating'), Counter({'early': 1})), (('automating', 'early'), Counter({'detection': 1})), (('early', 'detection'), Counter({'of': 1})), (('detection', 'of'), Counter({'cervical': 1})), (('of', 'cervical'), Counter({'cancer': 1})), (('cervical', 'cancer'), Counter({';': 1})), (('cancer', ';'), Counter({'this': 1})), (('_UNK_', 'liquid'), Counter({'state': 1})), (('liquid', 'state'), Counter({'machines': 1})), (('state', 'machines'), Counter({'in': 1})), (('machines', 'in'), Counter({'adbiatic': 1})), (('in', 'adbiatic'), Counter({'quantum': 1})), (('adbiatic', 'quantum'), Counter({'computers': 1})), (('quantum', 'computers'), Counter({'for': 1})), (('computers', 'for'), Counter({'general': 1})), (('for', 'general'), Counter({'computation': 1})), (('general', 'computation'), Counter({';': 1})), (('computation', ';'), Counter({'major': 1})), ((';', 'major'), Counter({'mistakes': 1})), (('major', 'mistakes'), Counter({'do': 1})), (('mistakes', 'do'), Counter({'not': 1})), (('do', 'not'), Counter({'read': 1})), (('not', 'read'), Counter({'_EOS_': 1})), (('_UNK_', 'mining'), Counter({'for': 1})), (('mining', 'for'), Counter({'trees': 2})), (('for', 'trees'), Counter({'in': 2})), (('trees', 'in'), Counter({'a': 2})), (('in', 'a'), Counter({'graph': 2, 'playful': 1})), (('a', 'graph'), Counter({'is': 2})), (('graph', 'is'), Counter({'np': 1, 'shown': 1})), (('is', 'np'), Counter({'-': 1})), (('np', '-'), Counter({'complete': 2})), (('-', 'complete'), Counter({';': 1, '.': 1})), (('complete', ';'), Counter({'mining': 1})), ((';', 'mining'), Counter({'for': 1})), (('is', 'shown'), Counter({'to': 1, 'that': 1})), (('shown', 'to'), Counter({'be': 1})), (('to', 'be'), Counter({'np': 1})), (('be', 'np'), Counter({'-': 1})), (('complete', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'towards'), Counter({'a': 1})), (('towards', 'a'), Counter({'hierarchical': 1})), (('a', 'hierarchical'), Counter({'model': 1})), (('hierarchical', 'model'), Counter({'of': 1})), (('model', 'of'), Counter({'a': 2, 'consciousness': 1, 'quantum': 1})), (('of', 'consciousness'), Counter({',': 1})), (('consciousness', ','), Counter({'intelligence': 1})), ((',', 'intelligence'), Counter({',': 1})), (('intelligence', ','), Counter({'mind': 1})), ((',', 'mind'), Counter({'and': 1})), (('mind', 'and'), Counter({'body': 1})), (('and', 'body'), Counter({';': 1})), (('body', ';'), Counter({'this': 1})), (('this', 'article'), Counter({'is': 2, 'aims': 1, ',': 1, 'has': 1})), (('article', 'is'), Counter({'taken': 1, 'written': 1})), (('is', 'taken'), Counter({'out': 1})), (('taken', 'out'), Counter({'.': 1})), (('out', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'a'), Counter({'note': 3, 'machine': 2, 'notation': 1, 'comment': 1, 'theory': 1, 'survey': 1, 'history': 1, 'new': 1, 'remark': 1, 'primer': 1})), (('a', 'notation'), Counter({'for': 2})), (('notation', 'for'), Counter({'markov': 2})), (('for', 'markov'), Counter({'decision': 2})), (('markov', 'decision'), Counter({'processes': 2})), (('decision', 'processes'), Counter({';': 1, '.': 1})), (('processes', ';'), Counter({'this': 1})), (('paper', 'specifies'), Counter({'a': 1})), (('specifies', 'a'), Counter({'notation': 1})), (('processes', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'icon'), Counter({'challenge': 1})), (('icon', 'challenge'), Counter({'on': 2})), (('challenge', 'on'), Counter({'algorithm': 2})), (('on', 'algorithm'), Counter({'selection': 2})), (('algorithm', 'selection'), Counter({';': 1, '.': 1})), (('selection', ';'), Counter({'we': 1})), (('we', 'present'), Counter({'the': 1, 'our': 1})), (('present', 'the'), Counter({'results': 1})), (('the', 'results'), Counter({'of': 1})), (('results', 'of'), Counter({'the': 1, 'object': 1})), (('the', 'icon'), Counter({'challenge': 1})), (('selection', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'recognition'), Counter({'of': 1})), (('recognition', 'of'), Counter({'regular': 1})), (('of', 'regular'), Counter({'shapes': 1})), (('regular', 'shapes'), Counter({'in': 1})), (('shapes', 'in'), Counter({'satelite': 1})), (('in', 'satelite'), Counter({'images': 1})), (('satelite', 'images'), Counter({';': 1})), (('images', ';'), Counter({'this': 1, 'the': 1})), (('author', 'ali'), Counter({'pourmohammad': 2})), (('ali', 'pourmohammad'), Counter({'.': 2})), (('pourmohammad', '.'), Counter({'_EOS_': 2})), (('_UNK_', 'glottochronologic'), Counter({'retrognostic': 1})), (('glottochronologic', 'retrognostic'), Counter({'of': 2})), (('retrognostic', 'of'), Counter({'language': 2})), (('of', 'language'), Counter({'system': 2})), (('language', 'system'), Counter({';': 1, 'is': 1})), (('system', ';'), Counter({'a': 1, 'we': 1})), (('a', 'glottochronologic'), Counter({'retrognostic': 1})), (('system', 'is'), Counter({'proposed': 1})), (('is', 'proposed'), Counter({'_EOS_': 1, '.': 1})), (('_UNK_', 'the'), Counter({'model': 1, 'yahoo': 1, 'logic': 1})), (('the', 'model'), Counter({'of': 1})), (('of', 'quantum'), Counter({'evolution': 1})), (('quantum', 'evolution'), Counter({';': 1})), (('evolution', ';'), Counter({'this': 2})), (('author', 'due'), Counter({'to': 4})), (('due', 'to'), Counter({'a': 3, 'extremely': 1, 'an': 1, 'some': 1})), (('to', 'extremely'), Counter({'unscientific': 1})), (('extremely', 'unscientific'), Counter({'errors': 1})), (('unscientific', 'errors'), Counter({'.': 1})), (('errors', '.'), Counter({'_EOS_': 2})), (('_UNK_', 'utility'), Counter({'-': 1})), (('utility', '-'), Counter({'probability': 1})), (('-', 'probability'), Counter({'duality': 1})), (('probability', 'duality'), Counter({';': 1})), (('duality', ';'), Counter({'this': 1})), (('paper', 'presents'), Counter({'duality': 1, 'some': 1, 'an': 1})), (('presents', 'duality'), Counter({'between': 1})), (('duality', 'between'), Counter({'probability': 1})), (('between', 'probability'), Counter({'distributions': 1})), (('probability', 'distributions'), Counter({'and': 1})), (('distributions', 'and'), Counter({'utility': 1})), (('and', 'utility'), Counter({'functions': 1})), (('utility', 'functions'), Counter({'.': 1})), (('functions', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'temporized'), Counter({'equilibria': 1})), (('temporized', 'equilibria'), Counter({';': 1})), (('equilibria', ';'), Counter({'this': 1})), (('to', 'a'), Counter({'crucial': 2, 'major': 1})), (('a', 'crucial'), Counter({'error': 1, 'sign': 1})), (('crucial', 'error'), Counter({'in': 1})), (('error', 'in'), Counter({'the': 1, 'eq': 1, 'equation': 1})), (('in', 'the'), Counter({'future': 2, 'submission': 1, 'translation': 1, 'face': 1, 'behavior': 1, 'area': 1})), (('the', 'submission'), Counter({'action': 1})), (('submission', 'action'), Counter({'.': 1})), (('action', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'backpropagation'), Counter({'in': 1})), (('backpropagation', 'in'), Counter({'matrix': 1})), (('in', 'matrix'), Counter({'notation': 2})), (('matrix', 'notation'), Counter({';': 1, '.': 1})), (('notation', ';'), Counter({'in': 1})), ((';', 'in'), Counter({'this': 10})), (('in', 'this'), Counter({'paper': 4, 'note': 2, 'work': 1, 'article': 1, 'survey': 1, 'manuscript': 1})), (('this', 'note'), Counter({'we': 2})), (('note', 'we'), Counter({'calculate': 1, 'study': 1})), (('we', 'calculate'), Counter({'the': 1})), (('calculate', 'the'), Counter({'gradient': 1, 'distance': 1})), (('the', 'gradient'), Counter({'of': 1})), (('gradient', 'of'), Counter({'the': 1})), (('the', 'network'), Counter({'function': 1})), (('network', 'function'), Counter({'in': 1})), (('function', 'in'), Counter({'matrix': 1})), (('notation', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'random'), Counter({'dfas': 1})), (('random', 'dfas'), Counter({'are': 1})), (('dfas', 'are'), Counter({'efficiently': 1})), (('are', 'efficiently'), Counter({'pac': 1})), (('efficiently', 'pac'), Counter({'learnable': 1})), (('pac', 'learnable'), Counter({';': 1})), (('learnable', ';'), Counter({'this': 1})), (('withdrawn', 'due'), Counter({'to': 1})), (('to', 'an'), Counter({'error': 1})), (('an', 'error'), Counter({'found': 1})), (('error', 'found'), Counter({'by': 1})), (('found', 'by'), Counter({'dana': 1})), (('by', 'dana'), Counter({'angluin': 1})), (('dana', 'angluin'), Counter({'and': 1})), (('angluin', 'and'), Counter({'lev': 1})), (('and', 'lev'), Counter({'reyzin': 1})), (('lev', 'reyzin'), Counter({'.': 1})), (('reyzin', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'network'), Counter({'motifs': 1})), (('network', 'motifs'), Counter({'in': 1})), (('motifs', 'in'), Counter({'music': 1})), (('in', 'music'), Counter({'sequences': 1})), (('music', 'sequences'), Counter({';': 1})), (('sequences', ';'), Counter({'this': 1})), (('author', 'because'), Counter({'it': 1})), (('because', 'it'), Counter({'needs': 1, 'is': 1})), (('it', 'needs'), Counter({'a': 1})), (('needs', 'a'), Counter({'deep': 1})), (('a', 'deep'), Counter({'methodological': 1})), (('deep', 'methodological'), Counter({'revision': 1})), (('methodological', 'revision'), Counter({'.': 1})), (('revision', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'glottochronology'), Counter({'and': 1})), (('glottochronology', 'and'), Counter({'problems': 1})), (('and', 'problems'), Counter({'of': 1})), (('problems', 'of'), Counter({'protolanguage': 1})), (('of', 'protolanguage'), Counter({'reconstruction': 1})), (('protolanguage', 'reconstruction'), Counter({';': 1})), (('reconstruction', ';'), Counter({'a': 1, 'in': 1})), (('a', 'method'), Counter({'of': 1, 'for': 1})), (('method', 'of'), Counter({'languages': 1, 'using': 1})), (('of', 'languages'), Counter({'genealogical': 1})), (('languages', 'genealogical'), Counter({'trees': 1})), (('genealogical', 'trees'), Counter({'construction': 1})), (('trees', 'construction'), Counter({'is': 1})), (('construction', 'is'), Counter({'proposed': 1})), (('proposed', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'using'), Counter({'slp': 1, 'sets': 1})), (('using', 'slp'), Counter({'neural': 1})), (('slp', 'neural'), Counter({'network': 1})), (('neural', 'network'), Counter({'to': 1, 'inspired': 1, 'learning': 1, 'building': 1})), (('network', 'to'), Counter({'persian': 1})), (('to', 'persian'), Counter({'handwritten': 1})), (('persian', 'handwritten'), Counter({'digits': 1})), (('handwritten', 'digits'), Counter({'recognition': 1})), (('digits', 'recognition'), Counter({';': 1})), (('recognition', ';'), Counter({'this': 1})), (('_UNK_', 'time'), Counter({'hopping': 1})), (('time', 'hopping'), Counter({'technique': 1})), (('hopping', 'technique'), Counter({'for': 1})), (('technique', 'for'), Counter({'faster': 1})), (('for', 'faster'), Counter({'reinforcement': 1})), (('faster', 'reinforcement'), Counter({'learning': 1})), (('reinforcement', 'learning'), Counter({'in': 1})), (('learning', 'in'), Counter({'simulations': 1})), (('in', 'simulations'), Counter({';': 1})), (('simulations', ';'), Counter({'this': 1})), (('this', 'preprint'), Counter({'has': 1})), (('preprint', 'has'), Counter({'been': 1})), (('author', 'for'), Counter({'revision': 1})), (('for', 'revision'), Counter({'_EOS_': 1})), (('_UNK_', 'convolutional'), Counter({'matching': 1})), (('convolutional', 'matching'), Counter({'pursuit': 1})), (('matching', 'pursuit'), Counter({'and': 2})), (('pursuit', 'and'), Counter({'dictionary': 1, 'k': 1})), (('and', 'dictionary'), Counter({'training': 1})), (('dictionary', 'training'), Counter({';': 1})), (('training', ';'), Counter({'matching': 1})), ((';', 'matching'), Counter({'pursuit': 1})), (('and', 'k'), Counter({'-': 1})), (('k', '-'), Counter({'svd': 1})), (('-', 'svd'), Counter({'is': 1})), (('svd', 'is'), Counter({'demonstrated': 1})), (('is', 'demonstrated'), Counter({'in': 1})), (('demonstrated', 'in'), Counter({'the': 1})), (('the', 'translation'), Counter({'invariant': 1})), (('translation', 'invariant'), Counter({'setting': 1})), (('invariant', 'setting'), Counter({'_EOS_': 1})), (('_UNK_', 'fitness'), Counter({'landscape': 1})), (('fitness', 'landscape'), Counter({'analysis': 1})), (('landscape', 'analysis'), Counter({'for': 1})), (('analysis', 'for'), Counter({'dynamic': 1, 'paracontracting': 1})), (('for', 'dynamic'), Counter({'resource': 1})), (('dynamic', 'resource'), Counter({'allocation': 1})), (('resource', 'allocation'), Counter({'in': 1, 'of': 1})), (('allocation', 'in'), Counter({'multiuser': 1})), (('in', 'multiuser'), Counter({'ofdm': 1})), (('multiuser', 'ofdm'), Counter({'based': 1})), (('ofdm', 'based'), Counter({'cognitive': 2})), (('based', 'cognitive'), Counter({'radio': 2})), (('cognitive', 'radio'), Counter({'systems': 2})), (('radio', 'systems'), Counter({';': 1, 'under': 1})), (('systems', ';'), Counter({'this': 1})), (('a', 'note'), Counter({'on': 3})), (('note', 'on'), Counter({'darwiche': 1, 'the': 1, 'adjusting': 1})), (('on', 'darwiche'), Counter({'and': 1})), (('darwiche', 'and'), Counter({'pearl': 2})), (('and', 'pearl'), Counter({';': 1, \"'\": 1})), (('pearl', ';'), Counter({'it': 1})), ((';', 'it'), Counter({'is': 1})), (('it', 'is'), Counter({'shown': 1, 'complete': 1, 'no': 1, 'intentionally': 1})), (('shown', 'that'), Counter({'darwiche': 1})), (('that', 'darwiche'), Counter({'and': 1})), (('pearl', \"'\"), Counter({'s': 1})), ((\"'\", 's'), Counter({'postulates': 1, 'assumption': 1})), (('s', 'postulates'), Counter({'imply': 1})), (('postulates', 'imply'), Counter({'an': 1})), (('imply', 'an'), Counter({'interesting': 1})), (('an', 'interesting'), Counter({'property': 1})), (('interesting', 'property'), Counter({',': 1})), (('property', ','), Counter({'not': 1})), ((',', 'not'), Counter({'noticed': 1})), (('not', 'noticed'), Counter({'by': 1})), (('noticed', 'by'), Counter({'the': 1})), (('the', 'authors'), Counter({'.': 1, 'due': 1})), (('authors', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'flip'), Counter({'-': 1})), (('flip', '-'), Counter({'flop': 1})), (('-', 'flop'), Counter({'sublinear': 1})), (('flop', 'sublinear'), Counter({'models': 1})), (('sublinear', 'models'), Counter({'for': 1, 'on': 1})), (('models', 'for'), Counter({'graphs': 1})), (('for', 'graphs'), Counter({':': 1})), (('graphs', ':'), Counter({'proof': 1})), ((':', 'proof'), Counter({'of': 1})), (('proof', 'of'), Counter({'theorem': 1})), (('of', 'theorem'), Counter({'1': 1})), (('theorem', '1'), Counter({';': 1})), (('1', ';'), Counter({'we': 1})), (('we', 'prove'), Counter({'that': 1})), (('prove', 'that'), Counter({'there': 1})), (('that', 'there'), Counter({'is': 1})), (('there', 'is'), Counter({'no': 1})), (('is', 'no'), Counter({'class': 1, 'more': 1})), (('no', 'class'), Counter({'-': 1})), (('class', '-'), Counter({'dual': 1})), (('-', 'dual'), Counter({'for': 1, 'message': 1})), (('dual', 'for'), Counter({'almost': 1})), (('for', 'almost'), Counter({'all': 1})), (('almost', 'all'), Counter({'sublinear': 1})), (('all', 'sublinear'), Counter({'models': 1})), (('models', 'on'), Counter({'graphs': 1})), (('on', 'graphs'), Counter({'.': 1})), (('graphs', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'autonomous'), Counter({'perceptron': 1})), (('autonomous', 'perceptron'), Counter({'neural': 1})), (('perceptron', 'neural'), Counter({'network': 1})), (('network', 'inspired'), Counter({'from': 1})), (('inspired', 'from'), Counter({'quantum': 1})), (('from', 'quantum'), Counter({'computing': 1})), (('quantum', 'computing'), Counter({';': 1})), (('computing', ';'), Counter({'this': 1})), (('this', 'abstract'), Counter({'will': 1})), (('abstract', 'will'), Counter({'be': 1})), (('be', 'modified'), Counter({'after': 1})), (('modified', 'after'), Counter({'correcting': 1})), (('after', 'correcting'), Counter({'the': 1})), (('correcting', 'the'), Counter({'minor': 1})), (('the', 'minor'), Counter({'error': 1})), (('minor', 'error'), Counter({'in': 1})), (('in', 'eq'), Counter({'.(': 1})), (('eq', '.('), Counter({'2': 1})), (('.(', '2'), Counter({')': 1})), (('2', ')'), Counter({'_EOS_': 1})), (('using', 'sets'), Counter({'of': 1})), (('sets', 'of'), Counter({'probability': 2})), (('of', 'probability'), Counter({'measures': 2})), (('probability', 'measures'), Counter({'to': 1, 'as': 1})), (('measures', 'to'), Counter({'represent': 1})), (('to', 'represent'), Counter({'uncertainty': 1})), (('represent', 'uncertainty'), Counter({';': 1})), (('uncertainty', ';'), Counter({'i': 1, 'we': 1})), ((';', 'i'), Counter({'explore': 1})), (('i', 'explore'), Counter({'the': 1})), (('explore', 'the'), Counter({'use': 1})), (('the', 'use'), Counter({'of': 1})), (('use', 'of'), Counter({'sets': 1, 'the': 1})), (('of', 'sets'), Counter({'of': 1})), (('measures', 'as'), Counter({'a': 1})), (('as', 'a'), Counter({'representation': 1, 'presentation': 1, 'game': 1})), (('a', 'representation'), Counter({'of': 1})), (('representation', 'of'), Counter({'uncertainty': 1, 'its': 1, 'lexical': 1, 'sat': 1})), (('of', 'uncertainty'), Counter({'.': 1})), (('uncertainty', '.'), Counter({'_EOS_': 2})), (('a', 'comment'), Counter({'on': 1})), (('comment', 'on'), Counter({'\"': 2, 'argumentation': 1})), (('on', 'argumentation'), Counter({';': 1})), (('argumentation', ';'), Counter({'we': 1})), (('we', 'use'), Counter({'the': 1, 'group': 1, 'marker': 1})), (('use', 'the'), Counter({'theory': 1})), (('the', 'theory'), Counter({'of': 1})), (('theory', 'of'), Counter({'defaults': 1, 'argumentation': 1, 'experiment': 1})), (('of', 'defaults'), Counter({'and': 1})), (('defaults', 'and'), Counter({'their': 1})), (('their', 'meaning'), Counter({'of': 1})), (('meaning', 'of'), Counter({'[': 1})), (('of', '['), Counter({'gs16': 1})), (('[', 'gs16'), Counter({']': 1})), (('gs16', ']'), Counter({'to': 1})), ((']', 'to'), Counter({'develop': 1})), (('to', 'develop'), Counter({'(': 1, 'a': 1})), (('develop', '('), Counter({'the': 1})), (('(', 'the'), Counter({'outline': 1})), (('the', 'outline'), Counter({'of': 1})), (('outline', 'of'), Counter({'a': 1})), (('a', ')'), Counter({'new': 1})), ((')', 'new'), Counter({'theory': 1})), (('new', 'theory'), Counter({'of': 1})), (('of', 'argumentation'), Counter({'.': 1})), (('argumentation', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'activitynet'), Counter({'challenge': 1})), (('activitynet', 'challenge'), Counter({'2017': 1})), (('challenge', '2017'), Counter({'summary': 2})), (('2017', 'summary'), Counter({';': 1, ':': 1})), (('summary', ';'), Counter({'the': 1})), ((';', 'the'), Counter({'activitynet': 1, 'author': 1, 'development': 1, 'method': 1, 'cornell': 1, 'aim': 1})), (('the', 'activitynet'), Counter({'large': 1})), (('activitynet', 'large'), Counter({'scale': 1})), (('large', 'scale'), Counter({'activity': 1, 'graphical': 1, 'structured': 1})), (('scale', 'activity'), Counter({'recognition': 1})), (('activity', 'recognition'), Counter({'challenge': 1})), (('recognition', 'challenge'), Counter({'2017': 1})), (('summary', ':'), Counter({'results': 1})), ((':', 'results'), Counter({'and': 1})), (('results', 'and'), Counter({'challenge': 1})), (('and', 'challenge'), Counter({'participants': 1})), (('challenge', 'participants'), Counter({'papers': 1})), (('participants', 'papers'), Counter({'.': 1})), (('papers', '.'), Counter({'_EOS_': 1})), (('the', 'yahoo'), Counter({'query': 1, 'webscope': 1})), (('yahoo', 'query'), Counter({'treebank': 1})), (('query', 'treebank'), Counter({',': 2})), (('treebank', ','), Counter({'v': 1, 'version': 1})), ((',', 'v'), Counter({'.': 1})), (('v', '.'), Counter({'1': 1})), (('.', '1'), Counter({'.': 1, 'will': 1})), (('1', '.'), Counter({'0': 2, '3': 1})), (('.', '0'), Counter({';': 1, ',': 1})), (('0', ';'), Counter({'a': 1})), (('description', 'and'), Counter({'annotation': 1})), (('and', 'annotation'), Counter({'guidelines': 1})), (('annotation', 'guidelines'), Counter({'for': 1})), (('guidelines', 'for'), Counter({'the': 1})), (('yahoo', 'webscope'), Counter({'release': 1})), (('webscope', 'release'), Counter({'of': 1})), (('release', 'of'), Counter({'query': 1})), (('of', 'query'), Counter({'treebank': 1})), ((',', 'version'), Counter({'1': 1})), (('version', '1'), Counter({'.': 1})), (('0', ','), Counter({'may': 1})), ((',', 'may'), Counter({'2016': 1, '17': 1})), (('may', '2016'), Counter({'.': 1})), (('2016', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'decision'), Counter({'under': 1})), (('decision', 'under'), Counter({'uncertainty': 1})), (('under', 'uncertainty'), Counter({';': 1})), (('we', 'derive'), Counter({'axiomatically': 1})), (('derive', 'axiomatically'), Counter({'the': 1})), (('axiomatically', 'the'), Counter({'probability': 1})), (('the', 'probability'), Counter({'function': 1})), (('probability', 'function'), Counter({'that': 1})), (('function', 'that'), Counter({'should': 1})), (('that', 'should'), Counter({'be': 1})), (('should', 'be'), Counter({'used': 1})), (('be', 'used'), Counter({'to': 1})), (('used', 'to'), Counter({'make': 1})), (('to', 'make'), Counter({'decisions': 1})), (('make', 'decisions'), Counter({'given': 1})), (('decisions', 'given'), Counter({'any': 1})), (('given', 'any'), Counter({'form': 1})), (('any', 'form'), Counter({'of': 1})), (('form', 'of'), Counter({'underlying': 1})), (('of', 'underlying'), Counter({'uncertainty': 1})), (('underlying', 'uncertainty'), Counter({'.': 1})), (('_UNK_', 'text'), Counter({'analysis': 1})), (('text', 'analysis'), Counter({'tools': 1})), (('analysis', 'tools'), Counter({'in': 1})), (('tools', 'in'), Counter({'spoken': 1})), (('in', 'spoken'), Counter({'language': 1})), (('spoken', 'language'), Counter({'processing': 1})), (('this', 'submission'), Counter({'contains': 1, 'has': 1})), (('submission', 'contains'), Counter({'the': 1})), (('contains', 'the'), Counter({'postscript': 1, 'proofs': 1})), (('the', 'postscript'), Counter({'of': 1})), (('postscript', 'of'), Counter({'the': 1})), (('the', 'final'), Counter({'version': 1})), (('final', 'version'), Counter({'of': 1})), (('version', 'of'), Counter({'the': 2})), (('the', 'slides'), Counter({'used': 1})), (('slides', 'used'), Counter({'in': 1})), (('used', 'in'), Counter({'our': 1})), (('in', 'our'), Counter({'acl': 1})), (('our', 'acl'), Counter({'-': 1})), (('acl', '-'), Counter({'94': 1})), (('-', '94'), Counter({'tutorial': 1})), (('94', 'tutorial'), Counter({'.': 1})), (('tutorial', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'discrimination'), Counter({'between': 1})), (('discrimination', 'between'), Counter({'arabic': 1})), (('between', 'arabic'), Counter({'and': 1})), (('arabic', 'and'), Counter({'latin': 1})), (('and', 'latin'), Counter({'from': 1})), (('latin', 'from'), Counter({'bilingual': 1})), (('from', 'bilingual'), Counter({'documents': 1})), (('bilingual', 'documents'), Counter({';': 1})), (('documents', ';'), Counter({'2011': 1})), ((';', '2011'), Counter({'international': 1})), (('2011', 'international'), Counter({'conference': 1})), (('international', 'conference'), Counter({'on': 3})), (('conference', 'on'), Counter({'logic': 2, 'communications': 1})), (('on', 'communications'), Counter({',': 1})), (('communications', ','), Counter({'computing': 1})), ((',', 'computing'), Counter({'and': 1})), (('computing', 'and'), Counter({'control': 1})), (('and', 'control'), Counter({'applications': 1})), (('control', 'applications'), Counter({'(': 1})), (('applications', '('), Counter({'ccca': 1})), (('(', 'ccca'), Counter({')': 1})), (('ccca', ')'), Counter({'_EOS_': 1})), (('a', 'machine'), Counter({'-': 1, 'learning': 1})), (('machine', '-'), Counter({'learning': 1})), (('-', 'learning'), Counter({'framework': 1})), (('learning', 'framework'), Counter({'for': 1})), (('framework', 'for'), Counter({'design': 1, 'mapping': 1})), (('for', 'design'), Counter({'for': 1})), (('design', 'for'), Counter({'manufacturability': 1})), (('for', 'manufacturability'), Counter({';': 1})), (('manufacturability', ';'), Counter({'this': 1})), (('a', 'duplicate'), Counter({'submission': 1})), (('duplicate', 'submission'), Counter({'(': 1})), (('submission', '('), Counter({'original': 1})), (('(', 'original'), Counter({'is': 1})), (('original', 'is'), Counter({'arxiv': 1})), (('is', 'arxiv'), Counter({':': 1})), (('arxiv', ':'), Counter({'1612': 1, '1306': 1, '0711': 1})), ((':', '1612'), Counter({'.': 1})), (('1612', '.'), Counter({'02141': 1})), (('.', '02141'), Counter({').': 1})), (('02141', ').'), Counter({'hence': 1})), ((').', 'hence'), Counter({'want': 1})), (('hence', 'want'), Counter({'to': 1})), (('want', 'to'), Counter({'withdraw': 1})), (('to', 'withdraw'), Counter({'it': 1})), (('withdraw', 'it'), Counter({'_EOS_': 1})), (('a', 'theory'), Counter({'of': 1})), (('of', 'experiment'), Counter({';': 1})), (('experiment', ';'), Counter({'this': 1})), (('article', 'aims'), Counter({'at': 1})), (('aims', 'at'), Counter({'clarifying': 1})), (('at', 'clarifying'), Counter({'the': 1})), (('clarifying', 'the'), Counter({'language': 1})), (('the', 'language'), Counter({'and': 1})), (('language', 'and'), Counter({'practice': 1})), (('and', 'practice'), Counter({'of': 1})), (('practice', 'of'), Counter({'scientific': 1})), (('of', 'scientific'), Counter({'experiment': 1})), (('scientific', 'experiment'), Counter({',': 1})), (('experiment', ','), Counter({'mainly': 1})), ((',', 'mainly'), Counter({'by': 1})), (('mainly', 'by'), Counter({'hooking': 1})), (('by', 'hooking'), Counter({'observability': 1})), (('hooking', 'observability'), Counter({'on': 1})), (('observability', 'on'), Counter({'calculability': 1})), (('on', 'calculability'), Counter({'.': 1})), (('calculability', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'are'), Counter({'minds': 1})), (('are', 'minds'), Counter({'computable': 1})), (('minds', 'computable'), Counter({'?': 1})), (('computable', '?'), Counter({';': 1})), (('this', 'essay'), Counter({'explores': 1})), (('essay', 'explores'), Counter({'the': 1})), (('explores', 'the'), Counter({'limits': 1})), (('the', 'limits'), Counter({'of': 1})), (('limits', 'of'), Counter({'turing': 1})), (('of', 'turing'), Counter({'machines': 1})), (('turing', 'machines'), Counter({'concerning': 1})), (('machines', 'concerning'), Counter({'the': 1})), (('concerning', 'the'), Counter({'modeling': 1})), (('the', 'modeling'), Counter({'of': 1})), (('modeling', 'of'), Counter({'minds': 1})), (('of', 'minds'), Counter({'and': 1})), (('minds', 'and'), Counter({'suggests': 1})), (('and', 'suggests'), Counter({'alternatives': 1})), (('suggests', 'alternatives'), Counter({'to': 1})), (('alternatives', 'to'), Counter({'go': 1})), (('to', 'go'), Counter({'beyond': 1})), (('go', 'beyond'), Counter({'those': 1})), (('beyond', 'those'), Counter({'limits': 1})), (('those', 'limits'), Counter({'.': 1})), (('limits', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'extraction'), Counter({'de': 1})), (('extraction', 'de'), Counter({'concepts': 1})), (('de', 'concepts'), Counter({'sous': 1})), (('concepts', 'sous'), Counter({'contraintes': 1})), (('sous', 'contraintes'), Counter({'dans': 1})), (('contraintes', 'dans'), Counter({'des': 1})), (('dans', 'des'), Counter({'donn√©es': 1})), (('des', 'donn√©es'), Counter({'d': 1})), (('donn√©es', 'd'), Counter({\"'\": 1})), (('d', \"'\"), Counter({'expression': 1})), ((\"'\", 'expression'), Counter({'de': 1})), (('expression', 'de'), Counter({'g√®nes': 1})), (('de', 'g√®nes'), Counter({';': 1})), (('g√®nes', ';'), Counter({'in': 1})), (('paper', ','), Counter({'we': 1})), ((',', 'we'), Counter({'propose': 1, 'mainly': 1, 'briefly': 1})), (('we', 'propose'), Counter({'a': 3, 'to': 1})), (('propose', 'a'), Counter({'new': 2, 'technique': 1, 'funny': 1})), (('a', 'technique'), Counter({'to': 1})), (('technique', 'to'), Counter({'extract': 1})), (('to', 'extract'), Counter({'constrained': 1})), (('extract', 'constrained'), Counter({'formal': 1})), (('constrained', 'formal'), Counter({'concepts': 1})), (('formal', 'concepts'), Counter({'.': 1})), (('concepts', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'comments'), Counter({'on': 1})), (('comments', 'on'), Counter({'\"': 1, '``': 1})), (('on', '\"'), Counter({'approaching': 2, 'a': 1})), (('\"', 'a'), Counter({'new': 1, 'sense': 1, 'primal': 1})), (('new', 'combination'), Counter({'of': 2})), (('combination', 'of'), Counter({'evidence': 2})), (('of', 'evidence'), Counter({'based': 2})), (('evidence', 'based'), Counter({'on': 2})), (('based', 'on'), Counter({'compromise': 2, 'the': 2, 'a': 1})), (('on', 'compromise'), Counter({'\"': 1, \"''\": 1})), (('compromise', '\"'), Counter({'by': 1})), (('\"', 'by'), Counter({'cong': 2, 'k': 1})), (('by', 'k'), Counter({'.': 2})), (('k', '.'), Counter({'yamada': 2})), (('.', 'yamada'), Counter({';': 1, '_EOS_': 1})), (('yamada', ';'), Counter({'comments': 1})), ((';', 'comments'), Counter({'on': 1})), (('on', '``'), Counter({'a': 1})), (('``', 'a'), Counter({'new': 1})), (('compromise', \"''\"), Counter({'by': 1})), ((\"''\", 'by'), Counter({'k': 1})), (('_UNK_', 'learning'), Counter({'low': 1, 'states': 1})), (('learning', 'low'), Counter({'-': 1})), (('low', '-'), Counter({'shot': 1})), (('-', 'shot'), Counter({'facial': 1, 'face': 1})), (('shot', 'facial'), Counter({'representations': 1})), (('facial', 'representations'), Counter({'via': 1})), (('representations', 'via'), Counter({'2d': 1})), (('via', '2d'), Counter({'warping': 1})), (('2d', 'warping'), Counter({';': 1, 'module': 1})), (('warping', ';'), Counter({'in': 1})), (('this', 'work'), Counter({',': 1, 'is': 1})), (('work', ','), Counter({'we': 1})), (('we', 'mainly'), Counter({'study': 1})), (('mainly', 'study'), Counter({'the': 1})), (('study', 'the'), Counter({'influence': 1, 'connection': 1})), (('the', 'influence'), Counter({'of': 1})), (('influence', 'of'), Counter({'the': 1})), (('the', '2d'), Counter({'warping': 1})), (('warping', 'module'), Counter({'for': 1})), (('module', 'for'), Counter({'one': 1})), (('for', 'one'), Counter({'-': 1})), (('one', '-'), Counter({'shot': 1, 'out': 1})), (('shot', 'face'), Counter({'recognition': 1})), (('face', 'recognition'), Counter({'.': 1})), (('recognition', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'automatic'), Counter({'generation': 1, 'liver': 1})), (('automatic', 'generation'), Counter({'of': 1})), (('generation', 'of'), Counter({'benchmarks': 1})), (('of', 'benchmarks'), Counter({'for': 1})), (('benchmarks', 'for'), Counter({'plagiarism': 1})), (('for', 'plagiarism'), Counter({'detection': 1})), (('plagiarism', 'detection'), Counter({'tools': 1})), (('detection', 'tools'), Counter({'using': 1})), (('tools', 'using'), Counter({'grammatical': 1})), (('using', 'grammatical'), Counter({'evolution': 1})), (('grammatical', 'evolution'), Counter({';': 1})), (('authors', 'due'), Counter({'to': 1})), (('a', 'major'), Counter({'rewriting': 1})), (('major', 'rewriting'), Counter({'.': 1})), (('rewriting', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'resource'), Counter({'allocation': 1})), (('allocation', 'of'), Counter({'mu': 1})), (('of', 'mu'), Counter({'-': 1})), (('mu', '-'), Counter({'ofdm': 1})), (('-', 'ofdm'), Counter({'based': 1})), (('systems', 'under'), Counter({'partial': 1})), (('under', 'partial'), Counter({'channel': 1})), (('partial', 'channel'), Counter({'state': 1})), (('channel', 'state'), Counter({'information': 1})), (('state', 'information'), Counter({';': 1})), (('information', ';'), Counter({'this': 1})), (('to', 'some'), Counter({'errors': 1})), (('some', 'errors'), Counter({'.': 1})), (('_UNK_', 'advances'), Counter({'in': 1})), (('advances', 'in'), Counter({'artificial': 2})), (('in', 'artificial'), Counter({'intelligence': 2})), (('artificial', 'intelligence'), Counter({'require': 2, ';': 1})), (('intelligence', 'require'), Counter({'progress': 2})), (('require', 'progress'), Counter({'across': 2})), (('progress', 'across'), Counter({'all': 2})), (('across', 'all'), Counter({'of': 2})), (('all', 'of'), Counter({'computer': 2})), (('of', 'computer'), Counter({'science': 2})), (('computer', 'science'), Counter({';': 1, '.': 1, 'and': 1})), (('science', ';'), Counter({'advances': 1})), ((';', 'advances'), Counter({'in': 1})), (('science', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'exploration'), Counter({'of': 1})), (('exploration', 'of'), Counter({'object': 1})), (('of', 'object'), Counter({'recognition': 2})), (('object', 'recognition'), Counter({'from': 2})), (('recognition', 'from'), Counter({'3d': 2})), (('from', '3d'), Counter({'point': 2})), (('3d', 'point'), Counter({'cloud': 2})), (('point', 'cloud'), Counter({';': 1, 'data': 1})), (('cloud', ';'), Counter({'we': 1})), (('present', 'our'), Counter({'latest': 1})), (('our', 'latest'), Counter({'experiment': 1})), (('latest', 'experiment'), Counter({'results': 1})), (('experiment', 'results'), Counter({'of': 1})), (('cloud', 'data'), Counter({'collected': 1})), (('data', 'collected'), Counter({'through': 1})), (('collected', 'through'), Counter({'moving': 1})), (('through', 'moving'), Counter({'car': 1})), (('moving', 'car'), Counter({'.': 1})), (('car', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'quantified'), Counter({'conditional': 1})), (('quantified', 'conditional'), Counter({'logics': 1, 'logic': 1})), (('conditional', 'logics'), Counter({'are': 1})), (('logics', 'are'), Counter({'fragments': 1})), (('are', 'fragments'), Counter({'of': 1})), (('fragments', 'of'), Counter({'hol': 1})), (('of', 'hol'), Counter({';': 1})), (('hol', ';'), Counter({'a': 1})), (('a', 'semantic'), Counter({'embedding': 1})), (('semantic', 'embedding'), Counter({'of': 1})), (('embedding', 'of'), Counter({'(': 1})), (('of', '('), Counter({'constant': 1})), (('(', 'constant'), Counter({'domain': 1})), (('constant', 'domain'), Counter({')': 1})), (('domain', ')'), Counter({'quantified': 1})), ((')', 'quantified'), Counter({'conditional': 1})), (('conditional', 'logic'), Counter({'in': 1})), (('logic', 'in'), Counter({'classical': 1})), (('in', 'classical'), Counter({'higher': 1})), (('classical', 'higher'), Counter({'-': 1})), (('higher', '-'), Counter({'order': 1})), (('-', 'order'), Counter({'logic': 1})), (('order', 'logic'), Counter({'is': 1})), (('logic', 'is'), Counter({'presented': 1})), (('is', 'presented'), Counter({'.': 1, 'as': 1})), (('presented', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'in'), Counter({'memoriam': 1})), (('in', 'memoriam'), Counter({'maurice': 1})), (('memoriam', 'maurice'), Counter({'gross': 1})), (('maurice', 'gross'), Counter({';': 1, '(': 1})), (('gross', ';'), Counter({'maurice': 1})), ((';', 'maurice'), Counter({'gross': 1})), (('gross', '('), Counter({'1934': 1})), (('(', '1934'), Counter({'-': 1})), (('1934', '-'), Counter({'2001': 1})), (('-', '2001'), Counter({')': 1})), (('2001', ')'), Counter({'was': 1})), ((')', 'was'), Counter({'both': 1})), (('was', 'both'), Counter({'a': 1})), (('both', 'a'), Counter({'great': 1})), (('a', 'great'), Counter({'linguist': 1})), (('great', 'linguist'), Counter({'and': 1})), (('linguist', 'and'), Counter({'a': 1})), (('and', 'a'), Counter({'pioneer': 1})), (('a', 'pioneer'), Counter({'in': 1})), (('pioneer', 'in'), Counter({'natural': 1})), (('in', 'natural'), Counter({'language': 1})), (('processing', '.'), Counter({'this': 1})), (('.', 'this'), Counter({'article': 1})), (('is', 'written'), Counter({'in': 1})), (('written', 'in'), Counter({'homage': 1})), (('in', 'homage'), Counter({'to': 1})), (('homage', 'to'), Counter({'his': 1})), (('to', 'his'), Counter({'memory': 1})), (('his', 'memory'), Counter({'_EOS_': 1})), (('_UNK_', 'introduction'), Counter({'to': 1})), (('the', '26th'), Counter({'international': 2})), (('26th', 'international'), Counter({'conference': 2})), (('on', 'logic'), Counter({'programming': 3})), (('logic', 'programming'), Counter({'special': 2, 'paradigm': 1, 'and': 1, 'classes': 1})), (('programming', 'special'), Counter({'issue': 2})), (('issue', ';'), Counter({'this': 1})), (('is', 'the'), Counter({'preface': 1, 'act': 1, 'proceedings': 1})), (('the', 'preface'), Counter({'to': 1})), (('preface', 'to'), Counter({'the': 1})), (('_UNK_', 'beyond'), Counter({'description': 1})), (('beyond', 'description'), Counter({'.': 1})), (('description', '.'), Counter({'comment': 1})), (('.', 'comment'), Counter({'on': 1})), (('\"', 'approaching'), Counter({'human': 2})), (('approaching', 'human'), Counter({'language': 2})), (('human', 'language'), Counter({'with': 2, 'in': 1})), (('language', 'with'), Counter({'complex': 2})), (('with', 'complex'), Counter({'networks': 2})), (('networks', '\"'), Counter({'by': 2})), (('by', 'cong'), Counter({'&': 2})), (('cong', '&'), Counter({'liu': 2})), (('&', 'liu'), Counter({';': 1, '_EOS_': 1})), (('liu', ';'), Counter({'comment': 1})), ((';', 'comment'), Counter({'on': 1})), (('_UNK_', 'norm'), Counter({'-': 1})), (('norm', '-'), Counter({'based': 1, 'constrained': 1})), (('-', 'based'), Counter({'capacity': 1, 'convergence': 1})), (('based', 'capacity'), Counter({'control': 1})), (('capacity', 'control'), Counter({'in': 1})), (('control', 'in'), Counter({'neural': 1})), (('in', 'neural'), Counter({'networks': 1})), (('we', 'investigate'), Counter({'the': 2})), (('investigate', 'the'), Counter({'capacity': 1, 'grapheme': 1})), (('the', 'capacity'), Counter({',': 1})), (('capacity', ','), Counter({'convexity': 1})), ((',', 'convexity'), Counter({'and': 1})), (('convexity', 'and'), Counter({'characterization': 1})), (('and', 'characterization'), Counter({'of': 1})), (('characterization', 'of'), Counter({'a': 1})), (('a', 'general'), Counter({'family': 1, 'class': 1})), (('general', 'family'), Counter({'of': 1})), (('family', 'of'), Counter({'norm': 1})), (('of', 'norm'), Counter({'-': 1})), (('-', 'constrained'), Counter({'feed': 1})), (('constrained', 'feed'), Counter({'-': 1})), (('feed', '-'), Counter({'forward': 1})), (('-', 'forward'), Counter({'networks': 1})), (('forward', 'networks'), Counter({'.': 1})), (('networks', '.'), Counter({'_EOS_': 3})), (('_UNK_', 'about'), Counter({'compression': 1})), (('about', 'compression'), Counter({'of': 1})), (('compression', 'of'), Counter({'vocabulary': 1})), (('of', 'vocabulary'), Counter({'in': 1})), (('vocabulary', 'in'), Counter({'computer': 1})), (('in', 'computer'), Counter({'oriented': 1, '-': 1, 'science': 1})), (('computer', 'oriented'), Counter({'languages': 1})), (('oriented', 'languages'), Counter({';': 1, '.': 1})), (('languages', ';'), Counter({'the': 1})), (('author', 'uses'), Counter({'the': 1})), (('uses', 'the'), Counter({'entropy': 1})), (('the', 'entropy'), Counter({'of': 2})), (('entropy', 'of'), Counter({'the': 2, 'telugu': 1})), (('the', 'ideal'), Counter({'bose': 1})), (('ideal', 'bose'), Counter({'-': 1})), (('bose', '-'), Counter({'einstein': 1})), (('-', 'einstein'), Counter({'gas': 1})), (('einstein', 'gas'), Counter({'to': 1})), (('gas', 'to'), Counter({'minimize': 1})), (('to', 'minimize'), Counter({'losses': 1})), (('minimize', 'losses'), Counter({'in': 1})), (('losses', 'in'), Counter({'computer': 1})), (('computer', '-'), Counter({'oriented': 1})), (('-', 'oriented'), Counter({'languages': 1})), (('languages', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'unary'), Counter({'coding': 1})), (('unary', 'coding'), Counter({'for': 1, 'of': 1})), (('coding', 'for'), Counter({'neural': 1})), (('for', 'neural'), Counter({'network': 1})), (('network', 'learning'), Counter({';': 1})), (('learning', ';'), Counter({'this': 2})), (('presents', 'some'), Counter({'properties': 1})), (('some', 'properties'), Counter({'of': 3})), (('properties', 'of'), Counter({'the': 2, 'unary': 1})), (('of', 'unary'), Counter({'coding': 1})), (('coding', 'of'), Counter({'significance': 1})), (('of', 'significance'), Counter({'for': 1})), (('significance', 'for'), Counter({'biological': 1})), (('for', 'biological'), Counter({'learning': 1})), (('biological', 'learning'), Counter({'and': 1})), (('learning', 'and'), Counter({'music': 2, 'instantaneously': 1, 'inference': 1})), (('and', 'instantaneously'), Counter({'trained': 1})), (('instantaneously', 'trained'), Counter({'neural': 1})), (('trained', 'neural'), Counter({'networks': 1})), (('_UNK_', 'some'), Counter({'properties': 1})), (('the', 'ukrainian'), Counter({'writing': 1, 'version': 1})), (('ukrainian', 'writing'), Counter({'system': 1})), (('writing', 'system'), Counter({';': 1})), (('the', 'grapheme'), Counter({'-': 1})), (('grapheme', '-'), Counter({'phoneme': 1})), (('-', 'phoneme'), Counter({'relation': 1})), (('phoneme', 'relation'), Counter({'in': 1})), (('relation', 'in'), Counter({'ukrainian': 1})), (('in', 'ukrainian'), Counter({'and': 1})), (('ukrainian', 'and'), Counter({'some': 1})), (('and', 'some'), Counter({'properties': 1})), (('ukrainian', 'version'), Counter({'of': 1})), (('the', 'cyrillic'), Counter({'alphabet': 1})), (('cyrillic', 'alphabet'), Counter({'.': 1})), (('alphabet', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'convex'), Counter({'multiview': 1})), (('convex', 'multiview'), Counter({'fisher': 1})), (('multiview', 'fisher'), Counter({'discriminant': 1})), (('fisher', 'discriminant'), Counter({'analysis': 1})), (('discriminant', 'analysis'), Counter({';': 1})), (('analysis', ';'), Counter({'section': 1, 'the': 1})), ((';', 'section'), Counter({'1': 1})), (('section', '1'), Counter({'.': 1})), (('.', '3'), Counter({'was': 1})), (('3', 'was'), Counter({'incorrect': 1})), (('was', 'incorrect'), Counter({',': 1})), (('incorrect', ','), Counter({'and': 1})), ((',', 'and'), Counter({'2': 1, 'language': 1, 'global': 1, 'not': 1, 'demonstrate': 1})), (('and', '2'), Counter({'.': 1})), (('2', '.'), Counter({'1': 1})), (('1', 'will'), Counter({'be': 1})), (('be', 'removed'), Counter({'from': 1})), (('removed', 'from'), Counter({'further': 1})), (('from', 'further'), Counter({'submissions': 1})), (('further', 'submissions'), Counter({'.': 1})), (('submissions', '.'), Counter({'a': 1})), (('.', 'a'), Counter({'rewritten': 1})), (('a', 'rewritten'), Counter({'version': 1})), (('rewritten', 'version'), Counter({'will': 1})), (('be', 'posted'), Counter({'in': 1})), (('posted', 'in'), Counter({'the': 1})), (('the', 'future'), Counter({'.': 2, 'value': 1})), (('future', '.'), Counter({'_EOS_': 2})), (('_UNK_', 'why'), Counter({'bother': 1, \"'\": 1})), (('why', 'bother'), Counter({'with': 1})), (('bother', 'with'), Counter({'syntax': 1})), (('with', 'syntax'), Counter({'?': 1})), (('syntax', '?'), Counter({';': 1})), (('this', 'short'), Counter({'note': 1})), (('short', 'note'), Counter({'discusses': 1})), (('note', 'discusses'), Counter({'the': 1})), (('discusses', 'the'), Counter({'role': 1})), (('the', 'role'), Counter({'of': 1})), (('role', 'of'), Counter({'syntax': 1})), (('of', 'syntax'), Counter({'vs': 1})), (('syntax', 'vs'), Counter({'.': 1})), (('vs', '.'), Counter({'semantics': 1})), (('.', 'semantics'), Counter({'and': 1})), (('semantics', 'and'), Counter({'the': 1})), (('and', 'the'), Counter({'interplay': 1, 'existence': 1})), (('the', 'interplay'), Counter({'between': 1})), (('interplay', 'between'), Counter({'logic': 1})), (('between', 'logic'), Counter({',': 1})), (('logic', ','), Counter({'philosophy': 1})), ((',', 'philosophy'), Counter({',': 1})), (('philosophy', ','), Counter({'and': 1})), (('and', 'language'), Counter({'in': 1})), (('language', 'in'), Counter({'computer': 1, 'the': 1})), (('science', 'and'), Counter({'game': 1})), (('and', 'game'), Counter({'theory': 1})), (('game', 'theory'), Counter({'.': 2})), (('theory', '.'), Counter({'_EOS_': 3})), (('why', \"'\"), Counter({'gsa': 2})), ((\"'\", 'gsa'), Counter({':': 2})), (('gsa', ':'), Counter({'a': 2})), (('a', 'gravitational'), Counter({'search': 2})), (('gravitational', 'search'), Counter({'algorithm': 2})), (('search', 'algorithm'), Counter({\"'\": 2})), (('algorithm', \"'\"), Counter({'is': 2, 's': 1})), ((\"'\", 'is'), Counter({'not': 2})), (('is', 'not'), Counter({'genuinely': 2, 'robust': 1, 'secure': 1})), (('not', 'genuinely'), Counter({'based': 2})), (('genuinely', 'based'), Counter({'on': 2})), (('on', 'the'), Counter({'law': 2, 'universality': 1, 'possibility': 1, 'car': 1, 'triangle': 1, 'mnist': 1, 'existence': 1})), (('the', 'law'), Counter({'of': 2})), (('law', 'of'), Counter({'gravity': 2})), (('of', 'gravity'), Counter({';': 1, '_EOS_': 1})), (('gravity', ';'), Counter({'why': 1})), ((';', 'why'), Counter({\"'\": 1})), (('_UNK_', 'neurocontrol'), Counter({'methods': 1})), (('neurocontrol', 'methods'), Counter({'review': 1})), (('methods', 'review'), Counter({';': 1})), (('review', ';'), Counter({'methods': 1})), ((';', 'methods'), Counter({'of': 1})), (('methods', 'of'), Counter({'applying': 1})), (('of', 'applying'), Counter({'neural': 1})), (('applying', 'neural'), Counter({'networks': 1})), (('networks', 'to'), Counter({'control': 1})), (('to', 'control'), Counter({'plants': 1})), (('control', 'plants'), Counter({'are': 1})), (('plants', 'are'), Counter({'considered': 1})), (('are', 'considered'), Counter({'.': 1})), (('considered', '.'), Counter({'methods': 1})), (('.', 'methods'), Counter({'and': 1})), (('methods', 'and'), Counter({'schemes': 1})), (('and', 'schemes'), Counter({'are': 1})), (('schemes', 'are'), Counter({'described': 1})), (('described', ','), Counter({'their': 1})), ((',', 'their'), Counter({'advantages': 1})), (('their', 'advantages'), Counter({'and': 1})), (('advantages', 'and'), Counter({'disadvantages': 1})), (('and', 'disadvantages'), Counter({'are': 1})), (('disadvantages', 'are'), Counter({'discussed': 1})), (('are', 'discussed'), Counter({'.': 1})), (('discussed', '.'), Counter({'_EOS_': 3})), (('_UNK_', 'on'), Counter({'the': 3})), (('the', 'universality'), Counter({'of': 1})), (('universality', 'of'), Counter({'online': 1})), (('of', 'online'), Counter({'mirror': 1})), (('online', 'mirror'), Counter({'descent': 1})), (('mirror', 'descent'), Counter({';': 1, 'can': 1})), (('descent', ';'), Counter({'we': 1})), (('we', 'show'), Counter({'that': 3, 'applications': 1, 'how': 1})), (('show', 'that'), Counter({'for': 1, 'it': 1, 'defensive': 1, 'a': 1})), (('that', 'for'), Counter({'a': 1})), (('for', 'a'), Counter({'general': 1, 'course': 1, 'multilayer': 1})), (('general', 'class'), Counter({'of': 1})), (('class', 'of'), Counter({'convex': 1})), (('of', 'convex'), Counter({'online': 1})), (('convex', 'online'), Counter({'learning': 1})), (('online', 'learning'), Counter({'problems': 1})), (('learning', 'problems'), Counter({',': 1})), (('problems', ','), Counter({'mirror': 1})), ((',', 'mirror'), Counter({'descent': 1})), (('descent', 'can'), Counter({'always': 1})), (('can', 'always'), Counter({'achieve': 1})), (('always', 'achieve'), Counter({'a': 1})), (('achieve', 'a'), Counter({'(': 1})), (('a', '('), Counter({'nearly': 1})), (('(', 'nearly'), Counter({')': 1})), (('nearly', ')'), Counter({'optimal': 1})), ((')', 'optimal'), Counter({'regret': 1})), (('optimal', 'regret'), Counter({'guarantee': 1})), (('regret', 'guarantee'), Counter({'.': 1})), (('guarantee', '.'), Counter({'_EOS_': 1})), (('the', 'possibility'), Counter({'of': 1})), (('possibility', 'of'), Counter({'making': 1})), (('of', 'making'), Counter({'the': 1})), (('making', 'the'), Counter({'complete': 1})), (('the', 'complete'), Counter({'computer': 1})), (('complete', 'computer'), Counter({'model': 1})), (('computer', 'model'), Counter({'of': 2})), (('a', 'human'), Counter({'brain': 1})), (('human', 'brain'), Counter({';': 1})), (('brain', ';'), Counter({'the': 1})), (('the', 'development'), Counter({'of': 1})), (('development', 'of'), Counter({'the': 1})), (('the', 'algorithm'), Counter({'of': 1})), (('algorithm', 'of'), Counter({'a': 1})), (('a', 'neural'), Counter({'network': 1})), (('network', 'building'), Counter({'by': 1})), (('building', 'by'), Counter({'the': 1})), (('the', 'corresponding'), Counter({'parts': 1})), (('corresponding', 'parts'), Counter({'of': 1})), (('parts', 'of'), Counter({'a': 1})), (('a', 'dna'), Counter({'code': 1})), (('dna', 'code'), Counter({'is': 1})), (('code', 'is'), Counter({'discussed': 1})), (('is', 'discussed'), Counter({'.': 1})), (('_UNK_', 'philosophy'), Counter({'in': 1})), (('philosophy', 'in'), Counter({'the': 1})), (('the', 'face'), Counter({'of': 1})), (('face', 'of'), Counter({'artificial': 1})), (('of', 'artificial'), Counter({'intelligence': 1})), (('intelligence', ';'), Counter({'in': 1})), (('article', ','), Counter({'i': 1})), ((',', 'i'), Counter({'discuss': 1})), (('i', 'discuss'), Counter({'how': 1})), (('discuss', 'how'), Counter({'the': 1})), (('how', 'the'), Counter({'ai': 1})), (('the', 'ai'), Counter({'community': 1})), (('ai', 'community'), Counter({'views': 1})), (('community', 'views'), Counter({'concerns': 1})), (('views', 'concerns'), Counter({'about': 1})), (('concerns', 'about'), Counter({'the': 1})), (('about', 'the'), Counter({'emergence': 1})), (('the', 'emergence'), Counter({'of': 1})), (('emergence', 'of'), Counter({'superintelligent': 1})), (('of', 'superintelligent'), Counter({'ai': 1})), (('superintelligent', 'ai'), Counter({'and': 1})), (('ai', 'and'), Counter({'related': 1})), (('and', 'related'), Counter({'philosophical': 1})), (('related', 'philosophical'), Counter({'issues': 1})), (('philosophical', 'issues'), Counter({'.': 1})), (('issues', '.'), Counter({'_EOS_': 1})), (('a', 'survey'), Counter({'on': 1, 'of': 1})), (('survey', 'on'), Counter({'contextual': 1})), (('on', 'contextual'), Counter({'multi': 1})), (('contextual', 'multi'), Counter({'-': 1})), (('multi', '-'), Counter({'armed': 1})), (('-', 'armed'), Counter({'bandits': 1})), (('armed', 'bandits'), Counter({';': 1})), (('bandits', ';'), Counter({'in': 1})), (('this', 'survey'), Counter({'we': 1})), (('survey', 'we'), Counter({'cover': 1})), (('we', 'cover'), Counter({'a': 1})), (('cover', 'a'), Counter({'few': 1})), (('a', 'few'), Counter({'stochastic': 1})), (('few', 'stochastic'), Counter({'and': 1})), (('stochastic', 'and'), Counter({'adversarial': 1})), (('and', 'adversarial'), Counter({'contextual': 1})), (('adversarial', 'contextual'), Counter({'bandit': 1})), (('contextual', 'bandit'), Counter({'algorithms': 1})), (('bandit', 'algorithms'), Counter({'.': 1})), (('algorithms', '.'), Counter({'we': 1})), (('.', 'we'), Counter({'analyze': 1, 'have': 1, 'show': 1})), (('we', 'analyze'), Counter({'each': 1})), (('analyze', 'each'), Counter({'algorithm': 1})), (('each', 'algorithm'), Counter({\"'\": 1})), (('s', 'assumption'), Counter({'and': 1})), (('assumption', 'and'), Counter({'regret': 1})), (('and', 'regret'), Counter({'bound': 1})), (('regret', 'bound'), Counter({'.': 1})), (('bound', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'parallels'), Counter({'of': 1})), (('parallels', 'of'), Counter({'human': 1})), (('of', 'human'), Counter({'language': 1})), (('the', 'behavior'), Counter({'of': 1})), (('behavior', 'of'), Counter({'bottlenose': 1})), (('of', 'bottlenose'), Counter({'dolphins': 1})), (('bottlenose', 'dolphins'), Counter({';': 1})), (('dolphins', ';'), Counter({'a': 1})), (('a', 'short'), Counter({'review': 1})), (('short', 'review'), Counter({'of': 1})), (('review', 'of'), Counter({'similarities': 1})), (('of', 'similarities'), Counter({'between': 1})), (('similarities', 'between'), Counter({'dolphins': 1})), (('between', 'dolphins'), Counter({'and': 1})), (('dolphins', 'and'), Counter({'humans': 1})), (('and', 'humans'), Counter({'with': 1})), (('humans', 'with'), Counter({'the': 1})), (('with', 'the'), Counter({'help': 1})), (('the', 'help'), Counter({'of': 1})), (('help', 'of'), Counter({'quantitative': 1})), (('of', 'quantitative'), Counter({'linguistics': 1})), (('quantitative', 'linguistics'), Counter({'and': 1})), (('linguistics', 'and'), Counter({'information': 1})), (('and', 'information'), Counter({'theory': 1})), (('information', 'theory'), Counter({'.': 1})), (('a', 'history'), Counter({'of': 1})), (('history', 'of'), Counter({'metaheuristics': 2})), (('of', 'metaheuristics'), Counter({';': 1, 'in': 1})), (('metaheuristics', ';'), Counter({'this': 1})), (('this', 'chapter'), Counter({'describes': 1})), (('chapter', 'describes'), Counter({'the': 1})), (('describes', 'the'), Counter({'history': 1})), (('the', 'history'), Counter({'of': 1})), (('metaheuristics', 'in'), Counter({'five': 1})), (('in', 'five'), Counter({'distinct': 1})), (('five', 'distinct'), Counter({'periods': 1})), (('distinct', 'periods'), Counter({',': 1})), (('periods', ','), Counter({'starting': 1})), ((',', 'starting'), Counter({'long': 1})), (('starting', 'long'), Counter({'before': 1})), (('long', 'before'), Counter({'the': 1})), (('before', 'the'), Counter({'first': 1})), (('the', 'first'), Counter({'international': 2, 'use': 1})), (('first', 'use'), Counter({'of': 1})), (('the', 'term'), Counter({'and': 1})), (('term', 'and'), Counter({'ending': 1})), (('and', 'ending'), Counter({'a': 1})), (('ending', 'a'), Counter({'long': 1})), (('a', 'long'), Counter({'time': 1})), (('long', 'time'), Counter({'in': 1})), (('time', 'in'), Counter({'the': 1})), (('_UNK_', 'an'), Counter({'energy': 1})), (('an', 'energy'), Counter({'efficient': 1})), (('energy', 'efficient'), Counter({'scheme': 1})), (('efficient', 'scheme'), Counter({'for': 1})), (('scheme', 'for'), Counter({'data': 1})), (('for', 'data'), Counter({'gathering': 1})), (('data', 'gathering'), Counter({'in': 1})), (('gathering', 'in'), Counter({'wireless': 1})), (('in', 'wireless'), Counter({'sensor': 1})), (('wireless', 'sensor'), Counter({'networks': 1})), (('sensor', 'networks'), Counter({'using': 1})), (('networks', 'using'), Counter({'particle': 1})), (('using', 'particle'), Counter({'swarm': 1})), (('particle', 'swarm'), Counter({'optimization': 1})), (('swarm', 'optimization'), Counter({';': 1})), (('optimization', ';'), Counter({'this': 2, 'in': 1})), (('crucial', 'sign'), Counter({'error': 1})), (('sign', 'error'), Counter({'in': 1})), (('in', 'equation'), Counter({'1': 1})), (('equation', '1'), Counter({'_EOS_': 1})), (('new', 'bengali'), Counter({'readability': 1})), (('bengali', 'readability'), Counter({'score': 1})), (('readability', 'score'), Counter({';': 1})), (('score', ';'), Counter({'in': 1})), (('paper', 'we'), Counter({'use': 2, 'have': 1})), (('we', 'have'), Counter({'proposed': 1, 'got': 1})), (('have', 'proposed'), Counter({'methods': 1})), (('proposed', 'methods'), Counter({'to': 1})), (('methods', 'to'), Counter({'analyze': 1})), (('to', 'analyze'), Counter({'the': 1})), (('analyze', 'the'), Counter({'readability': 1})), (('the', 'readability'), Counter({'of': 1})), (('readability', 'of'), Counter({'bengali': 1})), (('of', 'bengali'), Counter({'language': 1})), (('bengali', 'language'), Counter({'texts': 1})), (('language', 'texts'), Counter({'.': 1})), (('texts', '.'), Counter({'we': 1, '_EOS_': 1})), (('have', 'got'), Counter({'some': 1})), (('got', 'some'), Counter({'exceptionally': 1})), (('some', 'exceptionally'), Counter({'good': 1})), (('exceptionally', 'good'), Counter({'results': 1})), (('good', 'results'), Counter({'out': 1})), (('results', 'out'), Counter({'of': 1})), (('out', 'of'), Counter({'the': 1})), (('the', 'experiments'), Counter({'.': 1})), (('experiments', '.'), Counter({'_EOS_': 1})), (('the', 'logic'), Counter({'programming': 1})), (('programming', 'paradigm'), Counter({'and': 1})), (('paradigm', 'and'), Counter({'prolog': 1})), (('and', 'prolog'), Counter({';': 1, 'appropriate': 1})), (('prolog', ';'), Counter({'this': 1})), (('a', 'tutorial'), Counter({'on': 1})), (('tutorial', 'on'), Counter({'logic': 1})), (('programming', 'and'), Counter({'prolog': 1})), (('prolog', 'appropriate'), Counter({'for': 1})), (('appropriate', 'for'), Counter({'a': 1})), (('a', 'course'), Counter({'on': 1})), (('course', 'on'), Counter({'programming': 1})), (('on', 'programming'), Counter({'languages': 1})), (('programming', 'languages'), Counter({'for': 1})), (('languages', 'for'), Counter({'students': 1})), (('for', 'students'), Counter({'familiar': 1})), (('students', 'familiar'), Counter({'with': 1})), (('familiar', 'with'), Counter({'imperative': 1})), (('with', 'imperative'), Counter({'programming': 1})), (('imperative', 'programming'), Counter({'.': 1})), (('programming', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'calculate'), Counter({'distance': 1})), (('calculate', 'distance'), Counter({'to': 1})), (('distance', 'to'), Counter({'object': 1, 'the': 1})), (('to', 'object'), Counter({'in': 1})), (('object', 'in'), Counter({'the': 1, 'its': 1})), (('the', 'area'), Counter({'where': 1})), (('area', 'where'), Counter({'car': 1})), (('where', 'car'), Counter({',': 1})), (('car', ','), Counter({'using': 1, 'to': 1})), ((',', 'using'), Counter({'video': 1})), (('using', 'video'), Counter({'analysis': 1, 'cameras': 1})), (('video', 'analysis'), Counter({';': 1})), (('the', 'method'), Counter({'of': 1})), (('of', 'using'), Counter({'video': 1})), (('video', 'cameras'), Counter({'installed': 1})), (('cameras', 'installed'), Counter({'on': 1})), (('installed', 'on'), Counter({'the': 1})), (('the', 'car'), Counter({',': 1})), ((',', 'to'), Counter({'calculate': 1})), (('to', 'calculate'), Counter({'the': 1})), (('the', 'distance'), Counter({'to': 1})), (('the', 'object'), Counter({'in': 1})), (('in', 'its'), Counter({'area': 1})), (('its', 'area'), Counter({'of': 1})), (('area', 'of'), Counter({'movement': 1})), (('of', 'movement'), Counter({'.': 1})), (('movement', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'group'), Counter({'theory': 1})), (('group', 'theory'), Counter({',': 1})), (('theory', ','), Counter({'group': 1})), ((',', 'group'), Counter({'actions': 1})), (('group', 'actions'), Counter({',': 1})), (('actions', ','), Counter({'evolutionary': 1})), ((',', 'evolutionary'), Counter({'algorithms': 1})), (('evolutionary', 'algorithms'), Counter({',': 1})), (('algorithms', ','), Counter({'and': 1})), (('and', 'global'), Counter({'optimization': 1})), (('global', 'optimization'), Counter({';': 1})), (('use', 'group'), Counter({',': 1})), (('group', ','), Counter({'action': 1})), ((',', 'action'), Counter({'and': 1})), (('action', 'and'), Counter({'orbit': 1})), (('and', 'orbit'), Counter({'to': 1})), (('orbit', 'to'), Counter({'understand': 1})), (('to', 'understand'), Counter({'how': 1})), (('understand', 'how'), Counter({'evolutionary': 1})), (('how', 'evolutionary'), Counter({'solve': 1})), (('evolutionary', 'solve'), Counter({'nonconvex': 1})), (('solve', 'nonconvex'), Counter({'optimization': 1})), (('nonconvex', 'optimization'), Counter({'problems': 1})), (('optimization', 'problems'), Counter({'.': 1})), (('problems', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'entropy'), Counter({'of': 1})), (('of', 'telugu'), Counter({';': 1})), (('telugu', ';'), Counter({'this': 1})), (('presents', 'an'), Counter({'investigation': 1})), (('an', 'investigation'), Counter({'of': 1})), (('investigation', 'of'), Counter({'the': 1})), (('the', 'telugu'), Counter({'script': 1})), (('telugu', 'script'), Counter({'.': 1})), (('script', '.'), Counter({'since': 1})), (('.', 'since'), Counter({'this': 1})), (('since', 'this'), Counter({'script': 1})), (('this', 'script'), Counter({'is': 1})), (('script', 'is'), Counter({'syllabic': 1})), (('is', 'syllabic'), Counter({',': 1})), (('syllabic', ','), Counter({'and': 1})), (('and', 'not'), Counter({'alphabetic': 1})), (('not', 'alphabetic'), Counter({',': 1})), (('alphabetic', ','), Counter({'the': 1})), ((',', 'the'), Counter({'computation': 1})), (('the', 'computation'), Counter({'of': 1})), (('computation', 'of'), Counter({'entropy': 1})), (('of', 'entropy'), Counter({'is': 1})), (('entropy', 'is'), Counter({'somewhat': 1})), (('is', 'somewhat'), Counter({'complicated': 1})), (('somewhat', 'complicated'), Counter({'.': 1})), (('complicated', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'word'), Counter({'segmentation': 1})), (('word', 'segmentation'), Counter({'on': 2})), (('segmentation', 'on'), Counter({'micro': 2})), (('on', 'micro'), Counter({'-': 2})), (('micro', '-'), Counter({'blog': 2})), (('-', 'blog'), Counter({'texts': 2})), (('blog', 'texts'), Counter({'with': 1, '.': 1})), (('texts', 'with'), Counter({'external': 1})), (('with', 'external'), Counter({'lexicon': 1})), (('external', 'lexicon'), Counter({'and': 1})), (('lexicon', 'and'), Counter({'heterogeneous': 1})), (('and', 'heterogeneous'), Counter({'data': 1})), (('heterogeneous', 'data'), Counter({';': 1})), (('data', ';'), Counter({'this': 1})), (('paper', 'describes'), Counter({'our': 1})), (('describes', 'our'), Counter({'system': 1})), (('our', 'system'), Counter({'designed': 1})), (('system', 'designed'), Counter({'for': 1})), (('designed', 'for'), Counter({'the': 1})), (('the', 'nlpcc'), Counter({'2016': 1})), (('nlpcc', '2016'), Counter({'shared': 1})), (('2016', 'shared'), Counter({'task': 1})), (('shared', 'task'), Counter({'on': 1})), (('task', 'on'), Counter({'word': 1})), (('on', 'word'), Counter({'segmentation': 1})), (('_UNK_', 'guarded'), Counter({'resolution': 1})), (('guarded', 'resolution'), Counter({'for': 1})), (('resolution', 'for'), Counter({'answer': 1})), (('for', 'answer'), Counter({'set': 1})), (('answer', 'set'), Counter({'programming': 3})), (('set', 'programming'), Counter({';': 2, 'intended': 1})), (('programming', ';'), Counter({'we': 1, 'a': 1})), (('we', 'describe'), Counter({'a': 1})), (('describe', 'a'), Counter({'variant': 1})), (('a', 'variant'), Counter({'of': 1})), (('variant', 'of'), Counter({'resolution': 1})), (('of', 'resolution'), Counter({'rule': 1})), (('resolution', 'rule'), Counter({'of': 1})), (('rule', 'of'), Counter({'proof': 1})), (('of', 'proof'), Counter({'and': 1})), (('proof', 'and'), Counter({'show': 1})), (('and', 'show'), Counter({'that': 1})), (('that', 'it'), Counter({'is': 1, 'obtains': 1})), (('is', 'complete'), Counter({'for': 1})), (('complete', 'for'), Counter({'stable': 1})), (('for', 'stable'), Counter({'semantics': 1})), (('stable', 'semantics'), Counter({'of': 1})), (('semantics', 'of'), Counter({'logic': 1, 'answer': 1})), (('of', 'logic'), Counter({'programs': 1})), (('logic', 'programs'), Counter({'.': 1})), (('programs', '.'), Counter({'we': 1})), (('show', 'applications'), Counter({'of': 1})), (('applications', 'of'), Counter({'this': 1})), (('of', 'this'), Counter({'result': 1, 'work': 1})), (('this', 'result'), Counter({'.': 1})), (('result', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'cornell'), Counter({'spf': 1})), (('cornell', 'spf'), Counter({':': 1})), (('spf', ':'), Counter({'cornell': 1})), ((':', 'cornell'), Counter({'semantic': 1})), (('cornell', 'semantic'), Counter({'parsing': 2})), (('semantic', 'parsing'), Counter({'framework': 2})), (('parsing', 'framework'), Counter({';': 1, '(': 1})), (('framework', ';'), Counter({'the': 1})), (('the', 'cornell'), Counter({'semantic': 1})), (('framework', '('), Counter({'spf': 1})), (('(', 'spf'), Counter({')': 1})), (('spf', ')'), Counter({'is': 1})), ((')', 'is'), Counter({'a': 1})), (('a', 'learning'), Counter({'and': 1})), (('and', 'inference'), Counter({'framework': 1})), (('inference', 'framework'), Counter({'for': 1})), (('for', 'mapping'), Counter({'natural': 1})), (('mapping', 'natural'), Counter({'language': 1})), (('language', 'to'), Counter({'formal': 1})), (('to', 'formal'), Counter({'representation': 1})), (('formal', 'representation'), Counter({'of': 2})), (('of', 'its'), Counter({'meaning': 1})), (('its', 'meaning'), Counter({'.': 1})), (('meaning', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'semistability'), Counter({'-': 1})), (('semistability', '-'), Counter({'based': 1})), (('based', 'convergence'), Counter({'analysis': 1})), (('convergence', 'analysis'), Counter({'for': 1})), (('for', 'paracontracting'), Counter({'multiagent': 1})), (('paracontracting', 'multiagent'), Counter({'coordination': 1})), (('multiagent', 'coordination'), Counter({'optimization': 1})), (('coordination', 'optimization'), Counter({';': 1})), (('this', 'sequential'), Counter({'technical': 1})), (('sequential', 'technical'), Counter({'report': 1})), (('technical', 'report'), Counter({'extends': 1, ':': 1})), (('report', 'extends'), Counter({'some': 1})), (('extends', 'some'), Counter({'of': 1})), (('some', 'of'), Counter({'the': 1})), (('the', 'previous'), Counter({'results': 1})), (('previous', 'results'), Counter({'we': 1})), (('results', 'we'), Counter({'posted': 1})), (('we', 'posted'), Counter({'at': 1})), (('posted', 'at'), Counter({'arxiv': 1})), (('at', 'arxiv'), Counter({':': 1})), ((':', '1306'), Counter({'.': 1})), (('1306', '.'), Counter({'0225': 1})), (('.', '0225'), Counter({'.': 1})), (('0225', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'proceedings'), Counter({'of': 2})), (('proceedings', 'of'), Counter({'the': 2, 'nips': 2})), (('first', 'international'), Counter({'workshop': 2})), (('international', 'workshop'), Counter({'on': 2})), (('workshop', 'on'), Counter({'deep': 2})), (('on', 'deep'), Counter({'learning': 2})), (('deep', 'learning'), Counter({'and': 2})), (('and', 'music'), Counter({';': 1, ',': 1})), (('music', ';'), Counter({'proceedings': 1})), ((';', 'proceedings'), Counter({'of': 1})), (('music', ','), Counter({'joint': 1})), ((',', 'joint'), Counter({'with': 1})), (('joint', 'with'), Counter({'ijcnn': 1})), (('with', 'ijcnn'), Counter({',': 1})), (('ijcnn', ','), Counter({'anchorage': 1})), ((',', 'anchorage'), Counter({',': 1})), (('anchorage', ','), Counter({'us': 1})), ((',', 'us'), Counter({',': 1})), (('us', ','), Counter({'may': 1})), (('may', '17'), Counter({'-': 1})), (('17', '-'), Counter({'18': 1})), (('-', '18'), Counter({',': 1})), (('18', ','), Counter({'2017': 1})), ((',', '2017'), Counter({'_EOS_': 2})), (('_UNK_', 'attack'), Counter({'rmse': 1})), (('attack', 'rmse'), Counter({'leaderboard': 1})), (('rmse', 'leaderboard'), Counter({':': 1})), (('leaderboard', ':'), Counter({'an': 1})), ((':', 'an'), Counter({'introduction': 1})), (('an', 'introduction'), Counter({'and': 1})), (('introduction', 'and'), Counter({'case': 1})), (('and', 'case'), Counter({'study': 1})), (('case', 'study'), Counter({';': 1})), (('study', ';'), Counter({'in': 1})), (('this', 'manuscript'), Counter({',': 1})), (('manuscript', ','), Counter({'we': 1})), (('we', 'briefly'), Counter({'introduce': 1})), (('briefly', 'introduce'), Counter({'several': 1})), (('introduce', 'several'), Counter({'tricks': 1})), (('several', 'tricks'), Counter({'to': 1})), (('tricks', 'to'), Counter({'climb': 1})), (('to', 'climb'), Counter({'the': 1})), (('climb', 'the'), Counter({'leaderboards': 1})), (('the', 'leaderboards'), Counter({'which': 1})), (('leaderboards', 'which'), Counter({'use': 1})), (('which', 'use'), Counter({'rmse': 1})), (('use', 'rmse'), Counter({'for': 1})), (('rmse', 'for'), Counter({'evaluation': 1})), (('for', 'evaluation'), Counter({'without': 1})), (('evaluation', 'without'), Counter({'exploiting': 1})), (('without', 'exploiting'), Counter({'any': 1})), (('exploiting', 'any'), Counter({'training': 1})), (('any', 'training'), Counter({'data': 1})), (('training', 'data'), Counter({'.': 1})), (('data', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'standardization'), Counter({'of': 1})), (('standardization', 'of'), Counter({'the': 1})), (('the', 'formal'), Counter({'representation': 1})), (('of', 'lexical'), Counter({'information': 1})), (('lexical', 'information'), Counter({'for': 1})), (('information', 'for'), Counter({'nlp': 1})), (('for', 'nlp'), Counter({';': 1})), (('nlp', ';'), Counter({'a': 1})), (('survey', 'of'), Counter({'dictionary': 1})), (('of', 'dictionary'), Counter({'models': 1})), (('dictionary', 'models'), Counter({'and': 1})), (('models', 'and'), Counter({'formats': 1})), (('and', 'formats'), Counter({'is': 1})), (('formats', 'is'), Counter({'presented': 1})), (('presented', 'as'), Counter({'well': 1})), (('as', 'well'), Counter({'as': 1})), (('well', 'as'), Counter({'a': 1})), (('a', 'presentation'), Counter({'of': 1})), (('presentation', 'of'), Counter({'corresponding': 1})), (('of', 'corresponding'), Counter({'recent': 1})), (('corresponding', 'recent'), Counter({'standardisation': 1})), (('recent', 'standardisation'), Counter({'activities': 1})), (('standardisation', 'activities'), Counter({'.': 1})), (('activities', '.'), Counter({'_EOS_': 1})), (('machine', 'learning'), Counter({'model': 1, ';': 1, ',': 1})), (('learning', 'model'), Counter({'for': 1})), (('model', 'for'), Counter({'stock': 1})), (('for', 'stock'), Counter({'market': 1})), (('stock', 'market'), Counter({'prediction': 2})), (('market', 'prediction'), Counter({';': 1, 'is': 1})), (('prediction', ';'), Counter({'stock': 1})), ((';', 'stock'), Counter({'market': 1})), (('prediction', 'is'), Counter({'the': 1})), (('the', 'act'), Counter({'of': 1})), (('act', 'of'), Counter({'trying': 1})), (('of', 'trying'), Counter({'to': 1})), (('trying', 'to'), Counter({'determine': 1})), (('to', 'determine'), Counter({'the': 1})), (('determine', 'the'), Counter({'future': 1})), (('future', 'value'), Counter({'of': 1})), (('value', 'of'), Counter({'a': 1})), (('a', 'company'), Counter({'stock': 1})), (('company', 'stock'), Counter({'or': 1})), (('stock', 'or'), Counter({'other': 1})), (('or', 'other'), Counter({'financial': 1})), (('other', 'financial'), Counter({'instrument': 1})), (('financial', 'instrument'), Counter({'traded': 1})), (('instrument', 'traded'), Counter({'on': 1})), (('traded', 'on'), Counter({'a': 1})), (('on', 'a'), Counter({'financial': 1, 'priori': 1})), (('a', 'financial'), Counter({'exchange': 1})), (('financial', 'exchange'), Counter({'.': 1})), (('exchange', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'defensive'), Counter({'distillation': 1})), (('defensive', 'distillation'), Counter({'is': 2})), (('distillation', 'is'), Counter({'not': 2})), (('not', 'robust'), Counter({'to': 1})), (('robust', 'to'), Counter({'adversarial': 1})), (('to', 'adversarial'), Counter({'examples': 1})), (('adversarial', 'examples'), Counter({';': 1})), (('examples', ';'), Counter({'we': 1})), (('that', 'defensive'), Counter({'distillation': 1})), (('not', 'secure'), Counter({':': 1})), (('secure', ':'), Counter({'it': 1})), ((':', 'it'), Counter({'is': 1})), (('no', 'more'), Counter({'resistant': 1})), (('more', 'resistant'), Counter({'to': 1})), (('resistant', 'to'), Counter({'targeted': 1})), (('to', 'targeted'), Counter({'misclassification': 1})), (('targeted', 'misclassification'), Counter({'attacks': 1})), (('misclassification', 'attacks'), Counter({'than': 1})), (('attacks', 'than'), Counter({'unprotected': 1})), (('than', 'unprotected'), Counter({'neural': 1})), (('unprotected', 'neural'), Counter({'networks': 1})), (('of', 'nips'), Counter({'2017': 2})), (('nips', '2017'), Counter({'symposium': 2})), (('2017', 'symposium'), Counter({'on': 2})), (('symposium', 'on'), Counter({'interpretable': 2})), (('on', 'interpretable'), Counter({'machine': 2})), (('interpretable', 'machine'), Counter({'learning': 2})), (('the', 'proceedings'), Counter({'of': 1})), (('learning', ','), Counter({'held': 1})), ((',', 'held'), Counter({'in': 1})), (('held', 'in'), Counter({'long': 1})), (('in', 'long'), Counter({'beach': 1})), (('long', 'beach'), Counter({',': 1})), (('beach', ','), Counter({'california': 1})), ((',', 'california'), Counter({',': 1})), (('california', ','), Counter({'usa': 1})), ((',', 'usa'), Counter({'on': 1})), (('usa', 'on'), Counter({'december': 1})), (('on', 'december'), Counter({'7': 1})), (('december', '7'), Counter({',': 1})), (('7', ','), Counter({'2017': 1})), (('_UNK_', 'piecewise'), Counter({'linear': 2})), (('piecewise', 'linear'), Counter({'activation': 1, 'multilayer': 1})), (('linear', 'activation'), Counter({'functions': 1})), (('activation', 'functions'), Counter({'for': 1})), (('functions', 'for'), Counter({'more': 1})), (('for', 'more'), Counter({'efficient': 1})), (('more', 'efficient'), Counter({'deep': 1})), (('efficient', 'deep'), Counter({'networks': 1})), (('deep', 'networks'), Counter({';': 1})), (('submission', 'has'), Counter({'been': 1})), (('by', 'arxiv'), Counter({'administrators': 2})), (('arxiv', 'administrators'), Counter({'because': 2})), (('administrators', 'because'), Counter({'it': 1, 'the': 1})), (('is', 'intentionally'), Counter({'incomplete': 1})), (('intentionally', 'incomplete'), Counter({',': 1})), (('incomplete', ','), Counter({'which': 1})), ((',', 'which'), Counter({'is': 1})), (('which', 'is'), Counter({'in': 1})), (('is', 'in'), Counter({'violation': 1})), (('in', 'violation'), Counter({'of': 1})), (('violation', 'of'), Counter({'our': 1})), (('of', 'our'), Counter({'policies': 1})), (('our', 'policies'), Counter({'.': 1})), (('policies', '.'), Counter({'_EOS_': 1})), (('the', 'triangle'), Counter({'inequality': 2})), (('triangle', 'inequality'), Counter({'for': 2})), (('inequality', 'for'), Counter({'the': 2})), (('the', 'jaccard'), Counter({'distance': 2})), (('jaccard', 'distance'), Counter({';': 1, 'in': 1})), (('distance', ';'), Counter({'two': 1})), ((';', 'two'), Counter({'simple': 1})), (('two', 'simple'), Counter({'proofs': 1})), (('simple', 'proofs'), Counter({'of': 1})), (('proofs', 'of'), Counter({'the': 1})), (('distance', 'in'), Counter({'terms': 1})), (('in', 'terms'), Counter({'of': 1})), (('terms', 'of'), Counter({'nonnegative': 1})), (('of', 'nonnegative'), Counter({',': 1})), (('nonnegative', ','), Counter({'monotone': 1})), ((',', 'monotone'), Counter({',': 1})), (('monotone', ','), Counter({'submodular': 1})), ((',', 'submodular'), Counter({'functions': 1})), (('submodular', 'functions'), Counter({'are': 1})), (('functions', 'are'), Counter({'given': 1})), (('are', 'given'), Counter({'and': 1})), (('given', 'and'), Counter({'discussed': 1})), (('and', 'discussed'), Counter({'.': 1})), (('_UNK_', 'how'), Counter({'to': 1})), (('how', 'to'), Counter({'realize': 1, 'adjust': 1})), (('to', 'realize'), Counter({'\"': 1})), (('realize', '\"'), Counter({'a': 1})), (('a', 'sense'), Counter({'of': 1})), (('sense', 'of'), Counter({'humour': 2})), (('of', 'humour'), Counter({'\"': 2})), (('humour', '\"'), Counter({'in': 1, 'suggested': 1})), (('\"', 'in'), Counter({'computers': 1})), (('in', 'computers'), Counter({'?': 1})), (('computers', '?'), Counter({';': 1})), ((';', 'computer'), Counter({'model': 1})), (('a', '\"'), Counter({'sense': 1})), (('\"', 'sense'), Counter({'of': 1})), (('\"', 'suggested'), Counter({'previously': 1})), (('suggested', 'previously'), Counter({'[': 1})), (('previously', '['), Counter({'arxiv': 1})), (('[', 'arxiv'), Counter({':': 1})), ((':', '0711'), Counter({'.': 1})), (('0711', '.'), Counter({'2058': 1, '2061': 1, '2270': 1})), (('.', '2058'), Counter({',': 1})), (('2058', ','), Counter({'0711': 1})), ((',', '0711'), Counter({'.': 2})), (('.', '2061'), Counter({',': 1})), (('2061', ','), Counter({'0711': 1})), (('.', '2270'), Counter({']': 1})), (('2270', ']'), Counter({'is': 1})), ((']', 'is'), Counter({'raised': 1})), (('is', 'raised'), Counter({'to': 1})), (('raised', 'to'), Counter({'the': 1})), (('the', 'level'), Counter({'of': 1})), (('level', 'of'), Counter({'a': 1})), (('a', 'realistic'), Counter({'algorithm': 1})), (('realistic', 'algorithm'), Counter({'.': 1})), (('algorithm', '.'), Counter({'_EOS_': 1})), (('linear', 'multilayer'), Counter({'perceptrons': 1})), (('multilayer', 'perceptrons'), Counter({'and': 1})), (('perceptrons', 'and'), Counter({'dropout': 1})), (('and', 'dropout'), Counter({';': 1})), (('dropout', ';'), Counter({'we': 1})), (('new', 'type'), Counter({'of': 1})), (('type', 'of'), Counter({'hidden': 1})), (('of', 'hidden'), Counter({'layer': 1})), (('hidden', 'layer'), Counter({'for': 1})), (('layer', 'for'), Counter({'a': 1})), (('a', 'multilayer'), Counter({'perceptron': 1})), (('multilayer', 'perceptron'), Counter({',': 1})), (('perceptron', ','), Counter({'and': 1})), (('and', 'demonstrate'), Counter({'that': 1})), (('demonstrate', 'that'), Counter({'it': 1})), (('it', 'obtains'), Counter({'the': 1})), (('obtains', 'the'), Counter({'best': 1})), (('the', 'best'), Counter({'reported': 1})), (('best', 'reported'), Counter({'performance': 1})), (('reported', 'performance'), Counter({'for': 1})), (('performance', 'for'), Counter({'an': 1})), (('for', 'an'), Counter({'mlp': 1})), (('an', 'mlp'), Counter({'on': 1})), (('mlp', 'on'), Counter({'the': 1})), (('the', 'mnist'), Counter({'dataset': 1})), (('mnist', 'dataset'), Counter({'.': 1})), (('dataset', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'technical'), Counter({'report': 1})), (('report', ':'), Counter({'a': 1})), (('a', 'tool'), Counter({'for': 1})), (('tool', 'for'), Counter({'measuring': 1})), (('for', 'measuring'), Counter({'prosodic': 1, 'predictive': 1})), (('measuring', 'prosodic'), Counter({'accommodation': 1})), (('prosodic', 'accommodation'), Counter({';': 1})), (('accommodation', ';'), Counter({'this': 1})), (('article', 'has'), Counter({'been': 1})), (('because', 'the'), Counter({'submitter': 1})), (('the', 'submitter'), Counter({'did': 1})), (('submitter', 'did'), Counter({'not': 1})), (('did', 'not'), Counter({'have': 1})), (('not', 'have'), Counter({'the': 1})), (('have', 'the'), Counter({'legal': 1})), (('the', 'legal'), Counter({'authority': 1})), (('legal', 'authority'), Counter({'to': 1})), (('authority', 'to'), Counter({'grant': 1})), (('to', 'grant'), Counter({'the': 1})), (('grant', 'the'), Counter({'license': 1})), (('the', 'license'), Counter({'applied': 1})), (('license', 'applied'), Counter({'to': 1})), (('applied', 'to'), Counter({'the': 1})), (('the', 'work'), Counter({'.': 1})), (('work', '.'), Counter({'_EOS_': 1})), (('a', 'remark'), Counter({'on': 1})), (('remark', 'on'), Counter({'higher': 1})), (('on', 'higher'), Counter({'order': 1})), (('higher', 'order'), Counter({'rue': 2})), (('order', 'rue'), Counter({'-': 3})), (('rue', '-'), Counter({'resolution': 3})), (('-', 'resolution'), Counter({'with': 1, 'does': 1, 'approach': 1})), (('resolution', 'with'), Counter({'extrue': 1})), (('with', 'extrue'), Counter({';': 1})), (('extrue', ';'), Counter({'we': 1})), (('that', 'a'), Counter({'prominent': 1})), (('a', 'prominent'), Counter({'counterexample': 1})), (('prominent', 'counterexample'), Counter({'for': 1})), (('counterexample', 'for'), Counter({'the': 1})), (('the', 'completeness'), Counter({'of': 1})), (('completeness', 'of'), Counter({'first': 1})), (('of', 'first'), Counter({'order': 1})), (('first', 'order'), Counter({'rue': 1})), (('resolution', 'does'), Counter({'not': 1})), (('does', 'not'), Counter({'apply': 1})), (('not', 'apply'), Counter({'to': 1})), (('apply', 'to'), Counter({'the': 1})), (('the', 'higher'), Counter({'order': 1})), (('resolution', 'approach'), Counter({'extrue': 1})), (('approach', 'extrue'), Counter({'.': 1})), (('extrue', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'sat'), Counter({'as': 1})), (('sat', 'as'), Counter({'a': 1})), (('a', 'game'), Counter({';': 1})), (('game', ';'), Counter({'we': 1})), (('a', 'funny'), Counter({'representation': 1})), (('funny', 'representation'), Counter({'of': 1})), (('of', 'sat'), Counter({'.': 1})), (('sat', '.'), Counter({'while': 1})), (('.', 'while'), Counter({'the': 1})), (('while', 'the'), Counter({'primary': 1})), (('the', 'primary'), Counter({'interest': 1})), (('primary', 'interest'), Counter({'is': 1})), (('interest', 'is'), Counter({'to': 1})), (('is', 'to'), Counter({'present': 1, 'develop': 1})), (('to', 'present'), Counter({'propositional': 1})), (('present', 'propositional'), Counter({'satisfiability': 1})), (('propositional', 'satisfiability'), Counter({'in': 1})), (('satisfiability', 'in'), Counter({'a': 1})), (('a', 'playful'), Counter({'way': 1})), (('playful', 'way'), Counter({'for': 1})), (('way', 'for'), Counter({'pedagogical': 1})), (('for', 'pedagogical'), Counter({'purposes': 1})), (('pedagogical', 'purposes'), Counter({',': 1})), (('purposes', ','), Counter({'it': 1})), ((',', 'it'), Counter({'could': 1})), (('it', 'could'), Counter({'also': 1})), (('could', 'also'), Counter({'inspire': 1})), (('also', 'inspire'), Counter({'new': 1})), (('inspire', 'new'), Counter({'search': 1})), (('new', 'search'), Counter({'heuristics': 1})), (('search', 'heuristics'), Counter({'.': 1})), (('heuristics', '.'), Counter({'_EOS_': 1})), (('on', 'adjusting'), Counter({'$': 1})), (('adjusting', '$'), Counter({'r': 1})), (('$', 'r'), Counter({'^': 1})), (('r', '^'), Counter({'2': 2})), (('^', '2'), Counter({'$': 1, '$)': 1})), (('2', '$'), Counter({'for': 1})), (('$', 'for'), Counter({'using': 1})), (('for', 'using'), Counter({'with': 1})), (('using', 'with'), Counter({'cross': 1})), (('with', 'cross'), Counter({'-': 1})), (('cross', '-'), Counter({'validation': 2})), (('-', 'validation'), Counter({';': 1, '.': 1})), (('validation', ';'), Counter({'we': 1})), (('show', 'how'), Counter({'to': 1})), (('to', 'adjust'), Counter({'the': 1})), (('adjust', 'the'), Counter({'coefficient': 1})), (('the', 'coefficient'), Counter({'of': 1})), (('coefficient', 'of'), Counter({'determination': 1})), (('of', 'determination'), Counter({'($': 1})), (('determination', '($'), Counter({'r': 1})), (('($', 'r'), Counter({'^': 1})), (('2', '$)'), Counter({'when': 1})), (('$)', 'when'), Counter({'used': 1})), (('when', 'used'), Counter({'for': 1})), (('used', 'for'), Counter({'measuring': 1})), (('measuring', 'predictive'), Counter({'accuracy': 1})), (('predictive', 'accuracy'), Counter({'via': 1})), (('accuracy', 'via'), Counter({'leave': 1})), (('via', 'leave'), Counter({'-': 1})), (('leave', '-'), Counter({'one': 1})), (('-', 'one'), Counter({'-': 1})), (('-', 'out'), Counter({'cross': 1})), (('out', 'cross'), Counter({'-': 1})), (('validation', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'approximated'), Counter({'structured': 1})), (('approximated', 'structured'), Counter({'prediction': 1})), (('structured', 'prediction'), Counter({'for': 1, '\".': 1})), (('prediction', 'for'), Counter({'learning': 1})), (('for', 'learning'), Counter({'large': 1})), (('learning', 'large'), Counter({'scale': 1})), (('scale', 'graphical'), Counter({'models': 1})), (('graphical', 'models'), Counter({';': 1})), (('models', ';'), Counter({'this': 1})), (('this', 'manuscripts'), Counter({'contains': 1})), (('manuscripts', 'contains'), Counter({'the': 1})), (('the', 'proofs'), Counter({'for': 1})), (('proofs', 'for'), Counter({'\"': 1})), (('for', '\"'), Counter({'a': 1})), (('a', 'primal'), Counter({'-': 1})), (('primal', '-'), Counter({'dual': 1})), (('dual', 'message'), Counter({'-': 1})), (('message', '-'), Counter({'passing': 1})), (('-', 'passing'), Counter({'algorithm': 1})), (('passing', 'algorithm'), Counter({'for': 1})), (('algorithm', 'for'), Counter({'approximated': 1})), (('for', 'approximated'), Counter({'large': 1})), (('approximated', 'large'), Counter({'scale': 1})), (('scale', 'structured'), Counter({'prediction': 1})), (('prediction', '\".'), Counter({'_EOS_': 1})), (('automatic', 'liver'), Counter({'segmentation': 1})), (('liver', 'segmentation'), Counter({'method': 1})), (('segmentation', 'method'), Counter({'in': 1})), (('method', 'in'), Counter({'ct': 1})), (('in', 'ct'), Counter({'images': 1})), (('ct', 'images'), Counter({';': 1})), (('the', 'aim'), Counter({'of': 1})), (('aim', 'of'), Counter({'this': 1})), (('work', 'is'), Counter({'to': 1})), (('develop', 'a'), Counter({'method': 1})), (('method', 'for'), Counter({'automatic': 1})), (('for', 'automatic'), Counter({'segmentation': 1})), (('automatic', 'segmentation'), Counter({'of': 1})), (('segmentation', 'of'), Counter({'the': 1})), (('the', 'liver'), Counter({'based': 1, '.': 1})), (('liver', 'based'), Counter({'on': 1})), (('a', 'priori'), Counter({'knowledge': 1})), (('priori', 'knowledge'), Counter({'of': 1})), (('knowledge', 'of'), Counter({'the': 1})), (('the', 'image'), Counter({',': 1})), (('image', ','), Counter({'such': 1})), ((',', 'such'), Counter({'as': 1})), (('such', 'as'), Counter({'location': 1})), (('as', 'location'), Counter({'and': 1})), (('location', 'and'), Counter({'shape': 1})), (('and', 'shape'), Counter({'of': 1})), (('shape', 'of'), Counter({'the': 1})), (('liver', '.'), Counter({'_EOS_': 1})), (('the', 'existence'), Counter({'of': 3})), (('existence', 'of'), Counter({'a': 3})), (('a', 'projective'), Counter({'reconstruction': 2})), (('projective', 'reconstruction'), Counter({';': 1, 'and': 1})), (('we', 'study'), Counter({'the': 1})), (('the', 'connection'), Counter({'between': 1})), (('connection', 'between'), Counter({'the': 1})), (('between', 'the'), Counter({'existence': 1})), (('reconstruction', 'and'), Counter({'the': 1})), (('a', 'fundamental'), Counter({'matrix': 1})), (('fundamental', 'matrix'), Counter({'satisfying': 1})), (('matrix', 'satisfying'), Counter({'the': 1})), (('satisfying', 'the'), Counter({'epipolar': 1})), (('the', 'epipolar'), Counter({'constraints': 1})), (('epipolar', 'constraints'), Counter({'.': 1})), (('constraints', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'solving'), Counter({'traveling': 1})), (('solving', 'traveling'), Counter({'salesman': 2})), (('traveling', 'salesman'), Counter({'problem': 2})), (('salesman', 'problem'), Counter({'by': 1, '.': 1})), (('problem', 'by'), Counter({'marker': 1})), (('by', 'marker'), Counter({'method': 1})), (('marker', 'method'), Counter({';': 1, 'and': 1})), (('method', ';'), Counter({'in': 1})), (('use', 'marker'), Counter({'method': 1})), (('method', 'and'), Counter({'propose': 1})), (('and', 'propose'), Counter({'a': 1})), (('new', 'mutation'), Counter({'operator': 1})), (('mutation', 'operator'), Counter({'that': 1})), (('operator', 'that'), Counter({'selects': 1})), (('that', 'selects'), Counter({'the': 1})), (('selects', 'the'), Counter({'nearest': 1})), (('the', 'nearest'), Counter({'neighbor': 1})), (('nearest', 'neighbor'), Counter({'among': 1})), (('neighbor', 'among'), Counter({'all': 1})), (('among', 'all'), Counter({'near': 1})), (('all', 'near'), Counter({'neighbors': 1})), (('near', 'neighbors'), Counter({'solving': 1})), (('neighbors', 'solving'), Counter({'traveling': 1})), (('problem', '.'), Counter({'_EOS_': 1})), (('a', 'primer'), Counter({'on': 1})), (('primer', 'on'), Counter({'answer': 1})), (('on', 'answer'), Counter({'set': 1})), (('a', 'introduction'), Counter({'to': 1})), (('the', 'syntax'), Counter({'and': 1})), (('syntax', 'and'), Counter({'semantics': 1})), (('and', 'semantics'), Counter({'of': 1})), (('of', 'answer'), Counter({'set': 1})), (('programming', 'intended'), Counter({'as': 1})), (('intended', 'as'), Counter({'an': 1})), (('as', 'an'), Counter({'handout': 1, 'example': 1})), (('an', 'handout'), Counter({'to': 1})), (('handout', 'to'), Counter({'[': 1})), (('to', '['), Counter({'under': 1})), (('[', 'under'), Counter({']': 1})), (('under', ']'), Counter({'graduate': 1})), ((']', 'graduate'), Counter({'students': 1})), (('graduate', 'students'), Counter({'taking': 1})), (('students', 'taking'), Counter({'artificial': 1})), (('taking', 'artificial'), Counter({'intlligence': 1})), (('artificial', 'intlligence'), Counter({'or': 1})), (('intlligence', 'or'), Counter({'logic': 1})), (('or', 'logic'), Counter({'programming': 1})), (('programming', 'classes'), Counter({'.': 1})), (('classes', '.'), Counter({'_EOS_': 1})), (('_UNK_', 'agent'), Counter({'models': 1})), (('agent', 'models'), Counter({'of': 1})), (('models', 'of'), Counter({'political': 1})), (('of', 'political'), Counter({'interactions': 1})), (('political', 'interactions'), Counter({';': 1})), (('interactions', ';'), Counter({'looks': 1})), ((';', 'looks'), Counter({'at': 1})), (('looks', 'at'), Counter({'state': 1})), (('at', 'state'), Counter({'interactions': 1})), (('state', 'interactions'), Counter({'from': 1, 'as': 1})), (('interactions', 'from'), Counter({'an': 1})), (('from', 'an'), Counter({'agent': 1})), (('an', 'agent'), Counter({'based': 1})), (('agent', 'based'), Counter({'ai': 1})), (('based', 'ai'), Counter({'perspective': 1})), (('ai', 'perspective'), Counter({'to': 1})), (('perspective', 'to'), Counter({'see': 1})), (('to', 'see'), Counter({'state': 1})), (('see', 'state'), Counter({'interactions': 1})), (('interactions', 'as'), Counter({'an': 1})), (('an', 'example'), Counter({'of': 1})), (('example', 'of'), Counter({'emergent': 1})), (('of', 'emergent'), Counter({'intelligent': 1})), (('emergent', 'intelligent'), Counter({'behavior': 1})), (('intelligent', 'behavior'), Counter({'.': 1})), (('behavior', '.'), Counter({'exposes': 1})), (('.', 'exposes'), Counter({'basic': 1})), (('exposes', 'basic'), Counter({'principles': 1})), (('basic', 'principles'), Counter({'of': 1})), (('principles', 'of'), Counter({'game': 1})), (('of', 'game'), Counter({'theory': 1})), (('learning', 'states'), Counter({'representations': 1})), (('states', 'representations'), Counter({'in': 1})), (('representations', 'in'), Counter({'pomdp': 1})), (('in', 'pomdp'), Counter({';': 1})), (('pomdp', ';'), Counter({'we': 1})), (('propose', 'to'), Counter({'deal': 1})), (('to', 'deal'), Counter({'with': 1})), (('deal', 'with'), Counter({'sequential': 1})), (('with', 'sequential'), Counter({'processes': 1})), (('sequential', 'processes'), Counter({'where': 1})), (('processes', 'where'), Counter({'only': 1})), (('where', 'only'), Counter({'partial': 1})), (('only', 'partial'), Counter({'observations': 1})), (('partial', 'observations'), Counter({'are': 1})), (('observations', 'are'), Counter({'available': 1})), (('are', 'available'), Counter({'by': 1})), (('available', 'by'), Counter({'learning': 1})), (('by', 'learning'), Counter({'a': 1})), (('learning', 'a'), Counter({'latent': 1})), (('a', 'latent'), Counter({'representation': 1})), (('latent', 'representation'), Counter({'space': 1})), (('representation', 'space'), Counter({'on': 1})), (('space', 'on'), Counter({'which': 1})), (('on', 'which'), Counter({'policies': 1})), (('which', 'policies'), Counter({'may': 1})), (('policies', 'may'), Counter({'be': 1})), (('may', 'be'), Counter({'accurately': 1})), (('be', 'accurately'), Counter({'learned': 1})), (('accurately', 'learned'), Counter({'.': 1})), (('learned', '.'), Counter({'_EOS_': 1}))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### count_ngrams v02"
      ],
      "metadata": {
        "id": "x6vFTfRm2VT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "# pypi\n",
        "import attr\n",
        "\n",
        "def count_n_grams(data: list, n: int, start_token: str='<s>', end_token: str='<e>') -> dict:\n",
        "    \"\"\"\n",
        "    Count all n-grams in the data\n",
        "\n",
        "    Args:\n",
        "       data: List of lists of words\n",
        "       n: number of words in a sequence\n",
        "\n",
        "    Returns:\n",
        "       A dictionary that maps a tuple of n-words to its frequency\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize dictionary of n-grams and their counts\n",
        "    n_grams = {}\n",
        "\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "    # Go through each sentence in the data\n",
        "    for sentence in data: # complete this line\n",
        "\n",
        "        # prepend start token n times, and  append <e> one time\n",
        "        sentence = [start_token] * n + sentence + [end_token]\n",
        "\n",
        "        # convert list to tuple\n",
        "        # So that the sequence of words can be used as\n",
        "        # a key in the dictionary\n",
        "        sentence = tuple(sentence)\n",
        "\n",
        "        # Use 'i' to indicate the start of the n-gram\n",
        "        # from index 0\n",
        "        # to the last index where the end of the n-gram\n",
        "        # is within the sentence.\n",
        "\n",
        "        for i in range(0, len(sentence) - (n - 1)): # complete this line\n",
        "\n",
        "            # Get the n-gram from i to i+n\n",
        "            n_gram = sentence[i: i + n]\n",
        "\n",
        "            # check if the n-gram is in the dictionary\n",
        "            if n_gram in n_grams: # complete this line\n",
        "\n",
        "                # Increment the count for this n-gram\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                # Initialize this n-gram count to 1\n",
        "                n_grams[n_gram] = 1\n",
        "\n",
        "            ### END CODE HERE ###\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "GUxvMQhU3zaT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test It"
      ],
      "metadata": {
        "id": "owuE_YeK37gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_lines2 = [s.split() for s in dummy_lines]\n",
        "dummy_counts = count_n_grams(dummy_lines2, n=3)\n",
        "list(dummy_counts)[:5]"
      ],
      "metadata": {
        "id": "x5BgVtJE38-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3a87ac-14f0-484a-d4d1-6cc1436d5714"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', '<s>', '<s>'),\n",
              " ('<s>', '<s>', 'differential'),\n",
              " ('<s>', 'differential', 'contrastive'),\n",
              " ('differential', 'contrastive', 'divergence'),\n",
              " ('contrastive', 'divergence', ';')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n =3\n",
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "_n_grams = chain.from_iterable([\n",
        "                [sentence[cut: cut + n] for cut in range(0, len(sentence) - (n - 1))]\n",
        "                for sentence in sentences\n",
        "            ])\n"
      ],
      "metadata": {
        "id": "pBI9mx5t24pB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(_n_grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBFUz9Ay_-O2",
        "outputId": "65b072e1-1eaa-479e-c983-52e11ae91987"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i', 'like', 'a'],\n",
              " ['like', 'a', 'cat'],\n",
              " ['this', 'dog', 'is'],\n",
              " ['dog', 'is', 'like'],\n",
              " ['is', 'like', 'a'],\n",
              " ['like', 'a', 'cat']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build nodel"
      ],
      "metadata": {
        "id": "_TUcXDrSnIQc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVXWJIUUq4R-"
      },
      "source": [
        "Once we can count N-grams, we can build a probabilistic language model.\n",
        "The simplest way to compute probabilities is in proporiton to counts:\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0Z5wuH-1q4R-"
      },
      "outputs": [],
      "source": [
        "class NGramLanguageModel:    \n",
        "    def __init__(self, lines, n):\n",
        "        \"\"\" \n",
        "        Train a simple count-based language model: \n",
        "        compute probabilities P(w_t | prefix) given ngram counts\n",
        "        \n",
        "        :param n: computes probability of next token given (n - 1) previous words\n",
        "        :param lines: an iterable of strings with space-separated tokens\n",
        "        \"\"\"\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "    \n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        \n",
        "        # compute token proabilities given counts\n",
        "        #self.probs = defaultdict(Counter)\n",
        "        self.probs = defaultdict(dict)\n",
        "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
        "        \n",
        "        # populate self.probs with actual probabilities\n",
        "        #<YOUR CODE>\n",
        "        for prefix, token_counts in counts.items():\n",
        "          local_sum = sum(token_counts.values())\n",
        "          self.probs[prefix] = {\n",
        "              token: count / local_sum \n",
        "              for token, count in token_counts.items()\n",
        "          }\n",
        "            \n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
        "        \"\"\"\n",
        "        prefix = prefix.split()\n",
        "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
        "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
        "        return self.probs[tuple(prefix)]\n",
        "    \n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :param next_token: the next token to predict probability for\n",
        "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
        "        \"\"\"\n",
        "        #return self.get_possible_next_tokens(prefix).get(next_token, 0)\n",
        "        return self.get_possible_next_tokens(prefix).get(next_token, 1e-50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gNRtXRNq4R_"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Vsknp7fq4R_"
      },
      "outputs": [],
      "source": [
        "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
        "assert np.allclose(p_initial['learning'], 0.02)\n",
        "assert np.allclose(p_initial['a'], 0.13)\n",
        "assert np.allclose(p_initial.get('meow', 0), 0)\n",
        "assert np.allclose(sum(p_initial.values()), 1)\n",
        "\n",
        "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
        "assert np.allclose(p_a['machine'], 0.15384615)\n",
        "assert np.allclose(p_a['note'], 0.23076923)\n",
        "assert np.allclose(p_a.get('the', 0), 0)\n",
        "assert np.allclose(sum(p_a.values()), 1)\n",
        "\n",
        "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
        "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
        "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
        "    \"your 3-gram model should only depend on 2 previous words\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dummy_counts.keys())\n",
        "print(dummy_lm.probs.keys())\n",
        "print()\n",
        "\n",
        "print(dummy_counts[('_UNK_', '_UNK_')].keys())\n",
        "print(dummy_lm.probs[('_UNK_', '_UNK_')].keys())\n",
        "print()\n",
        "print(dummy_counts[('_UNK_', '_UNK_')])\n",
        "print(dummy_lm.probs[('_UNK_', '_UNK_')])\n",
        "print(dummy_lm.probs[('_UNK_', '_UNK_')]['what'])\n",
        "\n",
        "print()\n",
        "print(dummy_lm.probs[('differential', 'contrastive')])\n",
        "print(dummy_lm.probs[('we', 'claim')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0BqZgG5npQA",
        "outputId": "d6446e04-9786-4c10-a4ce-1ddb22d637cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([('_UNK_', '_UNK_'), ('_UNK_', 'differential'), ('differential', 'contrastive'), ('contrastive', 'divergence'), ('divergence', ';'), (';', 'this'), ('this', 'paper'), ('paper', 'has'), ('has', 'been'), ('been', 'retracted'), ('retracted', '.'), ('_UNK_', 'what'), ('what', 'does'), ('does', 'artificial'), ('artificial', 'life'), ('life', 'tell'), ('tell', 'us'), ('us', 'about'), ('about', 'death'), ('death', '?'), ('?', ';'), (';', 'short'), ('short', 'philosophical'), ('philosophical', 'essay'), ('_UNK_', 'p'), ('p', '='), ('=', 'np'), ('np', ';'), (';', 'we'), ('we', 'claim'), ('claim', 'to'), ('to', 'resolve'), ('resolve', 'the'), ('the', 'p'), ('p', '=?'), ('=?', 'np'), ('np', 'problem'), ('problem', 'via'), ('via', 'a'), ('a', 'formal'), ('formal', 'argument'), ('argument', 'for'), ('for', 'p'), ('np', '.'), ('_UNK_', 'computational'), ('computational', 'geometry'), ('geometry', 'column'), ('column', '38'), ('38', ';'), (';', 'recent'), ('recent', 'results'), ('results', 'on'), ('on', 'curve'), ('curve', 'reconstruction'), ('reconstruction', 'are'), ('are', 'described'), ('described', '.'), ('_UNK_', 'weak'), ('weak', 'evolvability'), ('evolvability', 'equals'), ('equals', 'strong'), ('strong', 'evolvability'), ('evolvability', ';'), (';', 'an'), ('an', 'updated'), ('updated', 'version'), ('version', 'will'), ('will', 'be'), ('be', 'uploaded'), ('uploaded', 'later'), ('later', '.'), ('_UNK_', 'creating'), ('creating', 'a'), ('a', 'new'), ('new', 'ontology'), ('ontology', ':'), (':', 'a'), ('a', 'modular'), ('modular', 'approach'), ('approach', ';'), (';', 'creating'), ('_UNK_', 'defeasible'), ('defeasible', 'reasoning'), ('reasoning', 'in'), ('in', 'oscar'), ('oscar', ';'), ('this', 'is'), ('is', 'a'), ('a', 'system'), ('system', 'description'), ('description', 'for'), ('for', 'the'), ('the', 'oscar'), ('oscar', 'defeasible'), ('defeasible', 'reasoner'), ('reasoner', '.'), ('_UNK_', 'essence'), ('essence', \"'\"), (\"'\", 'description'), ('description', ';'), (';', 'a'), ('a', 'description'), ('description', 'of'), ('of', 'the'), ('the', 'essence'), (\"'\", 'language'), ('language', 'as'), ('as', 'used'), ('used', 'by'), ('by', 'the'), ('the', 'tool'), ('tool', 'savile'), ('savile', 'row'), ('row', '.'), ('_UNK_', 'deep'), ('deep', 'neural'), ('neural', 'networks'), ('networks', '-'), ('-', 'a'), ('a', 'brief'), ('brief', 'history'), ('history', ';'), (';', 'introduction'), ('introduction', 'to'), ('to', 'deep'), ('networks', 'and'), ('and', 'their'), ('their', 'history'), ('history', '.'), ('_UNK_', 'statistical'), ('statistical', 'physics'), ('physics', 'for'), ('for', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', ';'), ('been', 'withdrawn'), ('withdrawn', 'by'), ('the', 'author'), ('author', '.'), ('_UNK_', 'complex'), ('complex', 'networks'), ('networks', ';'), ('to', 'the'), ('the', 'special'), ('special', 'issue'), ('issue', 'on'), ('on', 'complex'), ('networks', ','), (',', 'artificial'), ('life', 'journal'), ('journal', '.'), ('_UNK_', 'serious'), ('serious', 'flaws'), ('flaws', 'in'), ('in', 'korf'), ('korf', 'et'), ('et', 'al'), ('al', \".'\"), (\".'\", 's'), ('s', 'analysis'), ('analysis', 'on'), ('on', 'time'), ('time', 'complexity'), ('complexity', 'of'), ('of', 'a'), ('a', '*'), ('*', ';'), ('withdrawn', '.'), ('_UNK_', 'preprocessing'), ('preprocessing', ':'), ('a', 'step'), ('step', 'in'), ('in', 'automating'), ('automating', 'early'), ('early', 'detection'), ('detection', 'of'), ('of', 'cervical'), ('cervical', 'cancer'), ('cancer', ';'), ('_UNK_', 'liquid'), ('liquid', 'state'), ('state', 'machines'), ('machines', 'in'), ('in', 'adbiatic'), ('adbiatic', 'quantum'), ('quantum', 'computers'), ('computers', 'for'), ('for', 'general'), ('general', 'computation'), ('computation', ';'), (';', 'major'), ('major', 'mistakes'), ('mistakes', 'do'), ('do', 'not'), ('not', 'read'), ('_UNK_', 'mining'), ('mining', 'for'), ('for', 'trees'), ('trees', 'in'), ('in', 'a'), ('a', 'graph'), ('graph', 'is'), ('is', 'np'), ('np', '-'), ('-', 'complete'), ('complete', ';'), (';', 'mining'), ('is', 'shown'), ('shown', 'to'), ('to', 'be'), ('be', 'np'), ('complete', '.'), ('_UNK_', 'towards'), ('towards', 'a'), ('a', 'hierarchical'), ('hierarchical', 'model'), ('model', 'of'), ('of', 'consciousness'), ('consciousness', ','), (',', 'intelligence'), ('intelligence', ','), (',', 'mind'), ('mind', 'and'), ('and', 'body'), ('body', ';'), ('this', 'article'), ('article', 'is'), ('is', 'taken'), ('taken', 'out'), ('out', '.'), ('_UNK_', 'a'), ('a', 'notation'), ('notation', 'for'), ('for', 'markov'), ('markov', 'decision'), ('decision', 'processes'), ('processes', ';'), ('paper', 'specifies'), ('specifies', 'a'), ('processes', '.'), ('_UNK_', 'icon'), ('icon', 'challenge'), ('challenge', 'on'), ('on', 'algorithm'), ('algorithm', 'selection'), ('selection', ';'), ('we', 'present'), ('present', 'the'), ('the', 'results'), ('results', 'of'), ('the', 'icon'), ('selection', '.'), ('_UNK_', 'recognition'), ('recognition', 'of'), ('of', 'regular'), ('regular', 'shapes'), ('shapes', 'in'), ('in', 'satelite'), ('satelite', 'images'), ('images', ';'), ('author', 'ali'), ('ali', 'pourmohammad'), ('pourmohammad', '.'), ('_UNK_', 'glottochronologic'), ('glottochronologic', 'retrognostic'), ('retrognostic', 'of'), ('of', 'language'), ('language', 'system'), ('system', ';'), ('a', 'glottochronologic'), ('system', 'is'), ('is', 'proposed'), ('_UNK_', 'the'), ('the', 'model'), ('of', 'quantum'), ('quantum', 'evolution'), ('evolution', ';'), ('author', 'due'), ('due', 'to'), ('to', 'extremely'), ('extremely', 'unscientific'), ('unscientific', 'errors'), ('errors', '.'), ('_UNK_', 'utility'), ('utility', '-'), ('-', 'probability'), ('probability', 'duality'), ('duality', ';'), ('paper', 'presents'), ('presents', 'duality'), ('duality', 'between'), ('between', 'probability'), ('probability', 'distributions'), ('distributions', 'and'), ('and', 'utility'), ('utility', 'functions'), ('functions', '.'), ('_UNK_', 'temporized'), ('temporized', 'equilibria'), ('equilibria', ';'), ('to', 'a'), ('a', 'crucial'), ('crucial', 'error'), ('error', 'in'), ('in', 'the'), ('the', 'submission'), ('submission', 'action'), ('action', '.'), ('_UNK_', 'backpropagation'), ('backpropagation', 'in'), ('in', 'matrix'), ('matrix', 'notation'), ('notation', ';'), (';', 'in'), ('in', 'this'), ('this', 'note'), ('note', 'we'), ('we', 'calculate'), ('calculate', 'the'), ('the', 'gradient'), ('gradient', 'of'), ('the', 'network'), ('network', 'function'), ('function', 'in'), ('notation', '.'), ('_UNK_', 'random'), ('random', 'dfas'), ('dfas', 'are'), ('are', 'efficiently'), ('efficiently', 'pac'), ('pac', 'learnable'), ('learnable', ';'), ('withdrawn', 'due'), ('to', 'an'), ('an', 'error'), ('error', 'found'), ('found', 'by'), ('by', 'dana'), ('dana', 'angluin'), ('angluin', 'and'), ('and', 'lev'), ('lev', 'reyzin'), ('reyzin', '.'), ('_UNK_', 'network'), ('network', 'motifs'), ('motifs', 'in'), ('in', 'music'), ('music', 'sequences'), ('sequences', ';'), ('author', 'because'), ('because', 'it'), ('it', 'needs'), ('needs', 'a'), ('a', 'deep'), ('deep', 'methodological'), ('methodological', 'revision'), ('revision', '.'), ('_UNK_', 'glottochronology'), ('glottochronology', 'and'), ('and', 'problems'), ('problems', 'of'), ('of', 'protolanguage'), ('protolanguage', 'reconstruction'), ('reconstruction', ';'), ('a', 'method'), ('method', 'of'), ('of', 'languages'), ('languages', 'genealogical'), ('genealogical', 'trees'), ('trees', 'construction'), ('construction', 'is'), ('proposed', '.'), ('_UNK_', 'using'), ('using', 'slp'), ('slp', 'neural'), ('neural', 'network'), ('network', 'to'), ('to', 'persian'), ('persian', 'handwritten'), ('handwritten', 'digits'), ('digits', 'recognition'), ('recognition', ';'), ('_UNK_', 'time'), ('time', 'hopping'), ('hopping', 'technique'), ('technique', 'for'), ('for', 'faster'), ('faster', 'reinforcement'), ('reinforcement', 'learning'), ('learning', 'in'), ('in', 'simulations'), ('simulations', ';'), ('this', 'preprint'), ('preprint', 'has'), ('author', 'for'), ('for', 'revision'), ('_UNK_', 'convolutional'), ('convolutional', 'matching'), ('matching', 'pursuit'), ('pursuit', 'and'), ('and', 'dictionary'), ('dictionary', 'training'), ('training', ';'), (';', 'matching'), ('and', 'k'), ('k', '-'), ('-', 'svd'), ('svd', 'is'), ('is', 'demonstrated'), ('demonstrated', 'in'), ('the', 'translation'), ('translation', 'invariant'), ('invariant', 'setting'), ('_UNK_', 'fitness'), ('fitness', 'landscape'), ('landscape', 'analysis'), ('analysis', 'for'), ('for', 'dynamic'), ('dynamic', 'resource'), ('resource', 'allocation'), ('allocation', 'in'), ('in', 'multiuser'), ('multiuser', 'ofdm'), ('ofdm', 'based'), ('based', 'cognitive'), ('cognitive', 'radio'), ('radio', 'systems'), ('systems', ';'), ('a', 'note'), ('note', 'on'), ('on', 'darwiche'), ('darwiche', 'and'), ('and', 'pearl'), ('pearl', ';'), (';', 'it'), ('it', 'is'), ('shown', 'that'), ('that', 'darwiche'), ('pearl', \"'\"), (\"'\", 's'), ('s', 'postulates'), ('postulates', 'imply'), ('imply', 'an'), ('an', 'interesting'), ('interesting', 'property'), ('property', ','), (',', 'not'), ('not', 'noticed'), ('noticed', 'by'), ('the', 'authors'), ('authors', '.'), ('_UNK_', 'flip'), ('flip', '-'), ('-', 'flop'), ('flop', 'sublinear'), ('sublinear', 'models'), ('models', 'for'), ('for', 'graphs'), ('graphs', ':'), (':', 'proof'), ('proof', 'of'), ('of', 'theorem'), ('theorem', '1'), ('1', ';'), ('we', 'prove'), ('prove', 'that'), ('that', 'there'), ('there', 'is'), ('is', 'no'), ('no', 'class'), ('class', '-'), ('-', 'dual'), ('dual', 'for'), ('for', 'almost'), ('almost', 'all'), ('all', 'sublinear'), ('models', 'on'), ('on', 'graphs'), ('graphs', '.'), ('_UNK_', 'autonomous'), ('autonomous', 'perceptron'), ('perceptron', 'neural'), ('network', 'inspired'), ('inspired', 'from'), ('from', 'quantum'), ('quantum', 'computing'), ('computing', ';'), ('this', 'abstract'), ('abstract', 'will'), ('be', 'modified'), ('modified', 'after'), ('after', 'correcting'), ('correcting', 'the'), ('the', 'minor'), ('minor', 'error'), ('in', 'eq'), ('eq', '.('), ('.(', '2'), ('2', ')'), ('using', 'sets'), ('sets', 'of'), ('of', 'probability'), ('probability', 'measures'), ('measures', 'to'), ('to', 'represent'), ('represent', 'uncertainty'), ('uncertainty', ';'), (';', 'i'), ('i', 'explore'), ('explore', 'the'), ('the', 'use'), ('use', 'of'), ('of', 'sets'), ('measures', 'as'), ('as', 'a'), ('a', 'representation'), ('representation', 'of'), ('of', 'uncertainty'), ('uncertainty', '.'), ('a', 'comment'), ('comment', 'on'), ('on', 'argumentation'), ('argumentation', ';'), ('we', 'use'), ('use', 'the'), ('the', 'theory'), ('theory', 'of'), ('of', 'defaults'), ('defaults', 'and'), ('their', 'meaning'), ('meaning', 'of'), ('of', '['), ('[', 'gs16'), ('gs16', ']'), (']', 'to'), ('to', 'develop'), ('develop', '('), ('(', 'the'), ('the', 'outline'), ('outline', 'of'), ('a', ')'), (')', 'new'), ('new', 'theory'), ('of', 'argumentation'), ('argumentation', '.'), ('_UNK_', 'activitynet'), ('activitynet', 'challenge'), ('challenge', '2017'), ('2017', 'summary'), ('summary', ';'), (';', 'the'), ('the', 'activitynet'), ('activitynet', 'large'), ('large', 'scale'), ('scale', 'activity'), ('activity', 'recognition'), ('recognition', 'challenge'), ('summary', ':'), (':', 'results'), ('results', 'and'), ('and', 'challenge'), ('challenge', 'participants'), ('participants', 'papers'), ('papers', '.'), ('the', 'yahoo'), ('yahoo', 'query'), ('query', 'treebank'), ('treebank', ','), (',', 'v'), ('v', '.'), ('.', '1'), ('1', '.'), ('.', '0'), ('0', ';'), ('description', 'and'), ('and', 'annotation'), ('annotation', 'guidelines'), ('guidelines', 'for'), ('yahoo', 'webscope'), ('webscope', 'release'), ('release', 'of'), ('of', 'query'), (',', 'version'), ('version', '1'), ('0', ','), (',', 'may'), ('may', '2016'), ('2016', '.'), ('_UNK_', 'decision'), ('decision', 'under'), ('under', 'uncertainty'), ('we', 'derive'), ('derive', 'axiomatically'), ('axiomatically', 'the'), ('the', 'probability'), ('probability', 'function'), ('function', 'that'), ('that', 'should'), ('should', 'be'), ('be', 'used'), ('used', 'to'), ('to', 'make'), ('make', 'decisions'), ('decisions', 'given'), ('given', 'any'), ('any', 'form'), ('form', 'of'), ('of', 'underlying'), ('underlying', 'uncertainty'), ('_UNK_', 'text'), ('text', 'analysis'), ('analysis', 'tools'), ('tools', 'in'), ('in', 'spoken'), ('spoken', 'language'), ('this', 'submission'), ('submission', 'contains'), ('contains', 'the'), ('the', 'postscript'), ('postscript', 'of'), ('the', 'final'), ('final', 'version'), ('version', 'of'), ('the', 'slides'), ('slides', 'used'), ('used', 'in'), ('in', 'our'), ('our', 'acl'), ('acl', '-'), ('-', '94'), ('94', 'tutorial'), ('tutorial', '.'), ('_UNK_', 'discrimination'), ('discrimination', 'between'), ('between', 'arabic'), ('arabic', 'and'), ('and', 'latin'), ('latin', 'from'), ('from', 'bilingual'), ('bilingual', 'documents'), ('documents', ';'), (';', '2011'), ('2011', 'international'), ('international', 'conference'), ('conference', 'on'), ('on', 'communications'), ('communications', ','), (',', 'computing'), ('computing', 'and'), ('and', 'control'), ('control', 'applications'), ('applications', '('), ('(', 'ccca'), ('ccca', ')'), ('a', 'machine'), ('machine', '-'), ('-', 'learning'), ('learning', 'framework'), ('framework', 'for'), ('for', 'design'), ('design', 'for'), ('for', 'manufacturability'), ('manufacturability', ';'), ('a', 'duplicate'), ('duplicate', 'submission'), ('submission', '('), ('(', 'original'), ('original', 'is'), ('is', 'arxiv'), ('arxiv', ':'), (':', '1612'), ('1612', '.'), ('.', '02141'), ('02141', ').'), (').', 'hence'), ('hence', 'want'), ('want', 'to'), ('to', 'withdraw'), ('withdraw', 'it'), ('a', 'theory'), ('of', 'experiment'), ('experiment', ';'), ('article', 'aims'), ('aims', 'at'), ('at', 'clarifying'), ('clarifying', 'the'), ('the', 'language'), ('language', 'and'), ('and', 'practice'), ('practice', 'of'), ('of', 'scientific'), ('scientific', 'experiment'), ('experiment', ','), (',', 'mainly'), ('mainly', 'by'), ('by', 'hooking'), ('hooking', 'observability'), ('observability', 'on'), ('on', 'calculability'), ('calculability', '.'), ('_UNK_', 'are'), ('are', 'minds'), ('minds', 'computable'), ('computable', '?'), ('this', 'essay'), ('essay', 'explores'), ('explores', 'the'), ('the', 'limits'), ('limits', 'of'), ('of', 'turing'), ('turing', 'machines'), ('machines', 'concerning'), ('concerning', 'the'), ('the', 'modeling'), ('modeling', 'of'), ('of', 'minds'), ('minds', 'and'), ('and', 'suggests'), ('suggests', 'alternatives'), ('alternatives', 'to'), ('to', 'go'), ('go', 'beyond'), ('beyond', 'those'), ('those', 'limits'), ('limits', '.'), ('_UNK_', 'extraction'), ('extraction', 'de'), ('de', 'concepts'), ('concepts', 'sous'), ('sous', 'contraintes'), ('contraintes', 'dans'), ('dans', 'des'), ('des', 'donn√©es'), ('donn√©es', 'd'), ('d', \"'\"), (\"'\", 'expression'), ('expression', 'de'), ('de', 'g√®nes'), ('g√®nes', ';'), ('paper', ','), (',', 'we'), ('we', 'propose'), ('propose', 'a'), ('a', 'technique'), ('technique', 'to'), ('to', 'extract'), ('extract', 'constrained'), ('constrained', 'formal'), ('formal', 'concepts'), ('concepts', '.'), ('_UNK_', 'comments'), ('comments', 'on'), ('on', '\"'), ('\"', 'a'), ('new', 'combination'), ('combination', 'of'), ('of', 'evidence'), ('evidence', 'based'), ('based', 'on'), ('on', 'compromise'), ('compromise', '\"'), ('\"', 'by'), ('by', 'k'), ('k', '.'), ('.', 'yamada'), ('yamada', ';'), (';', 'comments'), ('on', '``'), ('``', 'a'), ('compromise', \"''\"), (\"''\", 'by'), ('_UNK_', 'learning'), ('learning', 'low'), ('low', '-'), ('-', 'shot'), ('shot', 'facial'), ('facial', 'representations'), ('representations', 'via'), ('via', '2d'), ('2d', 'warping'), ('warping', ';'), ('this', 'work'), ('work', ','), ('we', 'mainly'), ('mainly', 'study'), ('study', 'the'), ('the', 'influence'), ('influence', 'of'), ('the', '2d'), ('warping', 'module'), ('module', 'for'), ('for', 'one'), ('one', '-'), ('shot', 'face'), ('face', 'recognition'), ('recognition', '.'), ('_UNK_', 'automatic'), ('automatic', 'generation'), ('generation', 'of'), ('of', 'benchmarks'), ('benchmarks', 'for'), ('for', 'plagiarism'), ('plagiarism', 'detection'), ('detection', 'tools'), ('tools', 'using'), ('using', 'grammatical'), ('grammatical', 'evolution'), ('authors', 'due'), ('a', 'major'), ('major', 'rewriting'), ('rewriting', '.'), ('_UNK_', 'resource'), ('allocation', 'of'), ('of', 'mu'), ('mu', '-'), ('-', 'ofdm'), ('systems', 'under'), ('under', 'partial'), ('partial', 'channel'), ('channel', 'state'), ('state', 'information'), ('information', ';'), ('to', 'some'), ('some', 'errors'), ('_UNK_', 'advances'), ('advances', 'in'), ('in', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'require'), ('require', 'progress'), ('progress', 'across'), ('across', 'all'), ('all', 'of'), ('of', 'computer'), ('computer', 'science'), ('science', ';'), (';', 'advances'), ('science', '.'), ('_UNK_', 'exploration'), ('exploration', 'of'), ('of', 'object'), ('object', 'recognition'), ('recognition', 'from'), ('from', '3d'), ('3d', 'point'), ('point', 'cloud'), ('cloud', ';'), ('present', 'our'), ('our', 'latest'), ('latest', 'experiment'), ('experiment', 'results'), ('cloud', 'data'), ('data', 'collected'), ('collected', 'through'), ('through', 'moving'), ('moving', 'car'), ('car', '.'), ('_UNK_', 'quantified'), ('quantified', 'conditional'), ('conditional', 'logics'), ('logics', 'are'), ('are', 'fragments'), ('fragments', 'of'), ('of', 'hol'), ('hol', ';'), ('a', 'semantic'), ('semantic', 'embedding'), ('embedding', 'of'), ('of', '('), ('(', 'constant'), ('constant', 'domain'), ('domain', ')'), (')', 'quantified'), ('conditional', 'logic'), ('logic', 'in'), ('in', 'classical'), ('classical', 'higher'), ('higher', '-'), ('-', 'order'), ('order', 'logic'), ('logic', 'is'), ('is', 'presented'), ('presented', '.'), ('_UNK_', 'in'), ('in', 'memoriam'), ('memoriam', 'maurice'), ('maurice', 'gross'), ('gross', ';'), (';', 'maurice'), ('gross', '('), ('(', '1934'), ('1934', '-'), ('-', '2001'), ('2001', ')'), (')', 'was'), ('was', 'both'), ('both', 'a'), ('a', 'great'), ('great', 'linguist'), ('linguist', 'and'), ('and', 'a'), ('a', 'pioneer'), ('pioneer', 'in'), ('in', 'natural'), ('processing', '.'), ('.', 'this'), ('is', 'written'), ('written', 'in'), ('in', 'homage'), ('homage', 'to'), ('to', 'his'), ('his', 'memory'), ('_UNK_', 'introduction'), ('the', '26th'), ('26th', 'international'), ('on', 'logic'), ('logic', 'programming'), ('programming', 'special'), ('issue', ';'), ('is', 'the'), ('the', 'preface'), ('preface', 'to'), ('_UNK_', 'beyond'), ('beyond', 'description'), ('description', '.'), ('.', 'comment'), ('\"', 'approaching'), ('approaching', 'human'), ('human', 'language'), ('language', 'with'), ('with', 'complex'), ('networks', '\"'), ('by', 'cong'), ('cong', '&'), ('&', 'liu'), ('liu', ';'), (';', 'comment'), ('_UNK_', 'norm'), ('norm', '-'), ('-', 'based'), ('based', 'capacity'), ('capacity', 'control'), ('control', 'in'), ('in', 'neural'), ('we', 'investigate'), ('investigate', 'the'), ('the', 'capacity'), ('capacity', ','), (',', 'convexity'), ('convexity', 'and'), ('and', 'characterization'), ('characterization', 'of'), ('a', 'general'), ('general', 'family'), ('family', 'of'), ('of', 'norm'), ('-', 'constrained'), ('constrained', 'feed'), ('feed', '-'), ('-', 'forward'), ('forward', 'networks'), ('networks', '.'), ('_UNK_', 'about'), ('about', 'compression'), ('compression', 'of'), ('of', 'vocabulary'), ('vocabulary', 'in'), ('in', 'computer'), ('computer', 'oriented'), ('oriented', 'languages'), ('languages', ';'), ('author', 'uses'), ('uses', 'the'), ('the', 'entropy'), ('entropy', 'of'), ('the', 'ideal'), ('ideal', 'bose'), ('bose', '-'), ('-', 'einstein'), ('einstein', 'gas'), ('gas', 'to'), ('to', 'minimize'), ('minimize', 'losses'), ('losses', 'in'), ('computer', '-'), ('-', 'oriented'), ('languages', '.'), ('_UNK_', 'unary'), ('unary', 'coding'), ('coding', 'for'), ('for', 'neural'), ('network', 'learning'), ('learning', ';'), ('presents', 'some'), ('some', 'properties'), ('properties', 'of'), ('of', 'unary'), ('coding', 'of'), ('of', 'significance'), ('significance', 'for'), ('for', 'biological'), ('biological', 'learning'), ('learning', 'and'), ('and', 'instantaneously'), ('instantaneously', 'trained'), ('trained', 'neural'), ('_UNK_', 'some'), ('the', 'ukrainian'), ('ukrainian', 'writing'), ('writing', 'system'), ('the', 'grapheme'), ('grapheme', '-'), ('-', 'phoneme'), ('phoneme', 'relation'), ('relation', 'in'), ('in', 'ukrainian'), ('ukrainian', 'and'), ('and', 'some'), ('ukrainian', 'version'), ('the', 'cyrillic'), ('cyrillic', 'alphabet'), ('alphabet', '.'), ('_UNK_', 'convex'), ('convex', 'multiview'), ('multiview', 'fisher'), ('fisher', 'discriminant'), ('discriminant', 'analysis'), ('analysis', ';'), (';', 'section'), ('section', '1'), ('.', '3'), ('3', 'was'), ('was', 'incorrect'), ('incorrect', ','), (',', 'and'), ('and', '2'), ('2', '.'), ('1', 'will'), ('be', 'removed'), ('removed', 'from'), ('from', 'further'), ('further', 'submissions'), ('submissions', '.'), ('.', 'a'), ('a', 'rewritten'), ('rewritten', 'version'), ('be', 'posted'), ('posted', 'in'), ('the', 'future'), ('future', '.'), ('_UNK_', 'why'), ('why', 'bother'), ('bother', 'with'), ('with', 'syntax'), ('syntax', '?'), ('this', 'short'), ('short', 'note'), ('note', 'discusses'), ('discusses', 'the'), ('the', 'role'), ('role', 'of'), ('of', 'syntax'), ('syntax', 'vs'), ('vs', '.'), ('.', 'semantics'), ('semantics', 'and'), ('and', 'the'), ('the', 'interplay'), ('interplay', 'between'), ('between', 'logic'), ('logic', ','), (',', 'philosophy'), ('philosophy', ','), ('and', 'language'), ('language', 'in'), ('science', 'and'), ('and', 'game'), ('game', 'theory'), ('theory', '.'), ('why', \"'\"), (\"'\", 'gsa'), ('gsa', ':'), ('a', 'gravitational'), ('gravitational', 'search'), ('search', 'algorithm'), ('algorithm', \"'\"), (\"'\", 'is'), ('is', 'not'), ('not', 'genuinely'), ('genuinely', 'based'), ('on', 'the'), ('the', 'law'), ('law', 'of'), ('of', 'gravity'), ('gravity', ';'), (';', 'why'), ('_UNK_', 'neurocontrol'), ('neurocontrol', 'methods'), ('methods', 'review'), ('review', ';'), (';', 'methods'), ('methods', 'of'), ('of', 'applying'), ('applying', 'neural'), ('networks', 'to'), ('to', 'control'), ('control', 'plants'), ('plants', 'are'), ('are', 'considered'), ('considered', '.'), ('.', 'methods'), ('methods', 'and'), ('and', 'schemes'), ('schemes', 'are'), ('described', ','), (',', 'their'), ('their', 'advantages'), ('advantages', 'and'), ('and', 'disadvantages'), ('disadvantages', 'are'), ('are', 'discussed'), ('discussed', '.'), ('_UNK_', 'on'), ('the', 'universality'), ('universality', 'of'), ('of', 'online'), ('online', 'mirror'), ('mirror', 'descent'), ('descent', ';'), ('we', 'show'), ('show', 'that'), ('that', 'for'), ('for', 'a'), ('general', 'class'), ('class', 'of'), ('of', 'convex'), ('convex', 'online'), ('online', 'learning'), ('learning', 'problems'), ('problems', ','), (',', 'mirror'), ('descent', 'can'), ('can', 'always'), ('always', 'achieve'), ('achieve', 'a'), ('a', '('), ('(', 'nearly'), ('nearly', ')'), (')', 'optimal'), ('optimal', 'regret'), ('regret', 'guarantee'), ('guarantee', '.'), ('the', 'possibility'), ('possibility', 'of'), ('of', 'making'), ('making', 'the'), ('the', 'complete'), ('complete', 'computer'), ('computer', 'model'), ('a', 'human'), ('human', 'brain'), ('brain', ';'), ('the', 'development'), ('development', 'of'), ('the', 'algorithm'), ('algorithm', 'of'), ('a', 'neural'), ('network', 'building'), ('building', 'by'), ('the', 'corresponding'), ('corresponding', 'parts'), ('parts', 'of'), ('a', 'dna'), ('dna', 'code'), ('code', 'is'), ('is', 'discussed'), ('_UNK_', 'philosophy'), ('philosophy', 'in'), ('the', 'face'), ('face', 'of'), ('of', 'artificial'), ('intelligence', ';'), ('article', ','), (',', 'i'), ('i', 'discuss'), ('discuss', 'how'), ('how', 'the'), ('the', 'ai'), ('ai', 'community'), ('community', 'views'), ('views', 'concerns'), ('concerns', 'about'), ('about', 'the'), ('the', 'emergence'), ('emergence', 'of'), ('of', 'superintelligent'), ('superintelligent', 'ai'), ('ai', 'and'), ('and', 'related'), ('related', 'philosophical'), ('philosophical', 'issues'), ('issues', '.'), ('a', 'survey'), ('survey', 'on'), ('on', 'contextual'), ('contextual', 'multi'), ('multi', '-'), ('-', 'armed'), ('armed', 'bandits'), ('bandits', ';'), ('this', 'survey'), ('survey', 'we'), ('we', 'cover'), ('cover', 'a'), ('a', 'few'), ('few', 'stochastic'), ('stochastic', 'and'), ('and', 'adversarial'), ('adversarial', 'contextual'), ('contextual', 'bandit'), ('bandit', 'algorithms'), ('algorithms', '.'), ('.', 'we'), ('we', 'analyze'), ('analyze', 'each'), ('each', 'algorithm'), ('s', 'assumption'), ('assumption', 'and'), ('and', 'regret'), ('regret', 'bound'), ('bound', '.'), ('_UNK_', 'parallels'), ('parallels', 'of'), ('of', 'human'), ('the', 'behavior'), ('behavior', 'of'), ('of', 'bottlenose'), ('bottlenose', 'dolphins'), ('dolphins', ';'), ('a', 'short'), ('short', 'review'), ('review', 'of'), ('of', 'similarities'), ('similarities', 'between'), ('between', 'dolphins'), ('dolphins', 'and'), ('and', 'humans'), ('humans', 'with'), ('with', 'the'), ('the', 'help'), ('help', 'of'), ('of', 'quantitative'), ('quantitative', 'linguistics'), ('linguistics', 'and'), ('and', 'information'), ('information', 'theory'), ('a', 'history'), ('history', 'of'), ('of', 'metaheuristics'), ('metaheuristics', ';'), ('this', 'chapter'), ('chapter', 'describes'), ('describes', 'the'), ('the', 'history'), ('metaheuristics', 'in'), ('in', 'five'), ('five', 'distinct'), ('distinct', 'periods'), ('periods', ','), (',', 'starting'), ('starting', 'long'), ('long', 'before'), ('before', 'the'), ('the', 'first'), ('first', 'use'), ('the', 'term'), ('term', 'and'), ('and', 'ending'), ('ending', 'a'), ('a', 'long'), ('long', 'time'), ('time', 'in'), ('_UNK_', 'an'), ('an', 'energy'), ('energy', 'efficient'), ('efficient', 'scheme'), ('scheme', 'for'), ('for', 'data'), ('data', 'gathering'), ('gathering', 'in'), ('in', 'wireless'), ('wireless', 'sensor'), ('sensor', 'networks'), ('networks', 'using'), ('using', 'particle'), ('particle', 'swarm'), ('swarm', 'optimization'), ('optimization', ';'), ('crucial', 'sign'), ('sign', 'error'), ('in', 'equation'), ('equation', '1'), ('new', 'bengali'), ('bengali', 'readability'), ('readability', 'score'), ('score', ';'), ('paper', 'we'), ('we', 'have'), ('have', 'proposed'), ('proposed', 'methods'), ('methods', 'to'), ('to', 'analyze'), ('analyze', 'the'), ('the', 'readability'), ('readability', 'of'), ('of', 'bengali'), ('bengali', 'language'), ('language', 'texts'), ('texts', '.'), ('have', 'got'), ('got', 'some'), ('some', 'exceptionally'), ('exceptionally', 'good'), ('good', 'results'), ('results', 'out'), ('out', 'of'), ('the', 'experiments'), ('experiments', '.'), ('the', 'logic'), ('programming', 'paradigm'), ('paradigm', 'and'), ('and', 'prolog'), ('prolog', ';'), ('a', 'tutorial'), ('tutorial', 'on'), ('programming', 'and'), ('prolog', 'appropriate'), ('appropriate', 'for'), ('a', 'course'), ('course', 'on'), ('on', 'programming'), ('programming', 'languages'), ('languages', 'for'), ('for', 'students'), ('students', 'familiar'), ('familiar', 'with'), ('with', 'imperative'), ('imperative', 'programming'), ('programming', '.'), ('_UNK_', 'calculate'), ('calculate', 'distance'), ('distance', 'to'), ('to', 'object'), ('object', 'in'), ('the', 'area'), ('area', 'where'), ('where', 'car'), ('car', ','), (',', 'using'), ('using', 'video'), ('video', 'analysis'), ('the', 'method'), ('of', 'using'), ('video', 'cameras'), ('cameras', 'installed'), ('installed', 'on'), ('the', 'car'), (',', 'to'), ('to', 'calculate'), ('the', 'distance'), ('the', 'object'), ('in', 'its'), ('its', 'area'), ('area', 'of'), ('of', 'movement'), ('movement', '.'), ('_UNK_', 'group'), ('group', 'theory'), ('theory', ','), (',', 'group'), ('group', 'actions'), ('actions', ','), (',', 'evolutionary'), ('evolutionary', 'algorithms'), ('algorithms', ','), ('and', 'global'), ('global', 'optimization'), ('use', 'group'), ('group', ','), (',', 'action'), ('action', 'and'), ('and', 'orbit'), ('orbit', 'to'), ('to', 'understand'), ('understand', 'how'), ('how', 'evolutionary'), ('evolutionary', 'solve'), ('solve', 'nonconvex'), ('nonconvex', 'optimization'), ('optimization', 'problems'), ('problems', '.'), ('_UNK_', 'entropy'), ('of', 'telugu'), ('telugu', ';'), ('presents', 'an'), ('an', 'investigation'), ('investigation', 'of'), ('the', 'telugu'), ('telugu', 'script'), ('script', '.'), ('.', 'since'), ('since', 'this'), ('this', 'script'), ('script', 'is'), ('is', 'syllabic'), ('syllabic', ','), ('and', 'not'), ('not', 'alphabetic'), ('alphabetic', ','), (',', 'the'), ('the', 'computation'), ('computation', 'of'), ('of', 'entropy'), ('entropy', 'is'), ('is', 'somewhat'), ('somewhat', 'complicated'), ('complicated', '.'), ('_UNK_', 'word'), ('word', 'segmentation'), ('segmentation', 'on'), ('on', 'micro'), ('micro', '-'), ('-', 'blog'), ('blog', 'texts'), ('texts', 'with'), ('with', 'external'), ('external', 'lexicon'), ('lexicon', 'and'), ('and', 'heterogeneous'), ('heterogeneous', 'data'), ('data', ';'), ('paper', 'describes'), ('describes', 'our'), ('our', 'system'), ('system', 'designed'), ('designed', 'for'), ('the', 'nlpcc'), ('nlpcc', '2016'), ('2016', 'shared'), ('shared', 'task'), ('task', 'on'), ('on', 'word'), ('_UNK_', 'guarded'), ('guarded', 'resolution'), ('resolution', 'for'), ('for', 'answer'), ('answer', 'set'), ('set', 'programming'), ('programming', ';'), ('we', 'describe'), ('describe', 'a'), ('a', 'variant'), ('variant', 'of'), ('of', 'resolution'), ('resolution', 'rule'), ('rule', 'of'), ('of', 'proof'), ('proof', 'and'), ('and', 'show'), ('that', 'it'), ('is', 'complete'), ('complete', 'for'), ('for', 'stable'), ('stable', 'semantics'), ('semantics', 'of'), ('of', 'logic'), ('logic', 'programs'), ('programs', '.'), ('show', 'applications'), ('applications', 'of'), ('of', 'this'), ('this', 'result'), ('result', '.'), ('_UNK_', 'cornell'), ('cornell', 'spf'), ('spf', ':'), (':', 'cornell'), ('cornell', 'semantic'), ('semantic', 'parsing'), ('parsing', 'framework'), ('framework', ';'), ('the', 'cornell'), ('framework', '('), ('(', 'spf'), ('spf', ')'), (')', 'is'), ('a', 'learning'), ('and', 'inference'), ('inference', 'framework'), ('for', 'mapping'), ('mapping', 'natural'), ('language', 'to'), ('to', 'formal'), ('formal', 'representation'), ('of', 'its'), ('its', 'meaning'), ('meaning', '.'), ('_UNK_', 'semistability'), ('semistability', '-'), ('based', 'convergence'), ('convergence', 'analysis'), ('for', 'paracontracting'), ('paracontracting', 'multiagent'), ('multiagent', 'coordination'), ('coordination', 'optimization'), ('this', 'sequential'), ('sequential', 'technical'), ('technical', 'report'), ('report', 'extends'), ('extends', 'some'), ('some', 'of'), ('the', 'previous'), ('previous', 'results'), ('results', 'we'), ('we', 'posted'), ('posted', 'at'), ('at', 'arxiv'), (':', '1306'), ('1306', '.'), ('.', '0225'), ('0225', '.'), ('_UNK_', 'proceedings'), ('proceedings', 'of'), ('first', 'international'), ('international', 'workshop'), ('workshop', 'on'), ('on', 'deep'), ('deep', 'learning'), ('and', 'music'), ('music', ';'), (';', 'proceedings'), ('music', ','), (',', 'joint'), ('joint', 'with'), ('with', 'ijcnn'), ('ijcnn', ','), (',', 'anchorage'), ('anchorage', ','), (',', 'us'), ('us', ','), ('may', '17'), ('17', '-'), ('-', '18'), ('18', ','), (',', '2017'), ('_UNK_', 'attack'), ('attack', 'rmse'), ('rmse', 'leaderboard'), ('leaderboard', ':'), (':', 'an'), ('an', 'introduction'), ('introduction', 'and'), ('and', 'case'), ('case', 'study'), ('study', ';'), ('this', 'manuscript'), ('manuscript', ','), ('we', 'briefly'), ('briefly', 'introduce'), ('introduce', 'several'), ('several', 'tricks'), ('tricks', 'to'), ('to', 'climb'), ('climb', 'the'), ('the', 'leaderboards'), ('leaderboards', 'which'), ('which', 'use'), ('use', 'rmse'), ('rmse', 'for'), ('for', 'evaluation'), ('evaluation', 'without'), ('without', 'exploiting'), ('exploiting', 'any'), ('any', 'training'), ('training', 'data'), ('data', '.'), ('_UNK_', 'standardization'), ('standardization', 'of'), ('the', 'formal'), ('of', 'lexical'), ('lexical', 'information'), ('information', 'for'), ('for', 'nlp'), ('nlp', ';'), ('survey', 'of'), ('of', 'dictionary'), ('dictionary', 'models'), ('models', 'and'), ('and', 'formats'), ('formats', 'is'), ('presented', 'as'), ('as', 'well'), ('well', 'as'), ('a', 'presentation'), ('presentation', 'of'), ('of', 'corresponding'), ('corresponding', 'recent'), ('recent', 'standardisation'), ('standardisation', 'activities'), ('activities', '.'), ('machine', 'learning'), ('learning', 'model'), ('model', 'for'), ('for', 'stock'), ('stock', 'market'), ('market', 'prediction'), ('prediction', ';'), (';', 'stock'), ('prediction', 'is'), ('the', 'act'), ('act', 'of'), ('of', 'trying'), ('trying', 'to'), ('to', 'determine'), ('determine', 'the'), ('future', 'value'), ('value', 'of'), ('a', 'company'), ('company', 'stock'), ('stock', 'or'), ('or', 'other'), ('other', 'financial'), ('financial', 'instrument'), ('instrument', 'traded'), ('traded', 'on'), ('on', 'a'), ('a', 'financial'), ('financial', 'exchange'), ('exchange', '.'), ('_UNK_', 'defensive'), ('defensive', 'distillation'), ('distillation', 'is'), ('not', 'robust'), ('robust', 'to'), ('to', 'adversarial'), ('adversarial', 'examples'), ('examples', ';'), ('that', 'defensive'), ('not', 'secure'), ('secure', ':'), (':', 'it'), ('no', 'more'), ('more', 'resistant'), ('resistant', 'to'), ('to', 'targeted'), ('targeted', 'misclassification'), ('misclassification', 'attacks'), ('attacks', 'than'), ('than', 'unprotected'), ('unprotected', 'neural'), ('of', 'nips'), ('nips', '2017'), ('2017', 'symposium'), ('symposium', 'on'), ('on', 'interpretable'), ('interpretable', 'machine'), ('the', 'proceedings'), ('learning', ','), (',', 'held'), ('held', 'in'), ('in', 'long'), ('long', 'beach'), ('beach', ','), (',', 'california'), ('california', ','), (',', 'usa'), ('usa', 'on'), ('on', 'december'), ('december', '7'), ('7', ','), ('_UNK_', 'piecewise'), ('piecewise', 'linear'), ('linear', 'activation'), ('activation', 'functions'), ('functions', 'for'), ('for', 'more'), ('more', 'efficient'), ('efficient', 'deep'), ('deep', 'networks'), ('submission', 'has'), ('by', 'arxiv'), ('arxiv', 'administrators'), ('administrators', 'because'), ('is', 'intentionally'), ('intentionally', 'incomplete'), ('incomplete', ','), (',', 'which'), ('which', 'is'), ('is', 'in'), ('in', 'violation'), ('violation', 'of'), ('of', 'our'), ('our', 'policies'), ('policies', '.'), ('the', 'triangle'), ('triangle', 'inequality'), ('inequality', 'for'), ('the', 'jaccard'), ('jaccard', 'distance'), ('distance', ';'), (';', 'two'), ('two', 'simple'), ('simple', 'proofs'), ('proofs', 'of'), ('distance', 'in'), ('in', 'terms'), ('terms', 'of'), ('of', 'nonnegative'), ('nonnegative', ','), (',', 'monotone'), ('monotone', ','), (',', 'submodular'), ('submodular', 'functions'), ('functions', 'are'), ('are', 'given'), ('given', 'and'), ('and', 'discussed'), ('_UNK_', 'how'), ('how', 'to'), ('to', 'realize'), ('realize', '\"'), ('a', 'sense'), ('sense', 'of'), ('of', 'humour'), ('humour', '\"'), ('\"', 'in'), ('in', 'computers'), ('computers', '?'), (';', 'computer'), ('a', '\"'), ('\"', 'sense'), ('\"', 'suggested'), ('suggested', 'previously'), ('previously', '['), ('[', 'arxiv'), (':', '0711'), ('0711', '.'), ('.', '2058'), ('2058', ','), (',', '0711'), ('.', '2061'), ('2061', ','), ('.', '2270'), ('2270', ']'), (']', 'is'), ('is', 'raised'), ('raised', 'to'), ('the', 'level'), ('level', 'of'), ('a', 'realistic'), ('realistic', 'algorithm'), ('algorithm', '.'), ('linear', 'multilayer'), ('multilayer', 'perceptrons'), ('perceptrons', 'and'), ('and', 'dropout'), ('dropout', ';'), ('new', 'type'), ('type', 'of'), ('of', 'hidden'), ('hidden', 'layer'), ('layer', 'for'), ('a', 'multilayer'), ('multilayer', 'perceptron'), ('perceptron', ','), ('and', 'demonstrate'), ('demonstrate', 'that'), ('it', 'obtains'), ('obtains', 'the'), ('the', 'best'), ('best', 'reported'), ('reported', 'performance'), ('performance', 'for'), ('for', 'an'), ('an', 'mlp'), ('mlp', 'on'), ('the', 'mnist'), ('mnist', 'dataset'), ('dataset', '.'), ('_UNK_', 'technical'), ('report', ':'), ('a', 'tool'), ('tool', 'for'), ('for', 'measuring'), ('measuring', 'prosodic'), ('prosodic', 'accommodation'), ('accommodation', ';'), ('article', 'has'), ('because', 'the'), ('the', 'submitter'), ('submitter', 'did'), ('did', 'not'), ('not', 'have'), ('have', 'the'), ('the', 'legal'), ('legal', 'authority'), ('authority', 'to'), ('to', 'grant'), ('grant', 'the'), ('the', 'license'), ('license', 'applied'), ('applied', 'to'), ('the', 'work'), ('work', '.'), ('a', 'remark'), ('remark', 'on'), ('on', 'higher'), ('higher', 'order'), ('order', 'rue'), ('rue', '-'), ('-', 'resolution'), ('resolution', 'with'), ('with', 'extrue'), ('extrue', ';'), ('that', 'a'), ('a', 'prominent'), ('prominent', 'counterexample'), ('counterexample', 'for'), ('the', 'completeness'), ('completeness', 'of'), ('of', 'first'), ('first', 'order'), ('resolution', 'does'), ('does', 'not'), ('not', 'apply'), ('apply', 'to'), ('the', 'higher'), ('resolution', 'approach'), ('approach', 'extrue'), ('extrue', '.'), ('_UNK_', 'sat'), ('sat', 'as'), ('a', 'game'), ('game', ';'), ('a', 'funny'), ('funny', 'representation'), ('of', 'sat'), ('sat', '.'), ('.', 'while'), ('while', 'the'), ('the', 'primary'), ('primary', 'interest'), ('interest', 'is'), ('is', 'to'), ('to', 'present'), ('present', 'propositional'), ('propositional', 'satisfiability'), ('satisfiability', 'in'), ('a', 'playful'), ('playful', 'way'), ('way', 'for'), ('for', 'pedagogical'), ('pedagogical', 'purposes'), ('purposes', ','), (',', 'it'), ('it', 'could'), ('could', 'also'), ('also', 'inspire'), ('inspire', 'new'), ('new', 'search'), ('search', 'heuristics'), ('heuristics', '.'), ('on', 'adjusting'), ('adjusting', '$'), ('$', 'r'), ('r', '^'), ('^', '2'), ('2', '$'), ('$', 'for'), ('for', 'using'), ('using', 'with'), ('with', 'cross'), ('cross', '-'), ('-', 'validation'), ('validation', ';'), ('show', 'how'), ('to', 'adjust'), ('adjust', 'the'), ('the', 'coefficient'), ('coefficient', 'of'), ('of', 'determination'), ('determination', '($'), ('($', 'r'), ('2', '$)'), ('$)', 'when'), ('when', 'used'), ('used', 'for'), ('measuring', 'predictive'), ('predictive', 'accuracy'), ('accuracy', 'via'), ('via', 'leave'), ('leave', '-'), ('-', 'one'), ('-', 'out'), ('out', 'cross'), ('validation', '.'), ('_UNK_', 'approximated'), ('approximated', 'structured'), ('structured', 'prediction'), ('prediction', 'for'), ('for', 'learning'), ('learning', 'large'), ('scale', 'graphical'), ('graphical', 'models'), ('models', ';'), ('this', 'manuscripts'), ('manuscripts', 'contains'), ('the', 'proofs'), ('proofs', 'for'), ('for', '\"'), ('a', 'primal'), ('primal', '-'), ('dual', 'message'), ('message', '-'), ('-', 'passing'), ('passing', 'algorithm'), ('algorithm', 'for'), ('for', 'approximated'), ('approximated', 'large'), ('scale', 'structured'), ('prediction', '\".'), ('automatic', 'liver'), ('liver', 'segmentation'), ('segmentation', 'method'), ('method', 'in'), ('in', 'ct'), ('ct', 'images'), ('the', 'aim'), ('aim', 'of'), ('work', 'is'), ('develop', 'a'), ('method', 'for'), ('for', 'automatic'), ('automatic', 'segmentation'), ('segmentation', 'of'), ('the', 'liver'), ('liver', 'based'), ('a', 'priori'), ('priori', 'knowledge'), ('knowledge', 'of'), ('the', 'image'), ('image', ','), (',', 'such'), ('such', 'as'), ('as', 'location'), ('location', 'and'), ('and', 'shape'), ('shape', 'of'), ('liver', '.'), ('the', 'existence'), ('existence', 'of'), ('a', 'projective'), ('projective', 'reconstruction'), ('we', 'study'), ('the', 'connection'), ('connection', 'between'), ('between', 'the'), ('reconstruction', 'and'), ('a', 'fundamental'), ('fundamental', 'matrix'), ('matrix', 'satisfying'), ('satisfying', 'the'), ('the', 'epipolar'), ('epipolar', 'constraints'), ('constraints', '.'), ('_UNK_', 'solving'), ('solving', 'traveling'), ('traveling', 'salesman'), ('salesman', 'problem'), ('problem', 'by'), ('by', 'marker'), ('marker', 'method'), ('method', ';'), ('use', 'marker'), ('method', 'and'), ('and', 'propose'), ('new', 'mutation'), ('mutation', 'operator'), ('operator', 'that'), ('that', 'selects'), ('selects', 'the'), ('the', 'nearest'), ('nearest', 'neighbor'), ('neighbor', 'among'), ('among', 'all'), ('all', 'near'), ('near', 'neighbors'), ('neighbors', 'solving'), ('problem', '.'), ('a', 'primer'), ('primer', 'on'), ('on', 'answer'), ('a', 'introduction'), ('the', 'syntax'), ('syntax', 'and'), ('and', 'semantics'), ('of', 'answer'), ('programming', 'intended'), ('intended', 'as'), ('as', 'an'), ('an', 'handout'), ('handout', 'to'), ('to', '['), ('[', 'under'), ('under', ']'), (']', 'graduate'), ('graduate', 'students'), ('students', 'taking'), ('taking', 'artificial'), ('artificial', 'intlligence'), ('intlligence', 'or'), ('or', 'logic'), ('programming', 'classes'), ('classes', '.'), ('_UNK_', 'agent'), ('agent', 'models'), ('models', 'of'), ('of', 'political'), ('political', 'interactions'), ('interactions', ';'), (';', 'looks'), ('looks', 'at'), ('at', 'state'), ('state', 'interactions'), ('interactions', 'from'), ('from', 'an'), ('an', 'agent'), ('agent', 'based'), ('based', 'ai'), ('ai', 'perspective'), ('perspective', 'to'), ('to', 'see'), ('see', 'state'), ('interactions', 'as'), ('an', 'example'), ('example', 'of'), ('of', 'emergent'), ('emergent', 'intelligent'), ('intelligent', 'behavior'), ('behavior', '.'), ('.', 'exposes'), ('exposes', 'basic'), ('basic', 'principles'), ('principles', 'of'), ('of', 'game'), ('learning', 'states'), ('states', 'representations'), ('representations', 'in'), ('in', 'pomdp'), ('pomdp', ';'), ('propose', 'to'), ('to', 'deal'), ('deal', 'with'), ('with', 'sequential'), ('sequential', 'processes'), ('processes', 'where'), ('where', 'only'), ('only', 'partial'), ('partial', 'observations'), ('observations', 'are'), ('are', 'available'), ('available', 'by'), ('by', 'learning'), ('learning', 'a'), ('a', 'latent'), ('latent', 'representation'), ('representation', 'space'), ('space', 'on'), ('on', 'which'), ('which', 'policies'), ('policies', 'may'), ('may', 'be'), ('be', 'accurately'), ('accurately', 'learned'), ('learned', '.')])\n",
            "dict_keys([('_UNK_', '_UNK_'), ('_UNK_', 'differential'), ('differential', 'contrastive'), ('contrastive', 'divergence'), ('divergence', ';'), (';', 'this'), ('this', 'paper'), ('paper', 'has'), ('has', 'been'), ('been', 'retracted'), ('retracted', '.'), ('_UNK_', 'what'), ('what', 'does'), ('does', 'artificial'), ('artificial', 'life'), ('life', 'tell'), ('tell', 'us'), ('us', 'about'), ('about', 'death'), ('death', '?'), ('?', ';'), (';', 'short'), ('short', 'philosophical'), ('philosophical', 'essay'), ('_UNK_', 'p'), ('p', '='), ('=', 'np'), ('np', ';'), (';', 'we'), ('we', 'claim'), ('claim', 'to'), ('to', 'resolve'), ('resolve', 'the'), ('the', 'p'), ('p', '=?'), ('=?', 'np'), ('np', 'problem'), ('problem', 'via'), ('via', 'a'), ('a', 'formal'), ('formal', 'argument'), ('argument', 'for'), ('for', 'p'), ('np', '.'), ('_UNK_', 'computational'), ('computational', 'geometry'), ('geometry', 'column'), ('column', '38'), ('38', ';'), (';', 'recent'), ('recent', 'results'), ('results', 'on'), ('on', 'curve'), ('curve', 'reconstruction'), ('reconstruction', 'are'), ('are', 'described'), ('described', '.'), ('_UNK_', 'weak'), ('weak', 'evolvability'), ('evolvability', 'equals'), ('equals', 'strong'), ('strong', 'evolvability'), ('evolvability', ';'), (';', 'an'), ('an', 'updated'), ('updated', 'version'), ('version', 'will'), ('will', 'be'), ('be', 'uploaded'), ('uploaded', 'later'), ('later', '.'), ('_UNK_', 'creating'), ('creating', 'a'), ('a', 'new'), ('new', 'ontology'), ('ontology', ':'), (':', 'a'), ('a', 'modular'), ('modular', 'approach'), ('approach', ';'), (';', 'creating'), ('_UNK_', 'defeasible'), ('defeasible', 'reasoning'), ('reasoning', 'in'), ('in', 'oscar'), ('oscar', ';'), ('this', 'is'), ('is', 'a'), ('a', 'system'), ('system', 'description'), ('description', 'for'), ('for', 'the'), ('the', 'oscar'), ('oscar', 'defeasible'), ('defeasible', 'reasoner'), ('reasoner', '.'), ('_UNK_', 'essence'), ('essence', \"'\"), (\"'\", 'description'), ('description', ';'), (';', 'a'), ('a', 'description'), ('description', 'of'), ('of', 'the'), ('the', 'essence'), (\"'\", 'language'), ('language', 'as'), ('as', 'used'), ('used', 'by'), ('by', 'the'), ('the', 'tool'), ('tool', 'savile'), ('savile', 'row'), ('row', '.'), ('_UNK_', 'deep'), ('deep', 'neural'), ('neural', 'networks'), ('networks', '-'), ('-', 'a'), ('a', 'brief'), ('brief', 'history'), ('history', ';'), (';', 'introduction'), ('introduction', 'to'), ('to', 'deep'), ('networks', 'and'), ('and', 'their'), ('their', 'history'), ('history', '.'), ('_UNK_', 'statistical'), ('statistical', 'physics'), ('physics', 'for'), ('for', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', ';'), ('been', 'withdrawn'), ('withdrawn', 'by'), ('the', 'author'), ('author', '.'), ('_UNK_', 'complex'), ('complex', 'networks'), ('networks', ';'), ('to', 'the'), ('the', 'special'), ('special', 'issue'), ('issue', 'on'), ('on', 'complex'), ('networks', ','), (',', 'artificial'), ('life', 'journal'), ('journal', '.'), ('_UNK_', 'serious'), ('serious', 'flaws'), ('flaws', 'in'), ('in', 'korf'), ('korf', 'et'), ('et', 'al'), ('al', \".'\"), (\".'\", 's'), ('s', 'analysis'), ('analysis', 'on'), ('on', 'time'), ('time', 'complexity'), ('complexity', 'of'), ('of', 'a'), ('a', '*'), ('*', ';'), ('withdrawn', '.'), ('_UNK_', 'preprocessing'), ('preprocessing', ':'), ('a', 'step'), ('step', 'in'), ('in', 'automating'), ('automating', 'early'), ('early', 'detection'), ('detection', 'of'), ('of', 'cervical'), ('cervical', 'cancer'), ('cancer', ';'), ('_UNK_', 'liquid'), ('liquid', 'state'), ('state', 'machines'), ('machines', 'in'), ('in', 'adbiatic'), ('adbiatic', 'quantum'), ('quantum', 'computers'), ('computers', 'for'), ('for', 'general'), ('general', 'computation'), ('computation', ';'), (';', 'major'), ('major', 'mistakes'), ('mistakes', 'do'), ('do', 'not'), ('not', 'read'), ('_UNK_', 'mining'), ('mining', 'for'), ('for', 'trees'), ('trees', 'in'), ('in', 'a'), ('a', 'graph'), ('graph', 'is'), ('is', 'np'), ('np', '-'), ('-', 'complete'), ('complete', ';'), (';', 'mining'), ('is', 'shown'), ('shown', 'to'), ('to', 'be'), ('be', 'np'), ('complete', '.'), ('_UNK_', 'towards'), ('towards', 'a'), ('a', 'hierarchical'), ('hierarchical', 'model'), ('model', 'of'), ('of', 'consciousness'), ('consciousness', ','), (',', 'intelligence'), ('intelligence', ','), (',', 'mind'), ('mind', 'and'), ('and', 'body'), ('body', ';'), ('this', 'article'), ('article', 'is'), ('is', 'taken'), ('taken', 'out'), ('out', '.'), ('_UNK_', 'a'), ('a', 'notation'), ('notation', 'for'), ('for', 'markov'), ('markov', 'decision'), ('decision', 'processes'), ('processes', ';'), ('paper', 'specifies'), ('specifies', 'a'), ('processes', '.'), ('_UNK_', 'icon'), ('icon', 'challenge'), ('challenge', 'on'), ('on', 'algorithm'), ('algorithm', 'selection'), ('selection', ';'), ('we', 'present'), ('present', 'the'), ('the', 'results'), ('results', 'of'), ('the', 'icon'), ('selection', '.'), ('_UNK_', 'recognition'), ('recognition', 'of'), ('of', 'regular'), ('regular', 'shapes'), ('shapes', 'in'), ('in', 'satelite'), ('satelite', 'images'), ('images', ';'), ('author', 'ali'), ('ali', 'pourmohammad'), ('pourmohammad', '.'), ('_UNK_', 'glottochronologic'), ('glottochronologic', 'retrognostic'), ('retrognostic', 'of'), ('of', 'language'), ('language', 'system'), ('system', ';'), ('a', 'glottochronologic'), ('system', 'is'), ('is', 'proposed'), ('_UNK_', 'the'), ('the', 'model'), ('of', 'quantum'), ('quantum', 'evolution'), ('evolution', ';'), ('author', 'due'), ('due', 'to'), ('to', 'extremely'), ('extremely', 'unscientific'), ('unscientific', 'errors'), ('errors', '.'), ('_UNK_', 'utility'), ('utility', '-'), ('-', 'probability'), ('probability', 'duality'), ('duality', ';'), ('paper', 'presents'), ('presents', 'duality'), ('duality', 'between'), ('between', 'probability'), ('probability', 'distributions'), ('distributions', 'and'), ('and', 'utility'), ('utility', 'functions'), ('functions', '.'), ('_UNK_', 'temporized'), ('temporized', 'equilibria'), ('equilibria', ';'), ('to', 'a'), ('a', 'crucial'), ('crucial', 'error'), ('error', 'in'), ('in', 'the'), ('the', 'submission'), ('submission', 'action'), ('action', '.'), ('_UNK_', 'backpropagation'), ('backpropagation', 'in'), ('in', 'matrix'), ('matrix', 'notation'), ('notation', ';'), (';', 'in'), ('in', 'this'), ('this', 'note'), ('note', 'we'), ('we', 'calculate'), ('calculate', 'the'), ('the', 'gradient'), ('gradient', 'of'), ('the', 'network'), ('network', 'function'), ('function', 'in'), ('notation', '.'), ('_UNK_', 'random'), ('random', 'dfas'), ('dfas', 'are'), ('are', 'efficiently'), ('efficiently', 'pac'), ('pac', 'learnable'), ('learnable', ';'), ('withdrawn', 'due'), ('to', 'an'), ('an', 'error'), ('error', 'found'), ('found', 'by'), ('by', 'dana'), ('dana', 'angluin'), ('angluin', 'and'), ('and', 'lev'), ('lev', 'reyzin'), ('reyzin', '.'), ('_UNK_', 'network'), ('network', 'motifs'), ('motifs', 'in'), ('in', 'music'), ('music', 'sequences'), ('sequences', ';'), ('author', 'because'), ('because', 'it'), ('it', 'needs'), ('needs', 'a'), ('a', 'deep'), ('deep', 'methodological'), ('methodological', 'revision'), ('revision', '.'), ('_UNK_', 'glottochronology'), ('glottochronology', 'and'), ('and', 'problems'), ('problems', 'of'), ('of', 'protolanguage'), ('protolanguage', 'reconstruction'), ('reconstruction', ';'), ('a', 'method'), ('method', 'of'), ('of', 'languages'), ('languages', 'genealogical'), ('genealogical', 'trees'), ('trees', 'construction'), ('construction', 'is'), ('proposed', '.'), ('_UNK_', 'using'), ('using', 'slp'), ('slp', 'neural'), ('neural', 'network'), ('network', 'to'), ('to', 'persian'), ('persian', 'handwritten'), ('handwritten', 'digits'), ('digits', 'recognition'), ('recognition', ';'), ('_UNK_', 'time'), ('time', 'hopping'), ('hopping', 'technique'), ('technique', 'for'), ('for', 'faster'), ('faster', 'reinforcement'), ('reinforcement', 'learning'), ('learning', 'in'), ('in', 'simulations'), ('simulations', ';'), ('this', 'preprint'), ('preprint', 'has'), ('author', 'for'), ('for', 'revision'), ('_UNK_', 'convolutional'), ('convolutional', 'matching'), ('matching', 'pursuit'), ('pursuit', 'and'), ('and', 'dictionary'), ('dictionary', 'training'), ('training', ';'), (';', 'matching'), ('and', 'k'), ('k', '-'), ('-', 'svd'), ('svd', 'is'), ('is', 'demonstrated'), ('demonstrated', 'in'), ('the', 'translation'), ('translation', 'invariant'), ('invariant', 'setting'), ('_UNK_', 'fitness'), ('fitness', 'landscape'), ('landscape', 'analysis'), ('analysis', 'for'), ('for', 'dynamic'), ('dynamic', 'resource'), ('resource', 'allocation'), ('allocation', 'in'), ('in', 'multiuser'), ('multiuser', 'ofdm'), ('ofdm', 'based'), ('based', 'cognitive'), ('cognitive', 'radio'), ('radio', 'systems'), ('systems', ';'), ('a', 'note'), ('note', 'on'), ('on', 'darwiche'), ('darwiche', 'and'), ('and', 'pearl'), ('pearl', ';'), (';', 'it'), ('it', 'is'), ('shown', 'that'), ('that', 'darwiche'), ('pearl', \"'\"), (\"'\", 's'), ('s', 'postulates'), ('postulates', 'imply'), ('imply', 'an'), ('an', 'interesting'), ('interesting', 'property'), ('property', ','), (',', 'not'), ('not', 'noticed'), ('noticed', 'by'), ('the', 'authors'), ('authors', '.'), ('_UNK_', 'flip'), ('flip', '-'), ('-', 'flop'), ('flop', 'sublinear'), ('sublinear', 'models'), ('models', 'for'), ('for', 'graphs'), ('graphs', ':'), (':', 'proof'), ('proof', 'of'), ('of', 'theorem'), ('theorem', '1'), ('1', ';'), ('we', 'prove'), ('prove', 'that'), ('that', 'there'), ('there', 'is'), ('is', 'no'), ('no', 'class'), ('class', '-'), ('-', 'dual'), ('dual', 'for'), ('for', 'almost'), ('almost', 'all'), ('all', 'sublinear'), ('models', 'on'), ('on', 'graphs'), ('graphs', '.'), ('_UNK_', 'autonomous'), ('autonomous', 'perceptron'), ('perceptron', 'neural'), ('network', 'inspired'), ('inspired', 'from'), ('from', 'quantum'), ('quantum', 'computing'), ('computing', ';'), ('this', 'abstract'), ('abstract', 'will'), ('be', 'modified'), ('modified', 'after'), ('after', 'correcting'), ('correcting', 'the'), ('the', 'minor'), ('minor', 'error'), ('in', 'eq'), ('eq', '.('), ('.(', '2'), ('2', ')'), ('using', 'sets'), ('sets', 'of'), ('of', 'probability'), ('probability', 'measures'), ('measures', 'to'), ('to', 'represent'), ('represent', 'uncertainty'), ('uncertainty', ';'), (';', 'i'), ('i', 'explore'), ('explore', 'the'), ('the', 'use'), ('use', 'of'), ('of', 'sets'), ('measures', 'as'), ('as', 'a'), ('a', 'representation'), ('representation', 'of'), ('of', 'uncertainty'), ('uncertainty', '.'), ('a', 'comment'), ('comment', 'on'), ('on', 'argumentation'), ('argumentation', ';'), ('we', 'use'), ('use', 'the'), ('the', 'theory'), ('theory', 'of'), ('of', 'defaults'), ('defaults', 'and'), ('their', 'meaning'), ('meaning', 'of'), ('of', '['), ('[', 'gs16'), ('gs16', ']'), (']', 'to'), ('to', 'develop'), ('develop', '('), ('(', 'the'), ('the', 'outline'), ('outline', 'of'), ('a', ')'), (')', 'new'), ('new', 'theory'), ('of', 'argumentation'), ('argumentation', '.'), ('_UNK_', 'activitynet'), ('activitynet', 'challenge'), ('challenge', '2017'), ('2017', 'summary'), ('summary', ';'), (';', 'the'), ('the', 'activitynet'), ('activitynet', 'large'), ('large', 'scale'), ('scale', 'activity'), ('activity', 'recognition'), ('recognition', 'challenge'), ('summary', ':'), (':', 'results'), ('results', 'and'), ('and', 'challenge'), ('challenge', 'participants'), ('participants', 'papers'), ('papers', '.'), ('the', 'yahoo'), ('yahoo', 'query'), ('query', 'treebank'), ('treebank', ','), (',', 'v'), ('v', '.'), ('.', '1'), ('1', '.'), ('.', '0'), ('0', ';'), ('description', 'and'), ('and', 'annotation'), ('annotation', 'guidelines'), ('guidelines', 'for'), ('yahoo', 'webscope'), ('webscope', 'release'), ('release', 'of'), ('of', 'query'), (',', 'version'), ('version', '1'), ('0', ','), (',', 'may'), ('may', '2016'), ('2016', '.'), ('_UNK_', 'decision'), ('decision', 'under'), ('under', 'uncertainty'), ('we', 'derive'), ('derive', 'axiomatically'), ('axiomatically', 'the'), ('the', 'probability'), ('probability', 'function'), ('function', 'that'), ('that', 'should'), ('should', 'be'), ('be', 'used'), ('used', 'to'), ('to', 'make'), ('make', 'decisions'), ('decisions', 'given'), ('given', 'any'), ('any', 'form'), ('form', 'of'), ('of', 'underlying'), ('underlying', 'uncertainty'), ('_UNK_', 'text'), ('text', 'analysis'), ('analysis', 'tools'), ('tools', 'in'), ('in', 'spoken'), ('spoken', 'language'), ('this', 'submission'), ('submission', 'contains'), ('contains', 'the'), ('the', 'postscript'), ('postscript', 'of'), ('the', 'final'), ('final', 'version'), ('version', 'of'), ('the', 'slides'), ('slides', 'used'), ('used', 'in'), ('in', 'our'), ('our', 'acl'), ('acl', '-'), ('-', '94'), ('94', 'tutorial'), ('tutorial', '.'), ('_UNK_', 'discrimination'), ('discrimination', 'between'), ('between', 'arabic'), ('arabic', 'and'), ('and', 'latin'), ('latin', 'from'), ('from', 'bilingual'), ('bilingual', 'documents'), ('documents', ';'), (';', '2011'), ('2011', 'international'), ('international', 'conference'), ('conference', 'on'), ('on', 'communications'), ('communications', ','), (',', 'computing'), ('computing', 'and'), ('and', 'control'), ('control', 'applications'), ('applications', '('), ('(', 'ccca'), ('ccca', ')'), ('a', 'machine'), ('machine', '-'), ('-', 'learning'), ('learning', 'framework'), ('framework', 'for'), ('for', 'design'), ('design', 'for'), ('for', 'manufacturability'), ('manufacturability', ';'), ('a', 'duplicate'), ('duplicate', 'submission'), ('submission', '('), ('(', 'original'), ('original', 'is'), ('is', 'arxiv'), ('arxiv', ':'), (':', '1612'), ('1612', '.'), ('.', '02141'), ('02141', ').'), (').', 'hence'), ('hence', 'want'), ('want', 'to'), ('to', 'withdraw'), ('withdraw', 'it'), ('a', 'theory'), ('of', 'experiment'), ('experiment', ';'), ('article', 'aims'), ('aims', 'at'), ('at', 'clarifying'), ('clarifying', 'the'), ('the', 'language'), ('language', 'and'), ('and', 'practice'), ('practice', 'of'), ('of', 'scientific'), ('scientific', 'experiment'), ('experiment', ','), (',', 'mainly'), ('mainly', 'by'), ('by', 'hooking'), ('hooking', 'observability'), ('observability', 'on'), ('on', 'calculability'), ('calculability', '.'), ('_UNK_', 'are'), ('are', 'minds'), ('minds', 'computable'), ('computable', '?'), ('this', 'essay'), ('essay', 'explores'), ('explores', 'the'), ('the', 'limits'), ('limits', 'of'), ('of', 'turing'), ('turing', 'machines'), ('machines', 'concerning'), ('concerning', 'the'), ('the', 'modeling'), ('modeling', 'of'), ('of', 'minds'), ('minds', 'and'), ('and', 'suggests'), ('suggests', 'alternatives'), ('alternatives', 'to'), ('to', 'go'), ('go', 'beyond'), ('beyond', 'those'), ('those', 'limits'), ('limits', '.'), ('_UNK_', 'extraction'), ('extraction', 'de'), ('de', 'concepts'), ('concepts', 'sous'), ('sous', 'contraintes'), ('contraintes', 'dans'), ('dans', 'des'), ('des', 'donn√©es'), ('donn√©es', 'd'), ('d', \"'\"), (\"'\", 'expression'), ('expression', 'de'), ('de', 'g√®nes'), ('g√®nes', ';'), ('paper', ','), (',', 'we'), ('we', 'propose'), ('propose', 'a'), ('a', 'technique'), ('technique', 'to'), ('to', 'extract'), ('extract', 'constrained'), ('constrained', 'formal'), ('formal', 'concepts'), ('concepts', '.'), ('_UNK_', 'comments'), ('comments', 'on'), ('on', '\"'), ('\"', 'a'), ('new', 'combination'), ('combination', 'of'), ('of', 'evidence'), ('evidence', 'based'), ('based', 'on'), ('on', 'compromise'), ('compromise', '\"'), ('\"', 'by'), ('by', 'k'), ('k', '.'), ('.', 'yamada'), ('yamada', ';'), (';', 'comments'), ('on', '``'), ('``', 'a'), ('compromise', \"''\"), (\"''\", 'by'), ('_UNK_', 'learning'), ('learning', 'low'), ('low', '-'), ('-', 'shot'), ('shot', 'facial'), ('facial', 'representations'), ('representations', 'via'), ('via', '2d'), ('2d', 'warping'), ('warping', ';'), ('this', 'work'), ('work', ','), ('we', 'mainly'), ('mainly', 'study'), ('study', 'the'), ('the', 'influence'), ('influence', 'of'), ('the', '2d'), ('warping', 'module'), ('module', 'for'), ('for', 'one'), ('one', '-'), ('shot', 'face'), ('face', 'recognition'), ('recognition', '.'), ('_UNK_', 'automatic'), ('automatic', 'generation'), ('generation', 'of'), ('of', 'benchmarks'), ('benchmarks', 'for'), ('for', 'plagiarism'), ('plagiarism', 'detection'), ('detection', 'tools'), ('tools', 'using'), ('using', 'grammatical'), ('grammatical', 'evolution'), ('authors', 'due'), ('a', 'major'), ('major', 'rewriting'), ('rewriting', '.'), ('_UNK_', 'resource'), ('allocation', 'of'), ('of', 'mu'), ('mu', '-'), ('-', 'ofdm'), ('systems', 'under'), ('under', 'partial'), ('partial', 'channel'), ('channel', 'state'), ('state', 'information'), ('information', ';'), ('to', 'some'), ('some', 'errors'), ('_UNK_', 'advances'), ('advances', 'in'), ('in', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'require'), ('require', 'progress'), ('progress', 'across'), ('across', 'all'), ('all', 'of'), ('of', 'computer'), ('computer', 'science'), ('science', ';'), (';', 'advances'), ('science', '.'), ('_UNK_', 'exploration'), ('exploration', 'of'), ('of', 'object'), ('object', 'recognition'), ('recognition', 'from'), ('from', '3d'), ('3d', 'point'), ('point', 'cloud'), ('cloud', ';'), ('present', 'our'), ('our', 'latest'), ('latest', 'experiment'), ('experiment', 'results'), ('cloud', 'data'), ('data', 'collected'), ('collected', 'through'), ('through', 'moving'), ('moving', 'car'), ('car', '.'), ('_UNK_', 'quantified'), ('quantified', 'conditional'), ('conditional', 'logics'), ('logics', 'are'), ('are', 'fragments'), ('fragments', 'of'), ('of', 'hol'), ('hol', ';'), ('a', 'semantic'), ('semantic', 'embedding'), ('embedding', 'of'), ('of', '('), ('(', 'constant'), ('constant', 'domain'), ('domain', ')'), (')', 'quantified'), ('conditional', 'logic'), ('logic', 'in'), ('in', 'classical'), ('classical', 'higher'), ('higher', '-'), ('-', 'order'), ('order', 'logic'), ('logic', 'is'), ('is', 'presented'), ('presented', '.'), ('_UNK_', 'in'), ('in', 'memoriam'), ('memoriam', 'maurice'), ('maurice', 'gross'), ('gross', ';'), (';', 'maurice'), ('gross', '('), ('(', '1934'), ('1934', '-'), ('-', '2001'), ('2001', ')'), (')', 'was'), ('was', 'both'), ('both', 'a'), ('a', 'great'), ('great', 'linguist'), ('linguist', 'and'), ('and', 'a'), ('a', 'pioneer'), ('pioneer', 'in'), ('in', 'natural'), ('processing', '.'), ('.', 'this'), ('is', 'written'), ('written', 'in'), ('in', 'homage'), ('homage', 'to'), ('to', 'his'), ('his', 'memory'), ('_UNK_', 'introduction'), ('the', '26th'), ('26th', 'international'), ('on', 'logic'), ('logic', 'programming'), ('programming', 'special'), ('issue', ';'), ('is', 'the'), ('the', 'preface'), ('preface', 'to'), ('_UNK_', 'beyond'), ('beyond', 'description'), ('description', '.'), ('.', 'comment'), ('\"', 'approaching'), ('approaching', 'human'), ('human', 'language'), ('language', 'with'), ('with', 'complex'), ('networks', '\"'), ('by', 'cong'), ('cong', '&'), ('&', 'liu'), ('liu', ';'), (';', 'comment'), ('_UNK_', 'norm'), ('norm', '-'), ('-', 'based'), ('based', 'capacity'), ('capacity', 'control'), ('control', 'in'), ('in', 'neural'), ('we', 'investigate'), ('investigate', 'the'), ('the', 'capacity'), ('capacity', ','), (',', 'convexity'), ('convexity', 'and'), ('and', 'characterization'), ('characterization', 'of'), ('a', 'general'), ('general', 'family'), ('family', 'of'), ('of', 'norm'), ('-', 'constrained'), ('constrained', 'feed'), ('feed', '-'), ('-', 'forward'), ('forward', 'networks'), ('networks', '.'), ('_UNK_', 'about'), ('about', 'compression'), ('compression', 'of'), ('of', 'vocabulary'), ('vocabulary', 'in'), ('in', 'computer'), ('computer', 'oriented'), ('oriented', 'languages'), ('languages', ';'), ('author', 'uses'), ('uses', 'the'), ('the', 'entropy'), ('entropy', 'of'), ('the', 'ideal'), ('ideal', 'bose'), ('bose', '-'), ('-', 'einstein'), ('einstein', 'gas'), ('gas', 'to'), ('to', 'minimize'), ('minimize', 'losses'), ('losses', 'in'), ('computer', '-'), ('-', 'oriented'), ('languages', '.'), ('_UNK_', 'unary'), ('unary', 'coding'), ('coding', 'for'), ('for', 'neural'), ('network', 'learning'), ('learning', ';'), ('presents', 'some'), ('some', 'properties'), ('properties', 'of'), ('of', 'unary'), ('coding', 'of'), ('of', 'significance'), ('significance', 'for'), ('for', 'biological'), ('biological', 'learning'), ('learning', 'and'), ('and', 'instantaneously'), ('instantaneously', 'trained'), ('trained', 'neural'), ('_UNK_', 'some'), ('the', 'ukrainian'), ('ukrainian', 'writing'), ('writing', 'system'), ('the', 'grapheme'), ('grapheme', '-'), ('-', 'phoneme'), ('phoneme', 'relation'), ('relation', 'in'), ('in', 'ukrainian'), ('ukrainian', 'and'), ('and', 'some'), ('ukrainian', 'version'), ('the', 'cyrillic'), ('cyrillic', 'alphabet'), ('alphabet', '.'), ('_UNK_', 'convex'), ('convex', 'multiview'), ('multiview', 'fisher'), ('fisher', 'discriminant'), ('discriminant', 'analysis'), ('analysis', ';'), (';', 'section'), ('section', '1'), ('.', '3'), ('3', 'was'), ('was', 'incorrect'), ('incorrect', ','), (',', 'and'), ('and', '2'), ('2', '.'), ('1', 'will'), ('be', 'removed'), ('removed', 'from'), ('from', 'further'), ('further', 'submissions'), ('submissions', '.'), ('.', 'a'), ('a', 'rewritten'), ('rewritten', 'version'), ('be', 'posted'), ('posted', 'in'), ('the', 'future'), ('future', '.'), ('_UNK_', 'why'), ('why', 'bother'), ('bother', 'with'), ('with', 'syntax'), ('syntax', '?'), ('this', 'short'), ('short', 'note'), ('note', 'discusses'), ('discusses', 'the'), ('the', 'role'), ('role', 'of'), ('of', 'syntax'), ('syntax', 'vs'), ('vs', '.'), ('.', 'semantics'), ('semantics', 'and'), ('and', 'the'), ('the', 'interplay'), ('interplay', 'between'), ('between', 'logic'), ('logic', ','), (',', 'philosophy'), ('philosophy', ','), ('and', 'language'), ('language', 'in'), ('science', 'and'), ('and', 'game'), ('game', 'theory'), ('theory', '.'), ('why', \"'\"), (\"'\", 'gsa'), ('gsa', ':'), ('a', 'gravitational'), ('gravitational', 'search'), ('search', 'algorithm'), ('algorithm', \"'\"), (\"'\", 'is'), ('is', 'not'), ('not', 'genuinely'), ('genuinely', 'based'), ('on', 'the'), ('the', 'law'), ('law', 'of'), ('of', 'gravity'), ('gravity', ';'), (';', 'why'), ('_UNK_', 'neurocontrol'), ('neurocontrol', 'methods'), ('methods', 'review'), ('review', ';'), (';', 'methods'), ('methods', 'of'), ('of', 'applying'), ('applying', 'neural'), ('networks', 'to'), ('to', 'control'), ('control', 'plants'), ('plants', 'are'), ('are', 'considered'), ('considered', '.'), ('.', 'methods'), ('methods', 'and'), ('and', 'schemes'), ('schemes', 'are'), ('described', ','), (',', 'their'), ('their', 'advantages'), ('advantages', 'and'), ('and', 'disadvantages'), ('disadvantages', 'are'), ('are', 'discussed'), ('discussed', '.'), ('_UNK_', 'on'), ('the', 'universality'), ('universality', 'of'), ('of', 'online'), ('online', 'mirror'), ('mirror', 'descent'), ('descent', ';'), ('we', 'show'), ('show', 'that'), ('that', 'for'), ('for', 'a'), ('general', 'class'), ('class', 'of'), ('of', 'convex'), ('convex', 'online'), ('online', 'learning'), ('learning', 'problems'), ('problems', ','), (',', 'mirror'), ('descent', 'can'), ('can', 'always'), ('always', 'achieve'), ('achieve', 'a'), ('a', '('), ('(', 'nearly'), ('nearly', ')'), (')', 'optimal'), ('optimal', 'regret'), ('regret', 'guarantee'), ('guarantee', '.'), ('the', 'possibility'), ('possibility', 'of'), ('of', 'making'), ('making', 'the'), ('the', 'complete'), ('complete', 'computer'), ('computer', 'model'), ('a', 'human'), ('human', 'brain'), ('brain', ';'), ('the', 'development'), ('development', 'of'), ('the', 'algorithm'), ('algorithm', 'of'), ('a', 'neural'), ('network', 'building'), ('building', 'by'), ('the', 'corresponding'), ('corresponding', 'parts'), ('parts', 'of'), ('a', 'dna'), ('dna', 'code'), ('code', 'is'), ('is', 'discussed'), ('_UNK_', 'philosophy'), ('philosophy', 'in'), ('the', 'face'), ('face', 'of'), ('of', 'artificial'), ('intelligence', ';'), ('article', ','), (',', 'i'), ('i', 'discuss'), ('discuss', 'how'), ('how', 'the'), ('the', 'ai'), ('ai', 'community'), ('community', 'views'), ('views', 'concerns'), ('concerns', 'about'), ('about', 'the'), ('the', 'emergence'), ('emergence', 'of'), ('of', 'superintelligent'), ('superintelligent', 'ai'), ('ai', 'and'), ('and', 'related'), ('related', 'philosophical'), ('philosophical', 'issues'), ('issues', '.'), ('a', 'survey'), ('survey', 'on'), ('on', 'contextual'), ('contextual', 'multi'), ('multi', '-'), ('-', 'armed'), ('armed', 'bandits'), ('bandits', ';'), ('this', 'survey'), ('survey', 'we'), ('we', 'cover'), ('cover', 'a'), ('a', 'few'), ('few', 'stochastic'), ('stochastic', 'and'), ('and', 'adversarial'), ('adversarial', 'contextual'), ('contextual', 'bandit'), ('bandit', 'algorithms'), ('algorithms', '.'), ('.', 'we'), ('we', 'analyze'), ('analyze', 'each'), ('each', 'algorithm'), ('s', 'assumption'), ('assumption', 'and'), ('and', 'regret'), ('regret', 'bound'), ('bound', '.'), ('_UNK_', 'parallels'), ('parallels', 'of'), ('of', 'human'), ('the', 'behavior'), ('behavior', 'of'), ('of', 'bottlenose'), ('bottlenose', 'dolphins'), ('dolphins', ';'), ('a', 'short'), ('short', 'review'), ('review', 'of'), ('of', 'similarities'), ('similarities', 'between'), ('between', 'dolphins'), ('dolphins', 'and'), ('and', 'humans'), ('humans', 'with'), ('with', 'the'), ('the', 'help'), ('help', 'of'), ('of', 'quantitative'), ('quantitative', 'linguistics'), ('linguistics', 'and'), ('and', 'information'), ('information', 'theory'), ('a', 'history'), ('history', 'of'), ('of', 'metaheuristics'), ('metaheuristics', ';'), ('this', 'chapter'), ('chapter', 'describes'), ('describes', 'the'), ('the', 'history'), ('metaheuristics', 'in'), ('in', 'five'), ('five', 'distinct'), ('distinct', 'periods'), ('periods', ','), (',', 'starting'), ('starting', 'long'), ('long', 'before'), ('before', 'the'), ('the', 'first'), ('first', 'use'), ('the', 'term'), ('term', 'and'), ('and', 'ending'), ('ending', 'a'), ('a', 'long'), ('long', 'time'), ('time', 'in'), ('_UNK_', 'an'), ('an', 'energy'), ('energy', 'efficient'), ('efficient', 'scheme'), ('scheme', 'for'), ('for', 'data'), ('data', 'gathering'), ('gathering', 'in'), ('in', 'wireless'), ('wireless', 'sensor'), ('sensor', 'networks'), ('networks', 'using'), ('using', 'particle'), ('particle', 'swarm'), ('swarm', 'optimization'), ('optimization', ';'), ('crucial', 'sign'), ('sign', 'error'), ('in', 'equation'), ('equation', '1'), ('new', 'bengali'), ('bengali', 'readability'), ('readability', 'score'), ('score', ';'), ('paper', 'we'), ('we', 'have'), ('have', 'proposed'), ('proposed', 'methods'), ('methods', 'to'), ('to', 'analyze'), ('analyze', 'the'), ('the', 'readability'), ('readability', 'of'), ('of', 'bengali'), ('bengali', 'language'), ('language', 'texts'), ('texts', '.'), ('have', 'got'), ('got', 'some'), ('some', 'exceptionally'), ('exceptionally', 'good'), ('good', 'results'), ('results', 'out'), ('out', 'of'), ('the', 'experiments'), ('experiments', '.'), ('the', 'logic'), ('programming', 'paradigm'), ('paradigm', 'and'), ('and', 'prolog'), ('prolog', ';'), ('a', 'tutorial'), ('tutorial', 'on'), ('programming', 'and'), ('prolog', 'appropriate'), ('appropriate', 'for'), ('a', 'course'), ('course', 'on'), ('on', 'programming'), ('programming', 'languages'), ('languages', 'for'), ('for', 'students'), ('students', 'familiar'), ('familiar', 'with'), ('with', 'imperative'), ('imperative', 'programming'), ('programming', '.'), ('_UNK_', 'calculate'), ('calculate', 'distance'), ('distance', 'to'), ('to', 'object'), ('object', 'in'), ('the', 'area'), ('area', 'where'), ('where', 'car'), ('car', ','), (',', 'using'), ('using', 'video'), ('video', 'analysis'), ('the', 'method'), ('of', 'using'), ('video', 'cameras'), ('cameras', 'installed'), ('installed', 'on'), ('the', 'car'), (',', 'to'), ('to', 'calculate'), ('the', 'distance'), ('the', 'object'), ('in', 'its'), ('its', 'area'), ('area', 'of'), ('of', 'movement'), ('movement', '.'), ('_UNK_', 'group'), ('group', 'theory'), ('theory', ','), (',', 'group'), ('group', 'actions'), ('actions', ','), (',', 'evolutionary'), ('evolutionary', 'algorithms'), ('algorithms', ','), ('and', 'global'), ('global', 'optimization'), ('use', 'group'), ('group', ','), (',', 'action'), ('action', 'and'), ('and', 'orbit'), ('orbit', 'to'), ('to', 'understand'), ('understand', 'how'), ('how', 'evolutionary'), ('evolutionary', 'solve'), ('solve', 'nonconvex'), ('nonconvex', 'optimization'), ('optimization', 'problems'), ('problems', '.'), ('_UNK_', 'entropy'), ('of', 'telugu'), ('telugu', ';'), ('presents', 'an'), ('an', 'investigation'), ('investigation', 'of'), ('the', 'telugu'), ('telugu', 'script'), ('script', '.'), ('.', 'since'), ('since', 'this'), ('this', 'script'), ('script', 'is'), ('is', 'syllabic'), ('syllabic', ','), ('and', 'not'), ('not', 'alphabetic'), ('alphabetic', ','), (',', 'the'), ('the', 'computation'), ('computation', 'of'), ('of', 'entropy'), ('entropy', 'is'), ('is', 'somewhat'), ('somewhat', 'complicated'), ('complicated', '.'), ('_UNK_', 'word'), ('word', 'segmentation'), ('segmentation', 'on'), ('on', 'micro'), ('micro', '-'), ('-', 'blog'), ('blog', 'texts'), ('texts', 'with'), ('with', 'external'), ('external', 'lexicon'), ('lexicon', 'and'), ('and', 'heterogeneous'), ('heterogeneous', 'data'), ('data', ';'), ('paper', 'describes'), ('describes', 'our'), ('our', 'system'), ('system', 'designed'), ('designed', 'for'), ('the', 'nlpcc'), ('nlpcc', '2016'), ('2016', 'shared'), ('shared', 'task'), ('task', 'on'), ('on', 'word'), ('_UNK_', 'guarded'), ('guarded', 'resolution'), ('resolution', 'for'), ('for', 'answer'), ('answer', 'set'), ('set', 'programming'), ('programming', ';'), ('we', 'describe'), ('describe', 'a'), ('a', 'variant'), ('variant', 'of'), ('of', 'resolution'), ('resolution', 'rule'), ('rule', 'of'), ('of', 'proof'), ('proof', 'and'), ('and', 'show'), ('that', 'it'), ('is', 'complete'), ('complete', 'for'), ('for', 'stable'), ('stable', 'semantics'), ('semantics', 'of'), ('of', 'logic'), ('logic', 'programs'), ('programs', '.'), ('show', 'applications'), ('applications', 'of'), ('of', 'this'), ('this', 'result'), ('result', '.'), ('_UNK_', 'cornell'), ('cornell', 'spf'), ('spf', ':'), (':', 'cornell'), ('cornell', 'semantic'), ('semantic', 'parsing'), ('parsing', 'framework'), ('framework', ';'), ('the', 'cornell'), ('framework', '('), ('(', 'spf'), ('spf', ')'), (')', 'is'), ('a', 'learning'), ('and', 'inference'), ('inference', 'framework'), ('for', 'mapping'), ('mapping', 'natural'), ('language', 'to'), ('to', 'formal'), ('formal', 'representation'), ('of', 'its'), ('its', 'meaning'), ('meaning', '.'), ('_UNK_', 'semistability'), ('semistability', '-'), ('based', 'convergence'), ('convergence', 'analysis'), ('for', 'paracontracting'), ('paracontracting', 'multiagent'), ('multiagent', 'coordination'), ('coordination', 'optimization'), ('this', 'sequential'), ('sequential', 'technical'), ('technical', 'report'), ('report', 'extends'), ('extends', 'some'), ('some', 'of'), ('the', 'previous'), ('previous', 'results'), ('results', 'we'), ('we', 'posted'), ('posted', 'at'), ('at', 'arxiv'), (':', '1306'), ('1306', '.'), ('.', '0225'), ('0225', '.'), ('_UNK_', 'proceedings'), ('proceedings', 'of'), ('first', 'international'), ('international', 'workshop'), ('workshop', 'on'), ('on', 'deep'), ('deep', 'learning'), ('and', 'music'), ('music', ';'), (';', 'proceedings'), ('music', ','), (',', 'joint'), ('joint', 'with'), ('with', 'ijcnn'), ('ijcnn', ','), (',', 'anchorage'), ('anchorage', ','), (',', 'us'), ('us', ','), ('may', '17'), ('17', '-'), ('-', '18'), ('18', ','), (',', '2017'), ('_UNK_', 'attack'), ('attack', 'rmse'), ('rmse', 'leaderboard'), ('leaderboard', ':'), (':', 'an'), ('an', 'introduction'), ('introduction', 'and'), ('and', 'case'), ('case', 'study'), ('study', ';'), ('this', 'manuscript'), ('manuscript', ','), ('we', 'briefly'), ('briefly', 'introduce'), ('introduce', 'several'), ('several', 'tricks'), ('tricks', 'to'), ('to', 'climb'), ('climb', 'the'), ('the', 'leaderboards'), ('leaderboards', 'which'), ('which', 'use'), ('use', 'rmse'), ('rmse', 'for'), ('for', 'evaluation'), ('evaluation', 'without'), ('without', 'exploiting'), ('exploiting', 'any'), ('any', 'training'), ('training', 'data'), ('data', '.'), ('_UNK_', 'standardization'), ('standardization', 'of'), ('the', 'formal'), ('of', 'lexical'), ('lexical', 'information'), ('information', 'for'), ('for', 'nlp'), ('nlp', ';'), ('survey', 'of'), ('of', 'dictionary'), ('dictionary', 'models'), ('models', 'and'), ('and', 'formats'), ('formats', 'is'), ('presented', 'as'), ('as', 'well'), ('well', 'as'), ('a', 'presentation'), ('presentation', 'of'), ('of', 'corresponding'), ('corresponding', 'recent'), ('recent', 'standardisation'), ('standardisation', 'activities'), ('activities', '.'), ('machine', 'learning'), ('learning', 'model'), ('model', 'for'), ('for', 'stock'), ('stock', 'market'), ('market', 'prediction'), ('prediction', ';'), (';', 'stock'), ('prediction', 'is'), ('the', 'act'), ('act', 'of'), ('of', 'trying'), ('trying', 'to'), ('to', 'determine'), ('determine', 'the'), ('future', 'value'), ('value', 'of'), ('a', 'company'), ('company', 'stock'), ('stock', 'or'), ('or', 'other'), ('other', 'financial'), ('financial', 'instrument'), ('instrument', 'traded'), ('traded', 'on'), ('on', 'a'), ('a', 'financial'), ('financial', 'exchange'), ('exchange', '.'), ('_UNK_', 'defensive'), ('defensive', 'distillation'), ('distillation', 'is'), ('not', 'robust'), ('robust', 'to'), ('to', 'adversarial'), ('adversarial', 'examples'), ('examples', ';'), ('that', 'defensive'), ('not', 'secure'), ('secure', ':'), (':', 'it'), ('no', 'more'), ('more', 'resistant'), ('resistant', 'to'), ('to', 'targeted'), ('targeted', 'misclassification'), ('misclassification', 'attacks'), ('attacks', 'than'), ('than', 'unprotected'), ('unprotected', 'neural'), ('of', 'nips'), ('nips', '2017'), ('2017', 'symposium'), ('symposium', 'on'), ('on', 'interpretable'), ('interpretable', 'machine'), ('the', 'proceedings'), ('learning', ','), (',', 'held'), ('held', 'in'), ('in', 'long'), ('long', 'beach'), ('beach', ','), (',', 'california'), ('california', ','), (',', 'usa'), ('usa', 'on'), ('on', 'december'), ('december', '7'), ('7', ','), ('_UNK_', 'piecewise'), ('piecewise', 'linear'), ('linear', 'activation'), ('activation', 'functions'), ('functions', 'for'), ('for', 'more'), ('more', 'efficient'), ('efficient', 'deep'), ('deep', 'networks'), ('submission', 'has'), ('by', 'arxiv'), ('arxiv', 'administrators'), ('administrators', 'because'), ('is', 'intentionally'), ('intentionally', 'incomplete'), ('incomplete', ','), (',', 'which'), ('which', 'is'), ('is', 'in'), ('in', 'violation'), ('violation', 'of'), ('of', 'our'), ('our', 'policies'), ('policies', '.'), ('the', 'triangle'), ('triangle', 'inequality'), ('inequality', 'for'), ('the', 'jaccard'), ('jaccard', 'distance'), ('distance', ';'), (';', 'two'), ('two', 'simple'), ('simple', 'proofs'), ('proofs', 'of'), ('distance', 'in'), ('in', 'terms'), ('terms', 'of'), ('of', 'nonnegative'), ('nonnegative', ','), (',', 'monotone'), ('monotone', ','), (',', 'submodular'), ('submodular', 'functions'), ('functions', 'are'), ('are', 'given'), ('given', 'and'), ('and', 'discussed'), ('_UNK_', 'how'), ('how', 'to'), ('to', 'realize'), ('realize', '\"'), ('a', 'sense'), ('sense', 'of'), ('of', 'humour'), ('humour', '\"'), ('\"', 'in'), ('in', 'computers'), ('computers', '?'), (';', 'computer'), ('a', '\"'), ('\"', 'sense'), ('\"', 'suggested'), ('suggested', 'previously'), ('previously', '['), ('[', 'arxiv'), (':', '0711'), ('0711', '.'), ('.', '2058'), ('2058', ','), (',', '0711'), ('.', '2061'), ('2061', ','), ('.', '2270'), ('2270', ']'), (']', 'is'), ('is', 'raised'), ('raised', 'to'), ('the', 'level'), ('level', 'of'), ('a', 'realistic'), ('realistic', 'algorithm'), ('algorithm', '.'), ('linear', 'multilayer'), ('multilayer', 'perceptrons'), ('perceptrons', 'and'), ('and', 'dropout'), ('dropout', ';'), ('new', 'type'), ('type', 'of'), ('of', 'hidden'), ('hidden', 'layer'), ('layer', 'for'), ('a', 'multilayer'), ('multilayer', 'perceptron'), ('perceptron', ','), ('and', 'demonstrate'), ('demonstrate', 'that'), ('it', 'obtains'), ('obtains', 'the'), ('the', 'best'), ('best', 'reported'), ('reported', 'performance'), ('performance', 'for'), ('for', 'an'), ('an', 'mlp'), ('mlp', 'on'), ('the', 'mnist'), ('mnist', 'dataset'), ('dataset', '.'), ('_UNK_', 'technical'), ('report', ':'), ('a', 'tool'), ('tool', 'for'), ('for', 'measuring'), ('measuring', 'prosodic'), ('prosodic', 'accommodation'), ('accommodation', ';'), ('article', 'has'), ('because', 'the'), ('the', 'submitter'), ('submitter', 'did'), ('did', 'not'), ('not', 'have'), ('have', 'the'), ('the', 'legal'), ('legal', 'authority'), ('authority', 'to'), ('to', 'grant'), ('grant', 'the'), ('the', 'license'), ('license', 'applied'), ('applied', 'to'), ('the', 'work'), ('work', '.'), ('a', 'remark'), ('remark', 'on'), ('on', 'higher'), ('higher', 'order'), ('order', 'rue'), ('rue', '-'), ('-', 'resolution'), ('resolution', 'with'), ('with', 'extrue'), ('extrue', ';'), ('that', 'a'), ('a', 'prominent'), ('prominent', 'counterexample'), ('counterexample', 'for'), ('the', 'completeness'), ('completeness', 'of'), ('of', 'first'), ('first', 'order'), ('resolution', 'does'), ('does', 'not'), ('not', 'apply'), ('apply', 'to'), ('the', 'higher'), ('resolution', 'approach'), ('approach', 'extrue'), ('extrue', '.'), ('_UNK_', 'sat'), ('sat', 'as'), ('a', 'game'), ('game', ';'), ('a', 'funny'), ('funny', 'representation'), ('of', 'sat'), ('sat', '.'), ('.', 'while'), ('while', 'the'), ('the', 'primary'), ('primary', 'interest'), ('interest', 'is'), ('is', 'to'), ('to', 'present'), ('present', 'propositional'), ('propositional', 'satisfiability'), ('satisfiability', 'in'), ('a', 'playful'), ('playful', 'way'), ('way', 'for'), ('for', 'pedagogical'), ('pedagogical', 'purposes'), ('purposes', ','), (',', 'it'), ('it', 'could'), ('could', 'also'), ('also', 'inspire'), ('inspire', 'new'), ('new', 'search'), ('search', 'heuristics'), ('heuristics', '.'), ('on', 'adjusting'), ('adjusting', '$'), ('$', 'r'), ('r', '^'), ('^', '2'), ('2', '$'), ('$', 'for'), ('for', 'using'), ('using', 'with'), ('with', 'cross'), ('cross', '-'), ('-', 'validation'), ('validation', ';'), ('show', 'how'), ('to', 'adjust'), ('adjust', 'the'), ('the', 'coefficient'), ('coefficient', 'of'), ('of', 'determination'), ('determination', '($'), ('($', 'r'), ('2', '$)'), ('$)', 'when'), ('when', 'used'), ('used', 'for'), ('measuring', 'predictive'), ('predictive', 'accuracy'), ('accuracy', 'via'), ('via', 'leave'), ('leave', '-'), ('-', 'one'), ('-', 'out'), ('out', 'cross'), ('validation', '.'), ('_UNK_', 'approximated'), ('approximated', 'structured'), ('structured', 'prediction'), ('prediction', 'for'), ('for', 'learning'), ('learning', 'large'), ('scale', 'graphical'), ('graphical', 'models'), ('models', ';'), ('this', 'manuscripts'), ('manuscripts', 'contains'), ('the', 'proofs'), ('proofs', 'for'), ('for', '\"'), ('a', 'primal'), ('primal', '-'), ('dual', 'message'), ('message', '-'), ('-', 'passing'), ('passing', 'algorithm'), ('algorithm', 'for'), ('for', 'approximated'), ('approximated', 'large'), ('scale', 'structured'), ('prediction', '\".'), ('automatic', 'liver'), ('liver', 'segmentation'), ('segmentation', 'method'), ('method', 'in'), ('in', 'ct'), ('ct', 'images'), ('the', 'aim'), ('aim', 'of'), ('work', 'is'), ('develop', 'a'), ('method', 'for'), ('for', 'automatic'), ('automatic', 'segmentation'), ('segmentation', 'of'), ('the', 'liver'), ('liver', 'based'), ('a', 'priori'), ('priori', 'knowledge'), ('knowledge', 'of'), ('the', 'image'), ('image', ','), (',', 'such'), ('such', 'as'), ('as', 'location'), ('location', 'and'), ('and', 'shape'), ('shape', 'of'), ('liver', '.'), ('the', 'existence'), ('existence', 'of'), ('a', 'projective'), ('projective', 'reconstruction'), ('we', 'study'), ('the', 'connection'), ('connection', 'between'), ('between', 'the'), ('reconstruction', 'and'), ('a', 'fundamental'), ('fundamental', 'matrix'), ('matrix', 'satisfying'), ('satisfying', 'the'), ('the', 'epipolar'), ('epipolar', 'constraints'), ('constraints', '.'), ('_UNK_', 'solving'), ('solving', 'traveling'), ('traveling', 'salesman'), ('salesman', 'problem'), ('problem', 'by'), ('by', 'marker'), ('marker', 'method'), ('method', ';'), ('use', 'marker'), ('method', 'and'), ('and', 'propose'), ('new', 'mutation'), ('mutation', 'operator'), ('operator', 'that'), ('that', 'selects'), ('selects', 'the'), ('the', 'nearest'), ('nearest', 'neighbor'), ('neighbor', 'among'), ('among', 'all'), ('all', 'near'), ('near', 'neighbors'), ('neighbors', 'solving'), ('problem', '.'), ('a', 'primer'), ('primer', 'on'), ('on', 'answer'), ('a', 'introduction'), ('the', 'syntax'), ('syntax', 'and'), ('and', 'semantics'), ('of', 'answer'), ('programming', 'intended'), ('intended', 'as'), ('as', 'an'), ('an', 'handout'), ('handout', 'to'), ('to', '['), ('[', 'under'), ('under', ']'), (']', 'graduate'), ('graduate', 'students'), ('students', 'taking'), ('taking', 'artificial'), ('artificial', 'intlligence'), ('intlligence', 'or'), ('or', 'logic'), ('programming', 'classes'), ('classes', '.'), ('_UNK_', 'agent'), ('agent', 'models'), ('models', 'of'), ('of', 'political'), ('political', 'interactions'), ('interactions', ';'), (';', 'looks'), ('looks', 'at'), ('at', 'state'), ('state', 'interactions'), ('interactions', 'from'), ('from', 'an'), ('an', 'agent'), ('agent', 'based'), ('based', 'ai'), ('ai', 'perspective'), ('perspective', 'to'), ('to', 'see'), ('see', 'state'), ('interactions', 'as'), ('an', 'example'), ('example', 'of'), ('of', 'emergent'), ('emergent', 'intelligent'), ('intelligent', 'behavior'), ('behavior', '.'), ('.', 'exposes'), ('exposes', 'basic'), ('basic', 'principles'), ('principles', 'of'), ('of', 'game'), ('learning', 'states'), ('states', 'representations'), ('representations', 'in'), ('in', 'pomdp'), ('pomdp', ';'), ('propose', 'to'), ('to', 'deal'), ('deal', 'with'), ('with', 'sequential'), ('sequential', 'processes'), ('processes', 'where'), ('where', 'only'), ('only', 'partial'), ('partial', 'observations'), ('observations', 'are'), ('are', 'available'), ('available', 'by'), ('by', 'learning'), ('learning', 'a'), ('a', 'latent'), ('latent', 'representation'), ('representation', 'space'), ('space', 'on'), ('on', 'which'), ('which', 'policies'), ('policies', 'may'), ('may', 'be'), ('be', 'accurately'), ('accurately', 'learned'), ('learned', '.')])\n",
            "\n",
            "dict_keys(['differential', 'what', 'p', 'computational', 'weak', 'creating', 'defeasible', 'essence', 'deep', 'statistical', 'complex', 'serious', 'preprocessing', 'liquid', 'mining', 'towards', 'a', 'icon', 'recognition', 'glottochronologic', 'the', 'utility', 'temporized', 'backpropagation', 'random', 'network', 'glottochronology', 'using', 'time', 'convolutional', 'fitness', 'flip', 'autonomous', 'activitynet', 'decision', 'text', 'discrimination', 'are', 'extraction', 'comments', 'learning', 'automatic', 'resource', 'advances', 'exploration', 'quantified', 'in', 'introduction', 'beyond', 'norm', 'about', 'unary', 'some', 'convex', 'why', 'neurocontrol', 'on', 'philosophy', 'parallels', 'an', 'calculate', 'group', 'entropy', 'word', 'guarded', 'cornell', 'semistability', 'proceedings', 'attack', 'standardization', 'defensive', 'piecewise', 'how', 'technical', 'sat', 'approximated', 'solving', 'agent'])\n",
            "dict_keys(['differential', 'what', 'p', 'computational', 'weak', 'creating', 'defeasible', 'essence', 'deep', 'statistical', 'complex', 'serious', 'preprocessing', 'liquid', 'mining', 'towards', 'a', 'icon', 'recognition', 'glottochronologic', 'the', 'utility', 'temporized', 'backpropagation', 'random', 'network', 'glottochronology', 'using', 'time', 'convolutional', 'fitness', 'flip', 'autonomous', 'activitynet', 'decision', 'text', 'discrimination', 'are', 'extraction', 'comments', 'learning', 'automatic', 'resource', 'advances', 'exploration', 'quantified', 'in', 'introduction', 'beyond', 'norm', 'about', 'unary', 'some', 'convex', 'why', 'neurocontrol', 'on', 'philosophy', 'parallels', 'an', 'calculate', 'group', 'entropy', 'word', 'guarded', 'cornell', 'semistability', 'proceedings', 'attack', 'standardization', 'defensive', 'piecewise', 'how', 'technical', 'sat', 'approximated', 'solving', 'agent'])\n",
            "\n",
            "Counter({'a': 13, 'the': 3, 'on': 3, 'using': 2, 'learning': 2, 'automatic': 2, 'why': 2, 'proceedings': 2, 'piecewise': 2, 'differential': 1, 'what': 1, 'p': 1, 'computational': 1, 'weak': 1, 'creating': 1, 'defeasible': 1, 'essence': 1, 'deep': 1, 'statistical': 1, 'complex': 1, 'serious': 1, 'preprocessing': 1, 'liquid': 1, 'mining': 1, 'towards': 1, 'icon': 1, 'recognition': 1, 'glottochronologic': 1, 'utility': 1, 'temporized': 1, 'backpropagation': 1, 'random': 1, 'network': 1, 'glottochronology': 1, 'time': 1, 'convolutional': 1, 'fitness': 1, 'flip': 1, 'autonomous': 1, 'activitynet': 1, 'decision': 1, 'text': 1, 'discrimination': 1, 'are': 1, 'extraction': 1, 'comments': 1, 'resource': 1, 'advances': 1, 'exploration': 1, 'quantified': 1, 'in': 1, 'introduction': 1, 'beyond': 1, 'norm': 1, 'about': 1, 'unary': 1, 'some': 1, 'convex': 1, 'neurocontrol': 1, 'philosophy': 1, 'parallels': 1, 'an': 1, 'calculate': 1, 'group': 1, 'entropy': 1, 'word': 1, 'guarded': 1, 'cornell': 1, 'semistability': 1, 'attack': 1, 'standardization': 1, 'defensive': 1, 'how': 1, 'technical': 1, 'sat': 1, 'approximated': 1, 'solving': 1, 'agent': 1})\n",
            "{'differential': 0.01, 'what': 0.01, 'p': 0.01, 'computational': 0.01, 'weak': 0.01, 'creating': 0.01, 'defeasible': 0.01, 'essence': 0.01, 'deep': 0.01, 'statistical': 0.01, 'complex': 0.01, 'serious': 0.01, 'preprocessing': 0.01, 'liquid': 0.01, 'mining': 0.01, 'towards': 0.01, 'a': 0.13, 'icon': 0.01, 'recognition': 0.01, 'glottochronologic': 0.01, 'the': 0.03, 'utility': 0.01, 'temporized': 0.01, 'backpropagation': 0.01, 'random': 0.01, 'network': 0.01, 'glottochronology': 0.01, 'using': 0.02, 'time': 0.01, 'convolutional': 0.01, 'fitness': 0.01, 'flip': 0.01, 'autonomous': 0.01, 'activitynet': 0.01, 'decision': 0.01, 'text': 0.01, 'discrimination': 0.01, 'are': 0.01, 'extraction': 0.01, 'comments': 0.01, 'learning': 0.02, 'automatic': 0.02, 'resource': 0.01, 'advances': 0.01, 'exploration': 0.01, 'quantified': 0.01, 'in': 0.01, 'introduction': 0.01, 'beyond': 0.01, 'norm': 0.01, 'about': 0.01, 'unary': 0.01, 'some': 0.01, 'convex': 0.01, 'why': 0.02, 'neurocontrol': 0.01, 'on': 0.03, 'philosophy': 0.01, 'parallels': 0.01, 'an': 0.01, 'calculate': 0.01, 'group': 0.01, 'entropy': 0.01, 'word': 0.01, 'guarded': 0.01, 'cornell': 0.01, 'semistability': 0.01, 'proceedings': 0.02, 'attack': 0.01, 'standardization': 0.01, 'defensive': 0.01, 'piecewise': 0.02, 'how': 0.01, 'technical': 0.01, 'sat': 0.01, 'approximated': 0.01, 'solving': 0.01, 'agent': 0.01}\n",
            "0.01\n",
            "\n",
            "{'divergence': 1.0}\n",
            "{'to': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Init model"
      ],
      "metadata": {
        "id": "npFkgRPWnX1E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbJiEpuoq4R_"
      },
      "source": [
        "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyetR4yPq4SA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cad1c31-5ee0-4bbe-9be4-2544aaa9a91a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 13s, sys: 23.5 s, total: 2min 36s\n",
            "Wall time: 2min 37s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "lm = NGramLanguageModel(lines, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "s3yQk8GOKxbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling with Temperature"
      ],
      "metadata": {
        "id": "bUvqM-gQwJQc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pbYoRlaq4SA"
      },
      "source": [
        "The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n",
        "\n",
        "$ X = [] $\n",
        "\n",
        "__forever:__\n",
        "* $w_{next} \\sim P(w_{next} | X)$\n",
        "* $X = concat(X, w_{next})$\n",
        "\n",
        "\n",
        "Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature. In the latter case (temperature), one samples from\n",
        "\n",
        "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
        "\n",
        "Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSZWOL3Bq4SA"
      },
      "outputs": [],
      "source": [
        "def get_next_token(lm, prefix, temperature=1.0):\n",
        "    \"\"\"\n",
        "    return next token after prefix;\n",
        "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily. # –≤—Å–µ–≥–¥–∞ –±–µ—Ä–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω. –†–∞–∑—Ä—ã–≤–∞—Ç—å —Å–≤—è–∑–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ\n",
        "    \"\"\"\n",
        "    #<YOUR CODE>\n",
        "    suffixes = lm.get_possible_next_tokens(prefix)\n",
        "    tokens, token_probs = list(suffixes.keys()), list(suffixes.values())\n",
        "    #print(len(tokens), len(token_probs))\n",
        "    if temperature > 0:\n",
        "        token_probs = [p ** (1/temperature) for p in token_probs]\n",
        "        # –ù–æ—Ä–º–∏—Ä—É–µ–º\n",
        "        total = sum(token_probs)\n",
        "        token_probs = [ p / total for p in token_probs]\n",
        "        next_token = np.random.choice(tokens, p = token_probs)\n",
        "    # –µ—Å–ª–∏ temperature ==0\n",
        "    elif temperature==0:\n",
        "        if len(token_probs)>0:\n",
        "            next_token = tokens[np.argmax(token_probs)]\n",
        "        else:\n",
        "            next_token  = ''\n",
        "    else: # —Ç–∞–∫–æ–µ —Ç–æ–∂–µ –±—ã–≤–∞–µ—Ç –≤ –¥—Ä—É–≥–∏—Ö LM\n",
        "        if len(token_probs)>0:\n",
        "            next_token = tokens[np.argmax(token_probs)]\n",
        "        else:\n",
        "            next_token  = ''\n",
        "\n",
        "    return next_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJmQ2pE2q4SB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292a0246-e323-4035-fb5b-925742a447c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks nice!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
        "assert 250 < test_freqs['not'] < 450\n",
        "assert 8500 < test_freqs['been'] < 9500\n",
        "assert 1 < test_freqs['lately'] < 200\n",
        "\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
        "assert 1500 < test_freqs['learning'] < 3000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
        "assert 8000 < test_freqs['learning'] < 9000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
        "assert test_freqs['learning'] == 10000\n",
        "\n",
        "print(\"Looks nice!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling GPT2 HuggingFace"
      ],
      "metadata": {
        "id": "QdEZfib8wXIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "QPjYZJuyPVev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "model = transformers.AutoModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e0eff9e60a9a44e4a98d735f67008ef9",
            "fad58d15324245b6b75e3be6d5663b44",
            "13bd46e99c184a6aa0f920c212b8c929",
            "f77df3d8edcc48fd9c714b7d8e9a616b",
            "94b47f17600b47468a442bc15b112187",
            "e61313beae404ff0ac82be94d00ccf1f",
            "46af1e18e32c44c589454b5e48c54306",
            "8bebc247a61c4b66a9b3dc81713ab7ab",
            "30a44d126db547e5b71c13c7e8cad0d2",
            "9fccf7a3c5b54ed587ae7c6417e71b8c",
            "e5e017d346f3492ab622a774d4d79a5c",
            "b7053e0107374f87b8bb0e0b2545868a",
            "9a06dfdfc02042f5a4b4f09687417f2a",
            "2e6d41f27c674309b2fa863526e9528c",
            "0c03acea8e034ea0a2aeabef27860ace",
            "471a18eaae7d4c368ea42a8004b6977d",
            "c4717a3b9c6847c0b99a714ae8e5b951",
            "b538fe48c2af4d29bd60e4fb434f32ca",
            "a1f1e7e3b25346b19d102abadf1e7695",
            "808b409732834d29a66af944f79b284d",
            "373a2d36fdd347a794da7381246cff91",
            "2cb66700b43742bab1e629865d1189c7"
          ]
        },
        "id": "mSM8xaDHPcMz",
        "outputId": "8ea109be-d422-4627-b83b-7ec50091f3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0eff9e60a9a44e4a98d735f67008ef9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7053e0107374f87b8bb0e0b2545868a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizator = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "_PioRsG_QFD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "58eda14fb3cb442e93232ffb89adfd76",
            "5cdda5a9abe84ef8b659ef08eea4497c",
            "cf2dfe9550b0427faee8bda834849f42",
            "4fa62771e0354b40a595256a4f389bef",
            "0002d0274b374e3b83161f4a026cb916",
            "aa4ed503c22546418dd90d158ec2d174",
            "336e39bdbf9646159cf5a44f0aab0c65",
            "3802eec884c64d6d928f631b8bb19506",
            "2863bb5b39fb4a2ea9f7db25d5f6c115",
            "dab50edaeea94b36964711a3bed7face",
            "310ef09f51c34327898ab258c0832d5b",
            "6e7e1f90b0eb4dd8b6b0ad8d7ff07d28",
            "1b69d1a3091a4a6d849d8ee1fcfa2393",
            "9721f56b62dd40a4b86914912ece4666",
            "4f9bfa793e764ebdbcbfdfd94383a5c7",
            "fee6d7e306c743b6a448060ef2d41c05",
            "456ac64f5c3547679286dcf4989e973a",
            "12393c12f5064027945d581563dd2e63",
            "71ffff68ba9e44d3a94f92585af13c41",
            "c997c157ca53437f8ceac26524fa7272",
            "9573f858498241c19f4f42e9e8317a89",
            "759b0cd987164000b1cf35e9d80bf65d",
            "fdb252068e2e439f877ad5e0b95f012a",
            "2a75a40a34d94366aca19e13f7450291",
            "cd17f321ce164ac8bdecb86a27b0afd0",
            "af886b9596bd4b9bad5b98801e73d195",
            "ad27ea45229d458a85bbafe0937a750a",
            "ebf29c5f551d4b4690b6c0f2095bc364",
            "819a6c9274d544339eb379f3f490dd1f",
            "ebe06cd313d440a8a348e8469a31fa97",
            "5f579a6c1c8c4a17811142323b3552c5",
            "87035a9f802a4ad9afdd367ee85768e7",
            "1f1202298f894bb7a7b3fc226e4e82d9"
          ]
        },
        "outputId": "86bff098-3b78-4094-b209-bfbe4c4cb53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58eda14fb3cb442e93232ffb89adfd76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e7e1f90b0eb4dd8b6b0ad8d7ff07d28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdb252068e2e439f877ad5e0b95f012a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizator(\"A cat sat on a mat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It4B45FUQkbu",
        "outputId": "e70ea4da-6229-4f3b-9da2-086c4f9af5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [32, 3797, 3332, 319, 257, 2603], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').train(False).to(device)\n",
        "\n",
        "text = \"The Fermi paradox \"\n",
        "text = \"This NLP Semniar\"\n",
        "text = \"#include <iostream\"\n",
        "text = \"import transformers\"\n",
        "text = \"class SelfImprovingAGI (nn.Module)\"\n",
        "text = \"JURASSIC-1 ‚Äì Language Model\"\n",
        "text = \"Barack Obbama was born\"\n",
        "\n",
        "tokens = tokenizer.encode(text)\n",
        "num_steps = 1024\n",
        "line_length, max_length = 0, 70\n",
        "\n",
        "print(end=tokenizer.decode(tokens))\n",
        "try:\n",
        "    for i in range(num_steps):\n",
        "        with torch.no_grad():\n",
        "            logits = model(torch.as_tensor([tokens], device=device))[0]\n",
        "        #p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
        "        p_next = torch.softmax(logits[0, -1, :]*1.2, dim=-1).data.cpu().numpy() # –ø–æ–≤—ã—à–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–≤—è–∑–Ω–æ—Å—Ç–∏\n",
        "\n",
        "        #next_token_index = p_next.argmax() #<YOUR CODE: REPLACE THIS LINE>\n",
        "        next_token_index = np.random.choice(len(p_next), p=p_next)\n",
        "        # YOUR TASK: change the code so that it performs nucleus sampling\n",
        "\n",
        "        tokens.append(int(next_token_index))\n",
        "        print(end=tokenizer.decode(tokens[-1]))\n",
        "        line_length += len(tokenizer.decode(tokens[-1]))\n",
        "        if line_length >= max_length:\n",
        "            line_length = 0\n",
        "            print()\n",
        "\n",
        "except KeyboardInterrupt:         \n",
        "    print()\n",
        "    print(10*'*' + ' -  KeyboardInterrupt - ' + 10*'*')\n",
        "    #pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFNJNEMURE1s",
        "outputId": "075a98af-87d2-44a8-ce09-107c3a6107ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Barack Obbama was born in Tennessee. It's still a great country, and I'm very excited to join\n",
            " his family and community at this groundbreaking event.\n",
            "\n",
            "MILFORD: And how\n",
            " do you feel about this idea of online ownership of the keys?\n",
            "\n",
            "ARNOLD:\n",
            " That is so simple. Yes, they are\n",
            "********** -  KeyboardInterrupt - **********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe-6vg4Eq4SC"
      },
      "source": [
        "__More in the homework:__ nucleous sampling, top-k sampling, beam search(not for the faint of heart)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBpLOKlNq4SC"
      },
      "source": [
        "## Evaluating language models: perplexity - not realized\n",
        "\n",
        "Perplexity is a measure of how well does your model approximate true probability distribution behind data. __Smaller perplexity = better model__.\n",
        "\n",
        "To compute perplexity on one sentence, use:\n",
        "$$\n",
        "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
        "$$\n",
        "\n",
        "\n",
        "On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of 1, divided by __total length of all sentences__ in corpora.\n",
        "\n",
        "This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "other words\n",
        "$$\n",
        "\\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N} =\n",
        " \\exp(\\sum_{i=1} ^{N} \\ln(P(w_i)))^{\\frac1N}\n",
        "$$"
      ],
      "metadata": {
        "id": "HK2kAvwSZMo7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzTmFAUjq4SC"
      },
      "outputs": [],
      "source": [
        "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
        "    \"\"\"\n",
        "    :param lines: a list of strings with space-separated tokens\n",
        "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
        "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
        "    \n",
        "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
        "    \n",
        "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
        "    \"\"\"\n",
        "    #<YOUR CODE>\n",
        "    \n",
        "    return None\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting"
      ],
      "metadata": {
        "id": "u-2ceVpzem5j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUyrGEl7q4SD"
      },
      "outputs": [],
      "source": [
        "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
        "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
        "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
        "\n",
        "ppx1 = perplexity(lm1, dummy_lines)\n",
        "ppx3 = perplexity(lm3, dummy_lines)\n",
        "ppx10 = perplexity(lm10, dummy_lines)\n",
        "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
        "\n",
        "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
        "\n",
        "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n",
        "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
        "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
        "    \" Make sure you use min_logprob right\"\n",
        "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5IZtdT5q4SD"
      },
      "source": [
        "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrauD5Hjq4SD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "for n in (1, 2, 3):\n",
        "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Gram Model and Perplexity from Neurotic-Networking"
      ],
      "metadata": {
        "id": "O597FS6lLWhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup & Import"
      ],
      "metadata": {
        "id": "sex5JC1gTasB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install expects"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvtPLqudLnBF",
        "outputId": "6c4fdac2-7e87-405e-92eb-5c084515ef41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting expects\n",
            "  Downloading expects-0.9.0.tar.gz (27 kB)\n",
            "Building wheels for collected packages: expects\n",
            "  Building wheel for expects (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for expects: filename=expects-0.9.0-py3-none-any.whl size=18600 sha256=160b2ac13d1bd7888bd226b0319373b49e912ca6f12d62d544a9e75f613822c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/22/3f/d8c70e8db3a1c384e6d83ccb7853f8f12c75a4d20c1655a85c\n",
            "Successfully built expects\n",
            "Installing collected packages: expects\n",
            "Successfully installed expects-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from functools import partial\n",
        "from pprint import pprint\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "import numpy\n",
        "import pandas\n",
        "\n",
        "# pypi\n",
        "from expects import be_true, expect, have_keys\n",
        "import attr\n"
      ],
      "metadata": {
        "id": "4DnL0Qq2LbL4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functional version:  n_grams, probability/probabilities"
      ],
      "metadata": {
        "id": "IANeSaZiTe3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Count N-Grams\n",
        "Next, you will implement a function that computes the counts of n-grams for an arbitrary number .\n",
        "\n",
        "When computing the counts for n-grams, prepare the sentence beforehand by prepending n-1 starting markers \"<s\\>\" to indicate the beginning of the sentence.\n",
        "\n",
        "- For example, in the bi-gram model (N=2), a sequence with two start tokens \"<s\\><s\\>\" should predict the first word of a sentence.\n",
        "So, if the sentence is \"I like food\", modify it to be \"<s\\><s\\> I like food\".\n",
        "- Also prepare the sentence for counting by appending an end token \"<e\\>\" so that the model can predict when to finish a sentence.\n",
        "\n",
        "Technical note: In this implementation, you will store the counts as a dictionary.\n",
        "\n",
        "- The key of each key-value pair in the dictionary is a tuple of n words (and not a list)\n",
        "- The value in the key-value pair is the number of occurrences.\n",
        "- The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created). A tuple is \"immutable\", so it cannot be altered after it is first created. This makes a tuple suitable as a data type for the key in a dictionary.\n",
        "\n"
      ],
      "metadata": {
        "id": "4ULhFU8P0eSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Hints\n",
        "\n",
        "- To prepend or append, you can create lists and concatenate them using the + operator\n",
        "- To create a list of a repeated value, you can follow this syntax: ['a'] * 3 to get ['a','a','a']\n",
        "- To set the range for index 'i', think of this example: An n-gram where n=2 (bigram), and the sentence is length N=5 (including two start tokens and one end token). So the index positions are [0,1,2,3,4]. The largest index 'i' where a bigram can start is at position i=3, because the word tokens at position 3 and 4 will form the bigram.\n",
        "- Remember that the range() function excludes the value that is used for the maximum of the range. range(3) produces (0,1,2) but excludes 3."
      ],
      "metadata": {
        "id": "YoiFyzRO0f_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### count_n_grams"
      ],
      "metadata": {
        "id": "Hpo8LxxdVqUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "### GRADED FUNCTION: count_n_grams ###\n",
        "def count_n_grams(data: list, n: int, start_token: str='<s>', end_token: str='<e>') -> dict:\n",
        "    \"\"\"\n",
        "    Count all n-grams in the data\n",
        "\n",
        "    Args:\n",
        "       data: List of lists of words\n",
        "       n: number of words in a sequence\n",
        "\n",
        "    Returns:\n",
        "       A dictionary that maps a tuple of n-words to its frequency\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize dictionary of n-grams and their counts\n",
        "    n_grams = {}\n",
        "\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "    # Go through each sentence in the data\n",
        "    for sentence in data: # complete this line\n",
        "\n",
        "        # prepend start token n times, and  append <e> one time\n",
        "        sentence = [start_token] * n + sentence + [end_token]\n",
        "\n",
        "        # convert list to tuple\n",
        "        # So that the sequence of words can be used as\n",
        "        # a key in the dictionary\n",
        "        sentence = tuple(sentence)\n",
        "\n",
        "        # Use 'i' to indicate the start of the n-gram\n",
        "        # from index 0\n",
        "        # to the last index where the end of the n-gram\n",
        "        # is within the sentence.\n",
        "\n",
        "        for i in range(0, len(sentence) - (n - 1)): # complete this line\n",
        "\n",
        "            # Get the n-gram from i to i+n\n",
        "            n_gram = sentence[i: i + n]\n",
        "\n",
        "            # check if the n-gram is in the dictionary\n",
        "            if n_gram in n_grams: # complete this line\n",
        "\n",
        "                # Increment the count for this n-gram\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                # Initialize this n-gram count to 1\n",
        "                n_grams[n_gram] = 1\n",
        "\n",
        "            ### END CODE HERE ###\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "y-3WwXGL082b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Test It"
      ],
      "metadata": {
        "id": "vMqtDQTi1BXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **** Set Up ****\n",
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "\n",
        "# **** Unigram ****\n",
        "print(\"Uni-gram:\")\n",
        "expected = {('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
        "actual = count_n_grams(sentences, 1)\n",
        "print(actual)\n",
        "expect(actual).to(have_keys(expected))\n",
        "assert actual, expected\n",
        "\n",
        "# **** Bi-Gram ****\n",
        "print(\"Bi-gram:\")\n",
        "expected = {('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n",
        "actual = count_n_grams(sentences, 2)\n",
        "print(actual)\n",
        "expect(actual).to(have_keys(expected))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R17C-vza1EJu",
        "outputId": "1158466b-8cb8-48c0-c72f-649ddb770a1e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uni-gram:\n",
            "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
            "Bi-gram:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####estimate_probability & K-Smoothing"
      ],
      "metadata": {
        "id": "NRWWJ5IKxyHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# modified version\n",
        "def estimate_probability(word: str,\n",
        "                         previous_n_gram: tuple, \n",
        "                         n_gram_counts: dict,\n",
        "                         n_plus1_gram_counts: dict,\n",
        "                         vocabulary_size: int,\n",
        "                         k: float=1.0) -> float:\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
        "\n",
        "    Args:\n",
        "       word: next word\n",
        "       previous_n_gram: A tuple of words of length n\n",
        "       n_gram_counts: Dictionary of counts of n-grams\n",
        "       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       vocabulary_size: number of words in the vocabulary\n",
        "       k: positive constant, smoothing parameter\n",
        "\n",
        "    Returns:\n",
        "       A probability\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(previous_n_gram,tuple):\n",
        "        raise Exception('previous_n_gram must be a tuple of words of length n')\n",
        "    \n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "    if previous_n_gram_count==0:\n",
        "        denominator = previous_n_gram_count + k * vocabulary_size\n",
        "    else: \n",
        "        denominator = previous_n_gram_count\n",
        "\n",
        "    n_plus1_gram = previous_n_gram + (word,) # orihinal\n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
        "    if n_plus1_gram_count==0:\n",
        "        numerator = n_plus1_gram_count + k\n",
        "    else:\n",
        "        numerator = n_plus1_gram_count\n",
        "\n",
        "    # Calculate the probability as the numerator divided by denominator\n",
        "    probability = numerator/denominator\n",
        "\n",
        "    return probability"
      ],
      "metadata": {
        "id": "4vxedjXk8YWG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original vrsion\n",
        "def estimate_probability(word: str,\n",
        "                         previous_n_gram: tuple, \n",
        "                         n_gram_counts: dict,\n",
        "                         n_plus1_gram_counts: dict,\n",
        "                         vocabulary_size: int,\n",
        "                         k: float=1.0) -> float:\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
        "\n",
        "    Args:\n",
        "       word: next word\n",
        "       previous_n_gram: A sequence of words of length n\n",
        "       n_gram_counts: Dictionary of counts of n-grams\n",
        "       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       vocabulary_size: number of words in the vocabulary\n",
        "       k: positive constant, smoothing parameter\n",
        "\n",
        "    Returns:\n",
        "       A probability\n",
        "    \"\"\"\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "\n",
        "    n_plus1_gram = previous_n_gram + (word,)  \n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)       \n",
        "    return (n_plus1_gram_count + k)/(previous_n_gram_count + k * vocabulary_size)"
      ],
      "metadata": {
        "id": "ya__f-lpvSLu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test Code"
      ],
      "metadata": {
        "id": "MhRf_asS11eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat'],\n",
        "             ['and', 'he', 'and', 'she', 'and', 'you']] # my addition\n",
        "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]))\n",
        "print(\"num unique_words:\", len(unique_words))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "print(unigram_counts)\n",
        "print(\"unigram_counts[('a',)]=\", unigram_counts[('a',)])\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "print(bigram_counts)\n",
        "print(\"bigram_counts['a', 'cat']=\", bigram_counts['a', 'cat'])\n",
        "actual = estimate_probability(\"cat\", (\"a\",), unigram_counts, bigram_counts, len(unique_words), k=1)\n",
        "expected = 0.3333 # original\n",
        "expected = 1.0 # my correction\n",
        "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {actual:.4f}\")\n",
        "expect(math.isclose(actual, expected, abs_tol=1e-4)).to(be_true)\n",
        "print()\n",
        "print(\"unigram_counts[('and',)]=\", unigram_counts[('and',)])\n",
        "print(\"bigram_counts['and', 'he']=\", bigram_counts['and', 'he'])\n",
        "prob = estimate_probability(\"he\", (\"and\",), unigram_counts, bigram_counts, len(unique_words), k=1)\n",
        "print(f\"The estimated probability of word 'he' given the previous n-gram 'and' is: {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHz_Dn1B17km",
        "outputId": "9bf41533-5541-474a-a978-db1764798b87"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num unique_words: 11\n",
            "{('<s>',): 3, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 3, ('this',): 1, ('dog',): 1, ('is',): 1, ('and',): 3, ('he',): 1, ('she',): 1, ('you',): 1}\n",
            "unigram_counts[('a',)]= 2\n",
            "{('<s>', '<s>'): 3, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1, ('<s>', 'and'): 1, ('and', 'he'): 1, ('he', 'and'): 1, ('and', 'she'): 1, ('she', 'and'): 1, ('and', 'you'): 1, ('you', '<e>'): 1}\n",
            "bigram_counts['a', 'cat']= 2\n",
            "The estimated probability of word 'cat' given the previous n-gram 'a' is: 1.0000\n",
            "\n",
            "unigram_counts[('and',)]= 3\n",
            "bigram_counts['and', 'he']= 1\n",
            "The estimated probability of word 'he' given the previous n-gram 'and' is: 0.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### estimate_probabilities (one_sentence)"
      ],
      "metadata": {
        "id": "aWTypj-RXLjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
        "\n",
        "    Args:\n",
        "       previous_n_gram: A sequence of words of length n\n",
        "       n_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       vocabulary: List of words\n",
        "       k: positive constant, smoothing parameter\n",
        "\n",
        "    Returns:\n",
        "       A dictionary mapping from next words to the probability.\n",
        "    \"\"\"\n",
        "\n",
        "    # convert list to tuple to use it as a dictionary key\n",
        "    previous_n_gram = tuple(previous_n_gram) # original\n",
        "    #previous_n_gram = tuple([previous_n_gram])\n",
        "    #print(\"previous_n_gram\", previous_n_gram)\n",
        "\n",
        "    # add <e> <unk> to the vocabulary\n",
        "    # <s> is not needed since it should not appear as the next word\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"] # original\n",
        "    #vocabulary = vocabulary + [\"<e>\", \"<s>\", \"<unk>\"]\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        #probability = estimate_probability(word, previous_n_gram, \n",
        "        probability = estimate_probability(word, previous_n_gram, \n",
        "                                           n_gram_counts, n_plus1_gram_counts, \n",
        "                                           vocabulary_size, k=k)\n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ],
      "metadata": {
        "id": "WUzDH7YH2Jk1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Test It"
      ],
      "metadata": {
        "id": "7g-3jkN12MZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat'],\n",
        "             ['and', 'he', 'and', 'she', 'and', 'you', 'and', 'i', 'a', 'table', 'a', 'chear'] # my addition\n",
        "             ]\n",
        "#unique_words = list(set(sentences[0] + sentences[1]))\n",
        "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]))\n",
        "print(\"num unique_words:\", len(unique_words), unique_words)\n",
        "print(\"vocabulary\",  len(unique_words + [\"<s>\", \"<e>\", \"<unk>\"]), unique_words + [\"<s>\", \"<e>\", \"<unk>\"])\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "print(\"unigram_counts\", unigram_counts)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "pprint(\"bigram_counts:\") #,bigram_counts)\n",
        "print(bigram_counts)\n",
        "print(\"unigram_counts[('a',)]=\", unigram_counts[('a',)])\n",
        "\n",
        "actual = estimate_probabilities((\"a\",), unigram_counts, bigram_counts, unique_words, k=1)\n",
        "expected =  {'cat': 0.2727272727272727,\n",
        "             'i': 0.09090909090909091,\n",
        "             'this': 0.09090909090909091,\n",
        "             'a': 0.09090909090909091,\n",
        "             'is': 0.09090909090909091,\n",
        "             'like': 0.09090909090909091,\n",
        "             'dog': 0.09090909090909091,\n",
        "             '<e>': 0.09090909090909091,\n",
        "             '<unk>': 0.09090909090909091}\n",
        "#expect(actual).to(have_keys(**expected))\n",
        "pprint(actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ__04WV2N2v",
        "outputId": "6c007cab-3eeb-448f-cc62-aae847919d05"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num unique_words: 13 ['and', 'table', 'cat', 'this', 'she', 'i', 'dog', 'he', 'chear', 'you', 'like', 'a', 'is']\n",
            "vocabulary 16 ['and', 'table', 'cat', 'this', 'she', 'i', 'dog', 'he', 'chear', 'you', 'like', 'a', 'is', '<s>', '<e>', '<unk>']\n",
            "unigram_counts {('<s>',): 3, ('i',): 2, ('like',): 2, ('a',): 4, ('cat',): 2, ('<e>',): 3, ('this',): 1, ('dog',): 1, ('is',): 1, ('and',): 4, ('he',): 1, ('she',): 1, ('you',): 1, ('table',): 1, ('chear',): 1}\n",
            "'bigram_counts:'\n",
            "{('<s>', '<s>'): 3, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1, ('<s>', 'and'): 1, ('and', 'he'): 1, ('he', 'and'): 1, ('and', 'she'): 1, ('she', 'and'): 1, ('and', 'you'): 1, ('you', 'and'): 1, ('and', 'i'): 1, ('i', 'a'): 1, ('a', 'table'): 1, ('table', 'a'): 1, ('a', 'chear'): 1, ('chear', '<e>'): 1}\n",
            "unigram_counts[('a',)]= 4\n",
            "{'<e>': 0.25,\n",
            " '<unk>': 0.25,\n",
            " 'a': 0.25,\n",
            " 'and': 0.25,\n",
            " 'cat': 0.5,\n",
            " 'chear': 0.25,\n",
            " 'dog': 0.25,\n",
            " 'he': 0.25,\n",
            " 'i': 0.25,\n",
            " 'is': 0.25,\n",
            " 'like': 0.25,\n",
            " 'she': 0.25,\n",
            " 'table': 0.25,\n",
            " 'this': 0.25,\n",
            " 'you': 0.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat', 'like', 'a', 'smal', 'dog'],\n",
        "             ['and', 'he', 'and', 'she', 'and', 'you', 'and', 'i', 'a', 'table', 'a', 'chea'] # my addition\n",
        "             ]\n",
        "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]))\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "print(\"vocabulary\",  len(unique_words + [\"<s>\", \"<e>\", \"<unk>\"]), unique_words + [\"<s>\", \"<e>\", \"<unk>\"])\n",
        "print(\"bigram_counts\", bigram_counts)\n",
        "print(\"bigram_counts[('<s>', '<s>')]=\", bigram_counts[(\"<s>\", \"<s>\")])\n",
        "print(\"trigram_counts\", trigram_counts)\n",
        "#actual = estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1) # original\n",
        "\n",
        "actual = estimate_probabilities((\"<s>\", \"<s>\"), bigram_counts, trigram_counts, unique_words, k=1)\n",
        "\n",
        "expected =  {'cat': 0.09090909090909091,\n",
        "             'i': 0.18181818181818182,\n",
        "             'this': 0.18181818181818182,\n",
        "             'a': 0.09090909090909091,\n",
        "             'is': 0.09090909090909091,\n",
        "             'like': 0.09090909090909091,\n",
        "             'dog': 0.09090909090909091,\n",
        "             '<e>': 0.09090909090909091,\n",
        "             '<unk>': 0.09090909090909091}\n",
        "#expect(actual).to(have_keys(**expected))\n",
        "pprint(actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPUJOuo_2VcB",
        "outputId": "8b73f622-6b1c-435b-b0ec-f7fa26b1b7c0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary 17 ['and', 'table', 'cat', 'this', 'smal', 'she', 'i', 'dog', 'he', 'you', 'like', 'chea', 'a', 'is', '<s>', '<e>', '<unk>']\n",
            "bigram_counts {('<s>', '<s>'): 3, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 3, ('a', 'cat'): 2, ('cat', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1, ('cat', 'like'): 1, ('a', 'smal'): 1, ('smal', 'dog'): 1, ('dog', '<e>'): 1, ('<s>', 'and'): 1, ('and', 'he'): 1, ('he', 'and'): 1, ('and', 'she'): 1, ('she', 'and'): 1, ('and', 'you'): 1, ('you', 'and'): 1, ('and', 'i'): 1, ('i', 'a'): 1, ('a', 'table'): 1, ('table', 'a'): 1, ('a', 'chea'): 1, ('chea', '<e>'): 1}\n",
            "bigram_counts[('<s>', '<s>')]= 3\n",
            "trigram_counts {('<s>', '<s>', '<s>'): 3, ('<s>', '<s>', 'i'): 1, ('<s>', 'i', 'like'): 1, ('i', 'like', 'a'): 1, ('like', 'a', 'cat'): 2, ('a', 'cat', '<e>'): 1, ('<s>', '<s>', 'this'): 1, ('<s>', 'this', 'dog'): 1, ('this', 'dog', 'is'): 1, ('dog', 'is', 'like'): 1, ('is', 'like', 'a'): 1, ('a', 'cat', 'like'): 1, ('cat', 'like', 'a'): 1, ('like', 'a', 'smal'): 1, ('a', 'smal', 'dog'): 1, ('smal', 'dog', '<e>'): 1, ('<s>', '<s>', 'and'): 1, ('<s>', 'and', 'he'): 1, ('and', 'he', 'and'): 1, ('he', 'and', 'she'): 1, ('and', 'she', 'and'): 1, ('she', 'and', 'you'): 1, ('and', 'you', 'and'): 1, ('you', 'and', 'i'): 1, ('and', 'i', 'a'): 1, ('i', 'a', 'table'): 1, ('a', 'table', 'a'): 1, ('table', 'a', 'chea'): 1, ('a', 'chea', '<e>'): 1}\n",
            "{'<e>': 0.3333333333333333,\n",
            " '<unk>': 0.3333333333333333,\n",
            " 'a': 0.3333333333333333,\n",
            " 'and': 0.3333333333333333,\n",
            " 'cat': 0.3333333333333333,\n",
            " 'chea': 0.3333333333333333,\n",
            " 'dog': 0.3333333333333333,\n",
            " 'he': 0.3333333333333333,\n",
            " 'i': 0.3333333333333333,\n",
            " 'is': 0.3333333333333333,\n",
            " 'like': 0.3333333333333333,\n",
            " 'she': 0.3333333333333333,\n",
            " 'smal': 0.3333333333333333,\n",
            " 'table': 0.3333333333333333,\n",
            " 'this': 0.3333333333333333,\n",
            " 'you': 0.3333333333333333}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat', 'and', 'i', 'like', 'a', 'smal', 'dog'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat', 'like', 'a', 'smal', 'dog'],\n",
        "             ['and', 'he', 'and', 'she', 'and', 'you', 'and', 'i', 'seat', 'to', 'a', 'table', 'a', 'chea'] # my addition\n",
        "             ]\n",
        "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]))             \n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "print(\"bigram_counts\", bigram_counts)\n",
        "print(\"bigram_counts['like', 'a']=\", bigram_counts['like', 'a'])\n",
        "print(\"trigram_counts\", trigram_counts)\n",
        "#print(\"trigram_counts[('a',)]=\", unigram_counts[('a',)])\n",
        "\n",
        "\n",
        "actual = estimate_probabilities(('like', 'a'), bigram_counts, trigram_counts, unique_words, k=1)\n",
        "pprint(actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYD03DuG5Rh2",
        "outputId": "c86e0e30-3ee5-44fc-bdc2-a513793121bd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram_counts {('<s>', '<s>'): 3, ('<s>', 'i'): 1, ('i', 'like'): 2, ('like', 'a'): 4, ('a', 'cat'): 2, ('cat', 'and'): 1, ('and', 'i'): 2, ('a', 'smal'): 2, ('smal', 'dog'): 2, ('dog', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1, ('cat', 'like'): 1, ('<s>', 'and'): 1, ('and', 'he'): 1, ('he', 'and'): 1, ('and', 'she'): 1, ('she', 'and'): 1, ('and', 'you'): 1, ('you', 'and'): 1, ('i', 'seat'): 1, ('seat', 'to'): 1, ('to', 'a'): 1, ('a', 'table'): 1, ('table', 'a'): 1, ('a', 'chea'): 1, ('chea', '<e>'): 1}\n",
            "bigram_counts['like', 'a']= 4\n",
            "trigram_counts {('<s>', '<s>', '<s>'): 3, ('<s>', '<s>', 'i'): 1, ('<s>', 'i', 'like'): 1, ('i', 'like', 'a'): 2, ('like', 'a', 'cat'): 2, ('a', 'cat', 'and'): 1, ('cat', 'and', 'i'): 1, ('and', 'i', 'like'): 1, ('like', 'a', 'smal'): 2, ('a', 'smal', 'dog'): 2, ('smal', 'dog', '<e>'): 2, ('<s>', '<s>', 'this'): 1, ('<s>', 'this', 'dog'): 1, ('this', 'dog', 'is'): 1, ('dog', 'is', 'like'): 1, ('is', 'like', 'a'): 1, ('a', 'cat', 'like'): 1, ('cat', 'like', 'a'): 1, ('<s>', '<s>', 'and'): 1, ('<s>', 'and', 'he'): 1, ('and', 'he', 'and'): 1, ('he', 'and', 'she'): 1, ('and', 'she', 'and'): 1, ('she', 'and', 'you'): 1, ('and', 'you', 'and'): 1, ('you', 'and', 'i'): 1, ('and', 'i', 'seat'): 1, ('i', 'seat', 'to'): 1, ('seat', 'to', 'a'): 1, ('to', 'a', 'table'): 1, ('a', 'table', 'a'): 1, ('table', 'a', 'chea'): 1, ('a', 'chea', '<e>'): 1}\n",
            "{'<e>': 0.25,\n",
            " '<unk>': 0.25,\n",
            " 'a': 0.25,\n",
            " 'and': 0.25,\n",
            " 'cat': 0.5,\n",
            " 'chea': 0.25,\n",
            " 'dog': 0.25,\n",
            " 'he': 0.25,\n",
            " 'i': 0.25,\n",
            " 'is': 0.25,\n",
            " 'like': 0.25,\n",
            " 'seat': 0.25,\n",
            " 'she': 0.25,\n",
            " 'smal': 0.5,\n",
            " 'table': 0.25,\n",
            " 'this': 0.25,\n",
            " 'to': 0.25,\n",
            " 'you': 0.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class-Based version"
      ],
      "metadata": {
        "id": "RoqCYEwPU1gA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The N-Gram\n"
      ],
      "metadata": {
        "id": "eYlJj01c3sdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@attr.s(auto_attribs=True)\n",
        "class NGrams:\n",
        "    \"\"\"The N-Gram Language Model\n",
        "\n",
        "    Args:\n",
        "     data: the training data\n",
        "     n: the size of the n-grams\n",
        "     start_token: string to represent the start of a sentence\n",
        "     end_token: string to represent the end of a sentence\n",
        "    \"\"\"\n",
        "    data: list\n",
        "    n: int\n",
        "    start_token: str=\"<s>\"\n",
        "    end_token: str=\"<e>\"\n",
        "    _start_tokens: list=None\n",
        "    _end_tokens: list=None\n",
        "    _sentences: list=None\n",
        "    _n_grams: list=None\n",
        "    _counts: dict=None\n",
        "\n",
        "    @property\n",
        "    def start_tokens(self) -> list:\n",
        "        \"\"\"List of 'n' start tokens\"\"\"\n",
        "        if self._start_tokens is None:\n",
        "            self._start_tokens = [self.start_token] * self.n\n",
        "        return self._start_tokens\n",
        "\n",
        "    @property\n",
        "    def end_tokens(self) -> list:\n",
        "        \"\"\"List of 1 end-tokens\"\"\"\n",
        "        if self._end_tokens is None:\n",
        "            self._end_tokens = [self.end_token]\n",
        "        return self._end_tokens\n",
        "\n",
        "    @property\n",
        "    def sentences(self) -> list:\n",
        "        \"\"\"The data augmented with tags and converted to tuples\"\"\"\n",
        "        if self._sentences is None:\n",
        "            self._sentences = [tuple(self.start_tokens + sentence + self.end_tokens)\n",
        "                              for sentence in self.data]\n",
        "        return self._sentences\n",
        "\n",
        "    @property\n",
        "    def n_grams(self) -> list:\n",
        "        \"\"\"The n-grams from the data\n",
        "\n",
        "        Warning:\n",
        "        this flattens the n-grams so there isn't any sentence structure\n",
        "        \"\"\"\n",
        "        if self._n_grams is None:\n",
        "            self._n_grams = chain.from_iterable([\n",
        "                [sentence[cut: cut + self.n] for cut in range(0, len(sentence) - (self.n - 1))]\n",
        "                for sentence in self.sentences\n",
        "            ])\n",
        "        return self._n_grams\n",
        "\n",
        "    @property\n",
        "    def counts(self) -> Counter:\n",
        "        \"\"\"A count of all n-grams in the data\n",
        "\n",
        "        Returns:\n",
        "          A dictionary that maps a tuple of n-words to its frequency\n",
        "        \"\"\"\n",
        "        if self._counts is None:        \n",
        "            self._counts = Counter(self.n_grams)\n",
        "        return self._counts\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "ZpzwBkjO3ut_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Test It Out"
      ],
      "metadata": {
        "id": "K2qDLxgA5EFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from neurotic.nlp.autocomplete import NGrams\n",
        "\n",
        "# **** Set Up ****\n",
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "\n",
        "# **** Unigram ****\n",
        "\n",
        "expected = {('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2,\n",
        "            ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
        "uni_grams = NGrams(sentences, 1)\n",
        "actual = uni_grams.counts\n",
        "expect(actual).to(have_keys(expected))\n",
        "\n",
        "# **** Bi-Gram ****\n",
        "\n",
        "expected = {('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1,\n",
        "            ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2,\n",
        "            ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1,\n",
        "            ('is', 'like'): 1}\n",
        "bi_grams = NGrams(sentences, 2)\n",
        "actual = bi_grams.counts\n",
        "expect(actual).to(have_keys(expected))"
      ],
      "metadata": {
        "id": "7d2IYk3X5FVR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####N-Gram Probability\n"
      ],
      "metadata": {
        "id": "9w7XUjmO4Tk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@attr.s(auto_attribs=True)\n",
        "class NGramProbability:\n",
        "    \"\"\"Probability model for n-grams\n",
        "\n",
        "    Args:\n",
        "     data: the source for the n-grams\n",
        "     n: the size of the n-grams\n",
        "     k: smoothing factor\n",
        "     augment_vocabulary: hack because the two probability functions use different vocabularies\n",
        "    \"\"\"\n",
        "    data: list\n",
        "    n: int\n",
        "    k: float=1.0\n",
        "    augment_vocabulary: bool=True\n",
        "    _n_grams: NGrams=None\n",
        "    _n_plus_one: NGrams=None\n",
        "    _vocabulary: set=None\n",
        "    _vocabulary_size: int=None\n",
        "    _probabilities: dict=None\n",
        "    \n",
        "    @property\n",
        "    def n_grams(self) -> NGrams:\n",
        "        if self._n_grams is None:\n",
        "            self._n_grams = NGrams(data=self.data, n=self.n)\n",
        "        return self._n_grams\n",
        "    \n",
        "    @property\n",
        "    def n_plus_one(self) -> NGrams:\n",
        "        \"\"\"N+1 Grams\"\"\"\n",
        "        if self._n_plus_one is None:\n",
        "            self._n_plus_one = NGrams(data=self.data, n=self.n + 1)\n",
        "        return self._n_plus_one\n",
        "\n",
        "    @property\n",
        "    def vocabulary(self) -> set:\n",
        "        \"\"\"Unique words in the dictionary\"\"\"\n",
        "        if self._vocabulary is None:\n",
        "            data = list(chain.from_iterable(self.data)).copy()\n",
        "            if self.augment_vocabulary:\n",
        "                data += [\"<e>\", \"<unk>\"]\n",
        "            self._vocabulary = set(data)\n",
        "        return self._vocabulary\n",
        "\n",
        "    @property\n",
        "    def vocabulary_size(self) -> int:\n",
        "        \"\"\"Number of unique tokens in the data\"\"\"\n",
        "        if self._vocabulary_size is None:\n",
        "            self._vocabulary_size = len(self.vocabulary)\n",
        "        return self._vocabulary_size\n",
        "\n",
        "    def probability(self, word: str, previous_n_gram: tuple) -> float:\n",
        "        \"\"\"Calculates the probability of the word given the previous n-gram\"\"\"\n",
        "        # just in case it's a list\n",
        "        previous_n_gram = tuple(previous_n_gram)\n",
        "        previous_n_gram_count = self.n_grams.counts.get(previous_n_gram, 0)\n",
        "        denominator = previous_n_gram_count + self.k * self.vocabulary_size\n",
        "\n",
        "        n_plus1_gram = previous_n_gram + (word,)\n",
        "        n_plus1_gram_count = self.n_plus_one.counts.get(n_plus1_gram, 0)\n",
        "        numerator = n_plus1_gram_count + self.k\n",
        "        return numerator/denominator\n",
        "\n",
        "    def probabilities(self, previous_n_gram: tuple) -> dict:\n",
        "        \"\"\"Finds the probability of each word in the vocabulary\n",
        "\n",
        "        Args:\n",
        "        previous_n_gram: the preceding tuple to calculate probabilities\n",
        "\n",
        "        Returns:\n",
        "        word:<probability word follows previous n-gram> for the vocabulary\n",
        "        \"\"\"\n",
        "        previous_n_gram = tuple(previous_n_gram)\n",
        "        return {word: self.probability(word=word, previous_n_gram=previous_n_gram)\n",
        "                for word in self.vocabulary}"
      ],
      "metadata": {
        "id": "eg6aTYyj4V6D"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating the Perplexity"
      ],
      "metadata": {
        "id": "bpu4a3Wt8cxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functional version calculate_perplexity"
      ],
      "metadata": {
        "id": "uSFPS2wUWVar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: calculate_perplexity\n",
        "def calculate_perplexity(sentence: list,\n",
        "                         n_gram_counts: dict,\n",
        "                         n_plus1_gram_counts: dict,\n",
        "                         vocabulary_size: int,\n",
        "                         k: float=1.0):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a list of sentences\n",
        "\n",
        "    Args:\n",
        "       sentence: List of strings\n",
        "       n_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       vocabulary_size: number of unique words in the vocabulary\n",
        "       k: Positive smoothing constant\n",
        "\n",
        "    Returns:\n",
        "       Perplexity score\n",
        "    \"\"\"\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "\n",
        "    # prepend <s> and append <e>\n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "\n",
        "    # Cast the sentence from a list to a tuple\n",
        "    sentence = tuple(sentence)\n",
        "\n",
        "    # length of sentence (after adding <s> and <e> tokens)\n",
        "    N = len(sentence)\n",
        "\n",
        "    # The variable p will hold the product\n",
        "    # that is calculated inside the n-root\n",
        "    # Update this in the code below\n",
        "    product_pi = 1.0\n",
        "\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "    # Index t ranges from n to N - 1, inclusive on both ends\n",
        "    for t in range(n, N): # complete this line\n",
        "\n",
        "        # get the n-gram preceding the word at position t\n",
        "        n_gram = sentence[t - n: t]\n",
        "\n",
        "        # get the word at position t\n",
        "        word = sentence[t]\n",
        "\n",
        "        # Estimate the probability of the word given the n-gram\n",
        "        # using the n-gram counts, n-plus1-gram counts,\n",
        "        # vocabulary size, and smoothing constant\n",
        "        probability = estimate_probability(\n",
        "            word=word, previous_n_gram=n_gram,\n",
        "            vocabulary_size=vocabulary_size,\n",
        "            n_gram_counts=n_gram_counts,\n",
        "            n_plus1_gram_counts=n_plus1_gram_counts, k=k)\n",
        "\n",
        "        # Update the product of the probabilities\n",
        "        # This 'product_pi' is a cumulative product \n",
        "        # of the (1/P) factors that are calculated in the loop\n",
        "        product_pi *= 1/probability\n",
        "\n",
        "    # Take the Nth root of the product\n",
        "    perplexity = product_pi**(1/N)\n",
        "\n",
        "    ### END CODE HERE ### \n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "EEWHb8vq8eVg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test It"
      ],
      "metadata": {
        "id": "CQ_bg56Z8kPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = NGrams(sentences, 1).counts\n",
        "bigram_counts = NGrams(sentences, 2).counts\n",
        "\n",
        "\n",
        "perplexity_train1 = calculate_perplexity(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words), k=1.0)\n",
        "expected = 2.8040\n",
        "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
        "\n",
        "#expect(math.isclose(perplexity_train1, expected, abs_tol=1e-4)).to(be_true)\n",
        "test_sentence = ['i', 'like', 'a', 'dog']\n",
        "perplexity_test = calculate_perplexity(test_sentence,\n",
        "                                       unigram_counts, bigram_counts,\n",
        "                                       len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")\n",
        "expected = 3.9654\n",
        "#expect(math.isclose(perplexity_test, expected, abs_tol=1e-4)).to(be_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au6dGSop8mIc",
        "outputId": "b3215f61-4e81-41d7-c26d-c8b7557d7faf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for first train sample: 1.1225\n",
            "Perplexity for test sample: 1.2599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class-based Perplexity"
      ],
      "metadata": {
        "id": "a8IJshmRM4qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@attr.s(auto_attribs=True)\n",
        "class Perplexity:\n",
        "    \"\"\"Calculate perplexity\n",
        "\n",
        "    Args:\n",
        "     data: the tokenized training input\n",
        "     n: the size of the n-grams\n",
        "     augment_vocabulary: whether to augment the vocabulary for toy examples\n",
        "    \"\"\"\n",
        "    data: list\n",
        "    n: int\n",
        "    augment_vocabulary: bool=False\n",
        "    _probabilifier: NGramProbability=None\n",
        "\n",
        "    @property\n",
        "    def probabilifier(self) -> NGramProbability:\n",
        "        \"\"\"Probability Calculator\"\"\"\n",
        "        if self._probabilifier is None:\n",
        "            self._probabilifier = NGramProbability(\n",
        "                self.data, self.n,\n",
        "                augment_vocabulary=self.augment_vocabulary)\n",
        "        return self._probabilifier\n",
        "\n",
        "    def perplexity(self, sentence: list) -> float:\n",
        "        \"\"\"Calculates the perplexity for the sentence\"\"\"\n",
        "        sentence = tuple([\"<s>\"] * self.n + sentence + [\"<e>\"])\n",
        "        N = len(sentence)\n",
        "\n",
        "        n_grams = (sentence[position - self.n: position]\n",
        "                   for position in range(self.n, N))\n",
        "        words = (sentence[position]\n",
        "                 for position in range(self.n, N))\n",
        "        words_n_grams = zip(words, n_grams)\n",
        "        probabilities = (self.probabilifier.probability(word, n_gram)\n",
        "                         for word, n_gram in words_n_grams)\n",
        "        #product = math.prod((1/probability for probability in probabilities))\n",
        "        # math.prod in Python >=3.8 # https://stackoverflow.com/questions/69739594/math-prod-is-not-working-in-google-colab-notebook\n",
        "        from functools import reduce\n",
        "        import operator\n",
        "        product = reduce(operator.mul, (1/probability for probability in probabilities))  \n",
        "        return product**(1/N)"
      ],
      "metadata": {
        "id": "fgYca-7L8u9J"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test It"
      ],
      "metadata": {
        "id": "qTEgoqQQNfkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "\n",
        "model = Perplexity(sentences, 1, augment_vocabulary=False)\n",
        "\n",
        "actual = model.perplexity(sentences[0])\n",
        "\n",
        "expected = 2.8040\n",
        "print(f\"Perplexity for first train sample: {actual:.4f}\")\n",
        "\n",
        "expect(math.isclose(actual, expected, abs_tol=1e-4)).to(be_true)\n",
        "test_sentence = ['i', 'like', 'a', 'dog']\n",
        "model\n",
        "perplexity_test = model.perplexity(test_sentence)\n",
        "\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")\n",
        "expected = 3.9654\n",
        "expect(math.isclose(perplexity_test, expected, abs_tol=1e-4)).to(be_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7705pIa84tv",
        "outputId": "1fbaead7-3318-469f-8496-20fb1fba95d4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for first train sample: 2.8040\n",
            "Perplexity for test sample: 3.9654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LM Corpus Perplexity - my addition"
      ],
      "metadata": {
        "id": "VijB3tL2Vm3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### calculate_perplexity_corpus v01"
      ],
      "metadata": {
        "id": "yhXjj3oXZJFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# v2 - work\n",
        "def calculate_perplexity_corpus(lines,\n",
        "                                unigram_counts, bigram_counts,\n",
        "                                       unique_words_cnt, k=1.0):\n",
        "    perplexity_total = 1\n",
        "    for i, line in enumerate(lines):\n",
        "        perplexity_line = calculate_perplexity(line,\n",
        "                                          unigram_counts, bigram_counts,\n",
        "                                          unique_words_cnt, k=1.0)\n",
        "        if i<2 or i>len(lines)-3: print(f\"{i}: perplexity_line={perplexity_line:.3f}, perplexity_total={perplexity_total**(1/unique_words_cnt):.3f}\")\n",
        "        perplexity_total *= perplexity_line**len(line)  # (1/(po*p1*...pN) without **(1/len(line)) )\n",
        "    return perplexity_total**(1/unique_words_cnt)"
      ],
      "metadata": {
        "id": "qQwy8IdeY1Ol"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# v1 - test\n",
        "def calculate_perplexity_corpus(lines: list, #(list of lists of words)\n",
        "                         n_gram_counts: dict,\n",
        "                         n_plus1_gram_counts: dict,\n",
        "                         vocabulary_size: int,\n",
        "                         k: float=1.0):\n",
        "  \n",
        "    perplexity_total = 1\n",
        "    for i, line in enumerate(lines):\n",
        "        perplexity_line = calculate_perplexity(line, n_gram_counts, n_plus1_gram_counts,\n",
        "                                          vocabulary_size, k=1.0)\n",
        "        #if i<2: print(perplexity_line)\n",
        "        perplexity_total *= perplexity_line\n",
        "    return perplexity_total"
      ],
      "metadata": {
        "id": "pzMMsRBwYxJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test - short I"
      ],
      "metadata": {
        "id": "o_zWiDcklBZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "print(\"num unique_words:\", len(unique_words), unique_words)\n",
        "print(\"vocabulary\",  len(unique_words + [\"<s>\", \"<e>\", \"<unk>\"]), unique_words + [\"<s>\", \"<e>\", \"<unk>\"])\n",
        "\n",
        "uni_grams = NGrams(sentences, 1)\n",
        "bi_grams = NGrams(sentences, 2)\n",
        "tri_grams = NGrams(sentences, 3)\n",
        "\n",
        "perplexity_total = calculate_perplexity_corpus(sentences,unigram_counts, bigram_counts, len(unique_words), k=1.0)        \n",
        "print(f\"perplexity_total={perplexity_total:.3f}\")"
      ],
      "metadata": {
        "id": "lcEOYH8NZK-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83258a05-1399-4ccb-bc55-cc5906a19f09"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num unique_words: 7 ['cat', 'this', 'i', 'dog', 'like', 'a', 'is']\n",
            "vocabulary 10 ['cat', 'this', 'i', 'dog', 'like', 'a', 'is', '<s>', '<e>', '<unk>']\n",
            "0: perplexity_line=1.122, perplexity_total=1.000\n",
            "1: perplexity_line=1.091, perplexity_total=1.068\n",
            "perplexity_total=1.151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting Perplexity for Ya data"
      ],
      "metadata": {
        "id": "DgOTOoS0NveJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### preprocess Ya data"
      ],
      "metadata": {
        "id": "BV3qw-8zZoWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_ya = data.apply(lambda row: row['title'] + ' ; ' + row['summary'], axis=1).tolist()\n",
        "\n",
        "sorted(sentences_ya, key=len)[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bwEqtLdclFh",
        "outputId": "bb264101-f1b4-4034-8dce-98c3938cc47d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
              " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
              " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_tokenized = [sentence.split() for sentence in sentences_ya]\n",
        "sentences_tokenized_test = sentences_tokenized[:3]\n",
        "print(sentences_tokenized_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQDFu70wgJ5q",
        "outputId": "ebaf29bd-02e1-4e60-9b29-f7b3a36618bb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Dual', 'Recurrent', 'Attention', 'Units', 'for', 'Visual', 'Question', 'Answering', ';', 'We', 'propose', 'an', 'architecture', 'for', 'VQA', 'which', 'utilizes', 'recurrent', 'layers', 'to', 'generate', 'visual', 'and', 'textual', 'attention.', 'The', 'memory', 'characteristic', 'of', 'the', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'a', 'rich', 'joint', 'embedding', 'of', 'visual', 'and', 'textual', 'features', 'and', 'enables', 'the', 'model', 'to', 'reason', 'relations', 'between', 'several', 'parts', 'of', 'the', 'image', 'and', 'question.', 'Our', 'single', 'model', 'outperforms', 'the', 'first', 'place', 'winner', 'on', 'the', 'VQA', '1.0', 'dataset,', 'performs', 'within', 'margin', 'to', 'the', 'current', 'state-of-the-art', 'ensemble', 'model.', 'We', 'also', 'experiment', 'with', 'replacing', 'attention', 'mechanisms', 'in', 'other', 'state-of-the-art', 'models', 'with', 'our', 'implementation', 'and', 'show', 'increased', 'accuracy.', 'In', 'both', 'cases,', 'our', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'in', 'tasks', 'requiring', 'sequential', 'or', 'relational', 'reasoning', 'on', 'the', 'VQA', 'dataset.'], ['Sequential', 'Short-Text', 'Classification', 'with', 'Recurrent', 'and', 'Convolutional', 'Neural', 'Networks', ';', 'Recent', 'approaches', 'based', 'on', 'artificial', 'neural', 'networks', '(ANNs)', 'have', 'shown', 'promising', 'results', 'for', 'short-text', 'classification.', 'However,', 'many', 'short', 'texts', 'occur', 'in', 'sequences', '(e.g.,', 'sentences', 'in', 'a', 'document', 'or', 'utterances', 'in', 'a', 'dialog),', 'and', 'most', 'existing', 'ANN-based', 'systems', 'do', 'not', 'leverage', 'the', 'preceding', 'short', 'texts', 'when', 'classifying', 'a', 'subsequent', 'one.', 'In', 'this', 'work,', 'we', 'present', 'a', 'model', 'based', 'on', 'recurrent', 'neural', 'networks', 'and', 'convolutional', 'neural', 'networks', 'that', 'incorporates', 'the', 'preceding', 'short', 'texts.', 'Our', 'model', 'achieves', 'state-of-the-art', 'results', 'on', 'three', 'different', 'datasets', 'for', 'dialog', 'act', 'prediction.'], ['Multiresolution', 'Recurrent', 'Neural', 'Networks:', 'An', 'Application', 'to', 'Dialogue', 'Response', 'Generation', ';', 'We', 'introduce', 'the', 'multiresolution', 'recurrent', 'neural', 'network,', 'which', 'extends', 'the', 'sequence-to-sequence', 'framework', 'to', 'model', 'natural', 'language', 'generation', 'as', 'two', 'parallel', 'discrete', 'stochastic', 'processes:', 'a', 'sequence', 'of', 'high-level', 'coarse', 'tokens,', 'and', 'a', 'sequence', 'of', 'natural', 'language', 'tokens.', 'There', 'are', 'many', 'ways', 'to', 'estimate', 'or', 'learn', 'the', 'high-level', 'coarse', 'tokens,', 'but', 'we', 'argue', 'that', 'a', 'simple', 'extraction', 'procedure', 'is', 'sufficient', 'to', 'capture', 'a', 'wealth', 'of', 'high-level', 'discourse', 'semantics.', 'Such', 'procedure', 'allows', 'training', 'the', 'multiresolution', 'recurrent', 'neural', 'network', 'by', 'maximizing', 'the', 'exact', 'joint', 'log-likelihood', 'over', 'both', 'sequences.', 'In', 'contrast', 'to', 'the', 'standard', 'log-', 'likelihood', 'objective', 'w.r.t.', 'natural', 'language', 'tokens', '(word', 'perplexity),', 'optimizing', 'the', 'joint', 'log-likelihood', 'biases', 'the', 'model', 'towards', 'modeling', 'high-level', 'abstractions.', 'We', 'apply', 'the', 'proposed', 'model', 'to', 'the', 'task', 'of', 'dialogue', 'response', 'generation', 'in', 'two', 'challenging', 'domains:', 'the', 'Ubuntu', 'technical', 'support', 'domain,', 'and', 'Twitter', 'conversations.', 'On', 'Ubuntu,', 'the', 'model', 'outperforms', 'competing', 'approaches', 'by', 'a', 'substantial', 'margin,', 'achieving', 'state-of-the-art', 'results', 'according', 'to', 'both', 'automatic', 'evaluation', 'metrics', 'and', 'a', 'human', 'evaluation', 'study.', 'On', 'Twitter,', 'the', 'model', 'appears', 'to', 'generate', 'more', 'relevant', 'and', 'on-topic', 'responses', 'according', 'to', 'automatic', 'evaluation', 'metrics.', 'Finally,', 'our', 'experiments', 'demonstrate', 'that', 'the', 'proposed', 'model', 'is', 'more', 'adept', 'at', 'overcoming', 'the', 'sparsity', 'of', 'natural', 'language', 'and', 'is', 'better', 'able', 'to', 'capture', 'long-term', 'structure.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test - short II"
      ],
      "metadata": {
        "id": "GExuZ0D3Zmyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_test = sentences_ya[:3]\n",
        "unique_words_test = list(set(' '.join([sentence for sentence in sentences_test]).split() ))\n",
        "print(\"num unique_words:\", len(unique_words), unique_words_test[:10])\n",
        "print(\"vocabulary\",  len([\"<s>\", \"<e>\", \"<unk>\"] + unique_words_test), [\"<s>\", \"<e>\", \"<unk>\"] + unique_words_test[:10] )\n",
        "\n",
        "unigram_counts = NGrams(sentences_tokenized_test, 1).counts\n",
        "bigram_counts = NGrams(sentences_tokenized_test, 2).counts\n",
        "\n",
        "\n",
        "perplexity_train1 = calculate_perplexity(sentences_tokenized_test[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words_test), k=1.0)\n",
        "\n",
        "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
        "\n",
        "#expect(math.isclose(perplexity_train1, expected, abs_tol=1e-4)).to(be_true)\n",
        "test_sentence = sentences_tokenized_test[1]\n",
        "perplexity_test = calculate_perplexity(test_sentence,\n",
        "                                       unigram_counts, bigram_counts,\n",
        "                                       len(unique_words_test), k=1.0)\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")\n",
        "expected = 3.9654\n",
        "#expect(math.isclose(perplexity_test, expected, abs_tol=1e-4)).to(be_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuNOLytHvId6",
        "outputId": "1b17b702-2acf-40a2-9525-dfbb104f2aa0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num unique_words: 7 ['or', 'most', 'An', 'recurrent', 'introduce', 'which', 'several', 'Short-Text', '1.0', 'models']\n",
            "vocabulary 258 ['<s>', '<e>', '<unk>', 'or', 'most', 'An', 'recurrent', 'introduce', 'which', 'several', 'Short-Text', '1.0', 'models']\n",
            "Perplexity for first train sample: 2.0549\n",
            "Perplexity for test sample: 1.8940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### test full data"
      ],
      "metadata": {
        "id": "eWbWGktiR6Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "unique_words = list(set(' '.join([sentence for sentence in sentences_ya]).split() ))\n",
        "print(\"num unique_words:\", len(unique_words), unique_words[:10])\n",
        "#print(\"vocabulary\",  len([\"<s>\", \"<e>\", \"<unk>\"] + unique_words), [\"<s>\", \"<e>\", \"<unk>\"] + unique_words[:10] )\n",
        "print(\"vocabulary\",  len([\"<s>\", \"<e>\"] + unique_words), [\"<s>\", \"<e>\"] + unique_words[:10] )\n",
        "vocabulary_size = len(unique_words) +2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61841b92-3244-4ae1-f2a8-2ea9720cceb8",
        "id": "omhw6f-glYp3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num unique_words: 205608 ['irregularly', 'real$-$time', 'conferences~\\\\citep{Jain13,Jain13b}.', 'clinically', \"McCallum's\", 'segment;', 'CMCGAN,', 'hit-or-miss', 'Dyer', 'PSO-PS']\n",
            "vocabulary 205610 ['<s>', '<e>', 'irregularly', 'real$-$time', 'conferences~\\\\citep{Jain13,Jain13b}.', 'clinically', \"McCallum's\", 'segment;', 'CMCGAN,', 'hit-or-miss', 'Dyer', 'PSO-PS']\n",
            "CPU times: user 1.24 s, sys: 373 ms, total: 1.61 s\n",
            "Wall time: 1.61 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "uni_grams = NGrams(sentences_tokenized, 1)\n",
        "bi_grams = NGrams(sentences_tokenized, 2)\n",
        "tri_grams = NGrams(sentences_tokenized, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1pnbRosnryX",
        "outputId": "427e3de5-6240-4bdc-80b4-b66ab645602f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11 ¬µs, sys: 1e+03 ns, total: 12 ¬µs\n",
            "Wall time: 15.7 ¬µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(tri_grams.counts)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHNCD6GptyWK",
        "outputId": "7e2bc068-c238-49ea-aec8-3a66a3e04e52"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<s>', '<s>', '<s>'), ('<s>', '<s>', 'Dual'), ('<s>', 'Dual', 'Recurrent'), ('Dual', 'Recurrent', 'Attention'), ('Recurrent', 'Attention', 'Units')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "try:\n",
        "    perplexity_total = calculate_perplexity_corpus(sentences_tokenized, uni_grams.counts, bi_grams.counts, vocabulary_size, k=1.0)        \n",
        "    #perplexity_total = calculate_perplexity_corpus(sentences_tokenized, unigram_counts, bigram_counts, vocabulary_size, k=1.0)\n",
        "    print(f\"perplexity_total={perplexity_total:.3f}\")\n",
        "except KeyboardInterrupt:\n",
        "    print(10*'*', \"KeyboardInterrupt\", 10*'*')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea234736-09eb-4f1d-ed43-3bbfbe719f74",
        "id": "2PED2JZumwyj"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: perplexity_line=114.394, perplexity_total=1.000\n",
            "1: perplexity_line=57.843, perplexity_total=1.003\n",
            "40998: perplexity_line=112.831, perplexity_total=inf\n",
            "40999: perplexity_line=85.841, perplexity_total=inf\n",
            "perplexity_total=inf\n",
            "CPU times: user 6min 1s, sys: 1.65 s, total: 6min 3s\n",
            "Wall time: 6min 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "try:\n",
        "    perplexity_total = calculate_perplexity_corpus(sentences_tokenized, uni_grams.counts, bi_grams.counts, vocabulary_size, k=1.0)        \n",
        "    #perplexity_total = calculate_perplexity_corpus(sentences_tokenized, unigram_counts, bigram_counts, vocabulary_size, k=1.0)\n",
        "    print(f\"perplexity_total={perplexity_total:.3f}\")\n",
        "except KeyboardInterrupt:\n",
        "    print(10*'*', \"KeyboardInterrupt\", 10*'*')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c37679b-319a-4311-f6a7-6c550be45e76",
        "id": "j35ALs9Hhaj4"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: perplexity_line=114.394, perplexity_total=1.000\n",
            "1: perplexity_line=57.843, perplexity_total=1.003\n",
            "********** KeyboardInterrupt **********\n",
            "CPU times: user 16.8 s, sys: 73.9 ms, total: 16.9 s\n",
            "Wall time: 16.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Perplexity Corpus v02"
      ],
      "metadata": {
        "id": "JSQSyVgXebhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### calculate_perplexity v02 = calculate_sentence_probabilities"
      ],
      "metadata": {
        "id": "tZO7yV3WgFrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: calculate_perplexity\n",
        "def calculate_sentence_probabilities_02(sentence: list,\n",
        "                         n_gram_counts: dict,\n",
        "                         n_plus1_gram_counts: dict,\n",
        "                         vocabulary_size: int,\n",
        "                         k: float=1.0):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a list of sentences\n",
        "\n",
        "    Args:\n",
        "       sentence: List of strings\n",
        "       n_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       vocabulary_size: number of unique words in the vocabulary\n",
        "       k: Positive smoothing constant\n",
        "\n",
        "    Returns:\n",
        "       sentence probabilities product\n",
        "    \"\"\"\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "\n",
        "    # prepend <s> and append <e>\n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "\n",
        "    # Cast the sentence from a list to a tuple\n",
        "    sentence = tuple(sentence)\n",
        "\n",
        "    # length of sentence (after adding <s> and <e> tokens)\n",
        "    N = len(sentence)\n",
        "\n",
        "    # The variable p will hold the product\n",
        "    # that is calculated inside the n-root\n",
        "    # Update this in the code below\n",
        "    product_pi = 1.0\n",
        "\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "    # Index t ranges from n to N - 1, inclusive on both ends\n",
        "    for t in range(n, N): # complete this line\n",
        "\n",
        "        # get the n-gram preceding the word at position t\n",
        "        n_gram = sentence[t - n: t]\n",
        "\n",
        "        # get the word at position t\n",
        "        word = sentence[t]\n",
        "\n",
        "        # Estimate the probability of the word given the n-gram\n",
        "        # using the n-gram counts, n-plus1-gram counts,\n",
        "        # vocabulary size, and smoothing constant\n",
        "        probability = estimate_probability(\n",
        "            word=word, previous_n_gram=n_gram,\n",
        "            vocabulary_size=vocabulary_size,\n",
        "            n_gram_counts=n_gram_counts,\n",
        "            n_plus1_gram_counts=n_plus1_gram_counts, k=k)\n",
        "\n",
        "        # Update the product of the probabilities\n",
        "        # This 'product_pi' is a cumulative product \n",
        "        # of the (1/P) factors that are calculated in the loop\n",
        "        product_pi *= probability\n",
        "\n",
        "    # Take the Nth root of the product\n",
        "    #perplexity = product_pi**(1/N)\n",
        "\n",
        "    ### END CODE HERE ### \n",
        "    return product_pi"
      ],
      "metadata": {
        "id": "5oU0tbMegFrh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test It"
      ],
      "metadata": {
        "id": "H67mX34cjy6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words_test = list(set(' '.join([sentence for sentence in sentences_test]).split() ))\n",
        "\n",
        "unigram_counts = NGrams(sentences, 1).counts\n",
        "bigram_counts = NGrams(sentences, 2).counts\n",
        "\n",
        "\n",
        "perplexity_train1 = calculate_sentence_probabilities_02(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words_test), k=1.0)\n",
        "expected = 2.8040\n",
        "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
        "\n",
        "#expect(math.isclose(perplexity_train1, expected, abs_tol=1e-4)).to(be_true)\n",
        "test_sentence = ['i', 'like', 'a', 'dog']\n",
        "perplexity_test = calculate_sentence_probabilities_02(test_sentence,\n",
        "                                       unigram_counts, bigram_counts,\n",
        "                                       len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")\n",
        "expected = 3.9654\n",
        "#expect(math.isclose(perplexity_test, expected, abs_tol=1e-4)).to(be_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a87335-8b99-4d11-b12c-f32947907762",
        "id": "Rg6J3A9Hjy6V"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for first train sample: 0.5000\n",
            "Perplexity for test sample: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### calculate_perplexity_corpus v02"
      ],
      "metadata": {
        "id": "oY0Ph39QgKkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# v3\n",
        "def calculate_perplexity_corpus_02(lines,\n",
        "                                unigram_counts, bigram_counts,\n",
        "                                       unique_words_cnt, k=1.0):\n",
        "    total_probabilties_product = 1.0\n",
        "    for i, line in enumerate(lines):\n",
        "        sentence_probabilties_product = calculate_sentence_probabilities_02(line,\n",
        "                                          unigram_counts, bigram_counts,\n",
        "                                          unique_words_cnt, k=1.0)\n",
        "        \n",
        "        total_probabilties_product *= sentence_probabilties_product\n",
        "        perplexity_total = total_probabilties_product**(1/unique_words_cnt)\n",
        "        if i<2 or i>len(lines)-3: print(f\"{i}: sentence_probabilties_product={sentence_probabilties_product:.5e},\" ,\n",
        "                      f\"total_probabilties_product={total_probabilties_product:.5e}\", \n",
        "                      f\"perplexity_total={perplexity_total**(1/unique_words_cnt):.3f},\")\n",
        "                      \n",
        "        \n",
        "        \n",
        "    return perplexity_total"
      ],
      "metadata": {
        "id": "ZlV6choJebhS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "sentences_tokenized = [sentence.split() for sentence in sentences_ya]\n",
        "sentences_tokenized_test = sentences_tokenized[:3]\n",
        "sentences_test = sentences_ya[:3]\n",
        "unique_words_test = list(set(' '.join([sentence for sentence in sentences_test]).split() ))\n",
        "try:\n",
        "    perplexity_total = calculate_perplexity_corpus_02(sentences_tokenized_test, uni_grams.counts, bi_grams.counts, len(unique_words_test), k=1.0)        \n",
        "    print(f\"perplexity_total={perplexity_total:.3f}\")\n",
        "except KeyboardInterrupt:\n",
        "    print(10*'*', \"KeyboardInterrupt\", 10*'*')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d13285a2-8352-4ca3-e6a3-a73c4a98fa7a",
        "id": "9-6WyUcfpwMe"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: sentence_probabilties_product=8.57346e-250, total_probabilties_product=8.57346e-250 perplexity_total=0.991,\n",
            "1: sentence_probabilties_product=6.66802e-170, total_probabilties_product=0.00000e+00 perplexity_total=0.000,\n",
            "2: sentence_probabilties_product=0.00000e+00, total_probabilties_product=0.00000e+00 perplexity_total=0.000,\n",
            "perplexity_total=0.000\n",
            "CPU times: user 12 s, sys: 797 ms, total: 12.8 s\n",
            "Wall time: 12.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Perplexity Corpus v03"
      ],
      "metadata": {
        "id": "I-xJRaNSnnLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### calculate_perplexity v03 = calculate_sentence_probabilities (sun log(prob)"
      ],
      "metadata": {
        "id": "QT2aps_tnizR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: calculate_perplexity\n",
        "def calculate_sentence_probabilities_03(sentence: list,\n",
        "                         n_gram_counts: dict,\n",
        "                         n_plus1_gram_counts: dict,\n",
        "                         vocabulary_size: int,\n",
        "                         k: float=1.0):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a list of sentences\n",
        "\n",
        "    Args:\n",
        "       sentence: List of strings\n",
        "       n_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "       vocabulary_size: number of unique words in the vocabulary\n",
        "       k: Positive smoothing constant\n",
        "\n",
        "    Returns:\n",
        "       sentence probabilities product\n",
        "    \"\"\"\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "\n",
        "    # prepend <s> and append <e>\n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "\n",
        "    # Cast the sentence from a list to a tuple\n",
        "    sentence = tuple(sentence)\n",
        "\n",
        "    # length of sentence (after adding <s> and <e> tokens)\n",
        "    N = len(sentence)\n",
        "\n",
        "    # The variable p will hold the product\n",
        "    # that is calculated inside the n-root\n",
        "    # Update this in the code below\n",
        "    #product_pi = 0.0\n",
        "    sum_log_pi = 0.0\n",
        "\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "    # Index t ranges from n to N - 1, inclusive on both ends\n",
        "    for t in range(n, N): # complete this line\n",
        "\n",
        "        # get the n-gram preceding the word at position t\n",
        "        n_gram = sentence[t - n: t]\n",
        "\n",
        "        # get the word at position t\n",
        "        word = sentence[t]\n",
        "\n",
        "        # Estimate the probability of the word given the n-gram\n",
        "        # using the n-gram counts, n-plus1-gram counts,\n",
        "        # vocabulary size, and smoothing constant\n",
        "        probability = estimate_probability(\n",
        "            word=word, previous_n_gram=n_gram,\n",
        "            vocabulary_size=vocabulary_size,\n",
        "            n_gram_counts=n_gram_counts,\n",
        "            n_plus1_gram_counts=n_plus1_gram_counts, k=k)\n",
        "\n",
        "        # Update the product of the probabilities\n",
        "        # This 'product_pi' is a cumulative product \n",
        "        # of the (1/P) factors that are calculated in the loop\n",
        "        sum_log_pi += np.log(probability)\n",
        "\n",
        "    # Take the Nth root of the product\n",
        "    #perplexity = product_pi**(1/N)\n",
        "\n",
        "    ### END CODE HERE ### \n",
        "    return sum_log_pi"
      ],
      "metadata": {
        "id": "RTGAU3e6nizR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test It"
      ],
      "metadata": {
        "id": "JVy9anvhnizS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words_test = list(set(' '.join([sentence for sentence in sentences_test]).split() ))\n",
        "\n",
        "unigram_counts = NGrams(sentences, 1).counts\n",
        "bigram_counts = NGrams(sentences, 2).counts\n",
        "\n",
        "\n",
        "perplexity_train1 = calculate_sentence_probabilities_03(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words_test), k=1.0)\n",
        "expected = 2.8040\n",
        "print(f\"Sum log(probs) for first train sample: {perplexity_train1:.4f}\")\n",
        "\n",
        "#expect(math.isclose(perplexity_train1, expected, abs_tol=1e-4)).to(be_true)\n",
        "test_sentence = ['i', 'like', 'a', 'dog']\n",
        "perplexity_test = calculate_sentence_probabilities_03(test_sentence,\n",
        "                                       unigram_counts, bigram_counts,\n",
        "                                       len(unique_words), k=1.0)\n",
        "print(f\"Sum log(probs) for test sample: {perplexity_test:.4f}\")\n",
        "expected = 3.9654\n",
        "#expect(math.isclose(perplexity_test, expected, abs_tol=1e-4)).to(be_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6c416b-88ce-4235-9b89-93547e8afa2d",
        "id": "8PdAaz41nizS"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum log(probs) for first train sample: -0.6931\n",
            "Sum log(probs) for test sample: -1.3863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### calculate_perplexity_corpus v03"
      ],
      "metadata": {
        "id": "1SC9PIyBnizS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# v3\n",
        "def calculate_perplexity_corpus_03(lines,\n",
        "                                unigram_counts, bigram_counts,\n",
        "                                       unique_words_cnt, k=1.0):\n",
        "    total_log_probabilties_sum = 0.0\n",
        "    for i, line in enumerate(lines):\n",
        "        sentence_log_probabilties_sum = calculate_sentence_probabilities_03(line,\n",
        "                                          unigram_counts, bigram_counts,\n",
        "                                          unique_words_cnt, k=1.0)\n",
        "        \n",
        "        total_log_probabilties_sum += sentence_log_probabilties_sum\n",
        "        perplexity_total = np.exp(total_log_probabilties_sum)**(1/unique_words_cnt)\n",
        "        if i<2 or i>len(lines)-3: print(f\"{i}: sentence_log_probabilties_sum={sentence_log_probabilties_sum:.5e},\" ,\n",
        "                      f\"total_log_probabilties_sum={total_log_probabilties_sum:.5e}\", \n",
        "                      f\"perplexity_total={perplexity_total**(1/unique_words_cnt):.8f},\")\n",
        "                      \n",
        "        \n",
        "    perplexity_total = np.exp(total_log_probabilties_sum**(1/unique_words_cnt))\n",
        "    return perplexity_total"
      ],
      "metadata": {
        "id": "Y1r7scVtnizS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_tokenized = [sentence.split() for sentence in sentences_ya]\n",
        "sentences_tokenized_test = sentences_tokenized[:3]\n",
        "sentences_test = sentences_ya[:3]\n",
        "unique_words_test = list(set(' '.join([sentence for sentence in sentences_test]).split() ))\n",
        "print(\"num unique_words:\", len(unique_words), unique_words_test[:10])\n",
        "#print(\"vocabulary\",  len([\"<s>\", \"<e>\", \"<unk>\"] + unique_words_test), [\"<s>\", \"<e>\", \"<unk>\"] + unique_words_test[:10] )\n",
        "print(\"vocabulary\",  len([\"<s>\", \"<e>\"] + unique_words_test), [\"<s>\", \"<e>\"] + unique_words_test[:10] )\n",
        "\n",
        "unigram_counts = NGrams(sentences_tokenized_test, 1).counts\n",
        "bigram_counts = NGrams(sentences_tokenized_test, 2).counts\n",
        "\n",
        "try:\n",
        "    perplexity_total = calculate_perplexity_corpus_03(sentences_tokenized_test, unigram_counts, bigram_counts, len(unique_words_test), k=1.0)        \n",
        "    print(f\"perplexity_total={perplexity_total:.3f}\")\n",
        "except KeyboardInterrupt:\n",
        "    print(10*'*', \"KeyboardInterrupt\", 10*'*')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1MCuXsgrtFW",
        "outputId": "71e8e675-c5a5-4530-a5c4-ba413fa93806"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num unique_words: 205608 ['or', 'most', 'An', 'recurrent', 'introduce', 'which', 'several', 'Short-Text', '1.0', 'models']\n",
            "vocabulary 258 ['<s>', '<e>', '<unk>', 'or', 'most', 'An', 'recurrent', 'introduce', 'which', 'several', 'Short-Text', '1.0', 'models']\n",
            "0: sentence_log_probabilties_sum=-8.71491e+01, total_log_probabilties_sum=-8.71491e+01 perplexity_total=0.99866066,\n",
            "1: sentence_log_probabilties_sum=-6.13150e+01, total_log_probabilties_sum=-1.48464e+02 perplexity_total=0.99771942,\n",
            "2: sentence_log_probabilties_sum=-1.62295e+02, total_log_probabilties_sum=-3.10759e+02 perplexity_total=0.99523233,\n",
            "perplexity_total=nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "sentences = dummy_lines\n",
        "unique_words_test = list(set(' '.join([sentence for sentence in sentences_test]).split() ))\n",
        "sentences = [s.split() for s in sentences]\n",
        "unigram_counts = NGrams(sentences, 1).counts\n",
        "bigram_counts = NGrams(sentences, 2).counts\n",
        "try:\n",
        "    perplexity_total = calculate_perplexity_corpus_03(sentences, bi_grams.counts, tri_grams.counts, len(list(bi_grams.counts)), k=1.0)        \n",
        "    print(f\"perplexity_total={perplexity_total:.3f}\")\n",
        "except KeyboardInterrupt:\n",
        "    print(10*'*', \"KeyboardInterrupt\", 10*'*')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b56e50-baaa-4d1b-f8a2-8762289a6b73",
        "id": "di_U4jZ0nizT"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: sentence_log_probabilties_sum=-2.37190e+01, total_log_probabilties_sum=-2.37190e+01 perplexity_total=0.78884141,\n",
            "1: sentence_log_probabilties_sum=-3.06268e+01, total_log_probabilties_sum=-5.43458e+01 perplexity_total=0.58073687,\n",
            "98: sentence_log_probabilties_sum=-8.12836e+01, total_log_probabilties_sum=-6.38692e+03 perplexity_total=0.00000000,\n",
            "99: sentence_log_probabilties_sum=-7.66785e+01, total_log_probabilties_sum=-6.46359e+03 perplexity_total=0.00000000,\n",
            "perplexity_total=nan\n",
            "CPU times: user 15 ms, sys: 0 ns, total: 15 ms\n",
            "Wall time: 14.9 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### calculate_perplexity_corpus v04"
      ],
      "metadata": {
        "id": "U8SNtG2nbKfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# v4\n",
        "def calculate_perplexity_corpus_04(lines: list, #(list of lists of words)\n",
        "                         n_gram_counts: dict,\n",
        "                         n_plus1_gram_counts: dict,\n",
        "                         vocabulary_size: int,\n",
        "                         k: float=1.0):\n",
        "  \n",
        "    perplexity_total = []\n",
        "    for i, line in enumerate(lines):\n",
        "        perplexity_line = calculate_perplexity(line, n_gram_counts, n_plus1_gram_counts,\n",
        "                                          vocabulary_size, k=1.0)\n",
        "        #if i<2: print(perplexity_line)\n",
        "        perplexity_total.append(perplexity_line)\n",
        "    \n",
        "    return np.array(perplexity_total).mean()"
      ],
      "metadata": {
        "id": "WRCouDkIbKfW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test It"
      ],
      "metadata": {
        "id": "clGrpDDcbfJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### two lines"
      ],
      "metadata": {
        "id": "kUoDx6Z8_Ufs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "print(\"num unique_words:\", len(unique_words), unique_words)\n",
        "print(\"vocabulary\",  len(unique_words + [\"<s>\", \"<e>\", \"<unk>\"]), unique_words + [\"<s>\", \"<e>\", \"<unk>\"])\n",
        "\n",
        "unigram_counts = NGrams(sentences, 1).counts\n",
        "bigram_counts = NGrams(sentences, 2).counts\n",
        "\n",
        "perplexity_total = calculate_perplexity_corpus_04(sentences,unigram_counts, bigram_counts, len(unique_words), k=1.0)        \n",
        "print(f\"perplexity_total={perplexity_total:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b6a873-d3dd-4355-9420-24d38c7bbab9",
        "id": "W4A9iOaDbeag"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num unique_words: 7 ['cat', 'this', 'i', 'dog', 'like', 'a', 'is']\n",
            "vocabulary 10 ['cat', 'this', 'i', 'dog', 'like', 'a', 'is', '<s>', '<e>', '<unk>']\n",
            "perplexity_total=12.899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 100 lines - unigram"
      ],
      "metadata": {
        "id": "sElsVUtx_XhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "sentences = dummy_lines\n",
        "unique_words_test = list(set(' '.join([sentence for sentence in sentences]).split() ))\n",
        "sentences = [s.split() for s in sentences]\n",
        "\n",
        "print(\"num unique_words:\", len(unique_words), unique_words)\n",
        "print(\"vocabulary\",  len(unique_words + [\"<s>\", \"<e>\"]), unique_words + [\"<s>\", \"<e>\", ])\n",
        "\n",
        "ppx1 = calculate_perplexity_corpus_04(sentences,unigram_counts, bigram_counts, len(unique_words)+2, k=1.0)        \n",
        "print(f\"perplexity_total(unigram)={ppx1:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtaKBPk1TOjk",
        "outputId": "e15abeb1-3b74-4aa5-e8b5-5638f2163e50"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num unique_words: 7 ['cat', 'this', 'i', 'dog', 'like', 'a', 'is']\n",
            "vocabulary 9 ['cat', 'this', 'i', 'dog', 'like', 'a', 'is', '<s>', '<e>']\n",
            "perplexity_total(unigram)=4.680\n",
            "CPU times: user 16 ms, sys: 997 ¬µs, total: 17 ms\n",
            "Wall time: 31.3 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 100 lines - bigram"
      ],
      "metadata": {
        "id": "R1DTrMX__o8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trigram_counts = NGrams(sentences, 3).counts\n",
        "\n",
        "ppx2 = calculate_perplexity_corpus_04(sentences, bigram_counts, trigram_counts, len(unique_words)+2, k=1.0)        \n",
        "print(f\"perplexity_total(bigram)={ppx2:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLpzZopJ9S4C",
        "outputId": "2b69a856-f111-4049-8bcc-86d4f5743733"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity_total(bigram)=1.501\n",
            "CPU times: user 16.2 ms, sys: 0 ns, total: 16.2 ms\n",
            "Wall time: 32.2 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 100 lines - trigram"
      ],
      "metadata": {
        "id": "YSlzIBOvBKnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "b4_grams_counts = NGrams(sentences, 4).counts\n",
        "ppx3 = calculate_perplexity_corpus_04(sentences, trigram_counts, b4_grams_counts, len(unique_words)+2, k=1.0)        \n",
        "print(f\"perplexity_total(trigram)={ppx2:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e205acea-2f93-4e00-e6a3-87ca0c3e40d3",
        "id": "4I5s9xorBKnR"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity_total(trigram)=1.501\n",
            "CPU times: user 15.6 ms, sys: 0 ns, total: 15.6 ms\n",
            "Wall time: 34.7 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 100 lines - n10_gram"
      ],
      "metadata": {
        "id": "eOXVGDG0_6uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n10_grams_counts = NGrams(sentences, 10).counts\n",
        "n11_grams_counts = NGrams(sentences, 11).counts\n",
        "#n10_grams_counts_cut = dict(list(n10_grams_counts)[:10])\n",
        "# from IPython.display import display\n",
        "display(list(n10_grams_counts)[:2], list(n10_grams_counts)[8]) \n",
        "#pprint(n10_grams_counts_cut )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "8vPJN8qcB1xr",
        "outputId": "fd6a469a-b038-4ccc-f674-b14e72e0a8cf"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>'),\n",
              " ('<s>',\n",
              "  '<s>',\n",
              "  '<s>',\n",
              "  '<s>',\n",
              "  '<s>',\n",
              "  '<s>',\n",
              "  '<s>',\n",
              "  '<s>',\n",
              "  '<s>',\n",
              "  'differential')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('<s>',\n",
              " '<s>',\n",
              " 'differential',\n",
              " 'contrastive',\n",
              " 'divergence',\n",
              " ';',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'has',\n",
              " 'been')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(n11_grams_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VriFx5bzDDme",
        "outputId": "dc8fcb4d-308d-40ee-c6b5-9f95e729d9a9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%time\n",
        "ppx10 = calculate_perplexity_corpus_04(sentences, n10_grams_counts, n11_grams_counts, len(unique_words)+2, k=1.0)        \n",
        "print(f\"perplexity_total(n10_gram)={ppx10:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_aFVIER9jit",
        "outputId": "1162b23e-57ac-4482-84a8-f85e4842327b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity_total(n10_gram)=1.137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### total"
      ],
      "metadata": {
        "id": "jNbyzoERDX0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Perplexities: ppx1=%.3f ppx2=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx2, ppx3, ppx10))\n",
        "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n",
        "assert ppx1 > ppx2 > ppx3 > ppx10, \"higher N models should overfit\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCqWqd44DeKA",
        "outputId": "ee3bdba5-edcd-4fdb-d411-be321c9c2e07"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexities: ppx1=4.680 ppx2=1.501 ppx3=1.256 ppx10=1.137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b384WSVXNveJ"
      },
      "source": [
        "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "bG7rn9MQIPYy",
        "outputId": "6fbd2713-5fc6-4242-cc92-596111f3d7ee"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dual recurrent attention units for visual question answering ; we propose an architecture for vqa which utilizes recurrent layers to generate visual and textual attention . the memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question . our single model outperforms the first place winner on the vqa 1 . 0 dataset , performs within margin to the current state - of - the - art ensemble model . we also experiment with replacing attention mechanisms in other state - of - the - art models with our implementation and show increased accuracy . in both cases , our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the vqa dataset .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set(' '.join(lines).split())\n",
        "print(len(list(vocabulary)), list(vocabulary)[:10] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5RzLmwuGCAk",
        "outputId": "33da0afe-ec10-4b25-d60a-974fe0db7bb3"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62760 ['causers', 'preorder', 'monkey', 'irregularly', 'teamwork', 'derivate', 'decorrelating', 'tempotron', 'maintenance', 'utility']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "vocabulary_size = len(vocabulary)+2 \n",
        "train_lines_tokens = [line.split() for line in train_lines]\n",
        "test_lines_tokens = [line.split() for line in test_lines]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuT2NHpEJTz1",
        "outputId": "59b25296-41ec-485e-911a-c87bffae94df"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.03 s, sys: 108 ms, total: 1.13 s\n",
            "Wall time: 1.14 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for n in [1]:\n",
        "    n_grams_counts = NGrams(train_lines_tokens, n).counts\n",
        "    n_plus1_grams_counts = NGrams(train_lines_tokens, n+1).counts\n",
        "    ppx = calculate_perplexity_corpus_04(test_lines_tokens, n_grams_counts, n_plus1_grams_counts, vocabulary_size, k=1.0)        \n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JoYZk39JnkG",
        "outputId": "cd284f48-376c-43b9-865e-6043aaf26e79"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1, Perplexity = inf\n",
            "CPU times: user 1min, sys: 602 ms, total: 1min 1s\n",
            "Wall time: 1min 1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for n in [2]:\n",
        "    n_grams_counts = NGrams(train_lines_tokens, n).counts\n",
        "    n_plus1_grams_counts = NGrams(train_lines_tokens, n+1).counts\n",
        "    ppx = calculate_perplexity_corpus_04(test_lines_tokens, n_grams_counts, n_plus1_grams_counts, vocabulary_size, k=1.0)        \n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN6Tnh1mLUrZ",
        "outputId": "80cb14f2-4946-4db5-a11d-bf2d8fa45603"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 2, Perplexity = inf\n",
            "CPU times: user 11min 16s, sys: 3.43 s, total: 11min 19s\n",
            "Wall time: 11min 19s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for n in [3]:\n",
        "    n_grams_counts = NGrams(train_lines_tokens, n).counts\n",
        "    n_plus1_grams_counts = NGrams(train_lines_tokens, n+1).counts\n",
        "    ppx = calculate_perplexity_corpus_04(test_lines_tokens, n_grams_counts, n_plus1_grams_counts, vocabulary_size, k=1.0)        \n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421d2050-6661-4629-d1b8-c3044b211500",
        "id": "zTGExSSWSvGm"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 3, Perplexity = inf\n",
            "CPU times: user 25min 49s, sys: 8.25 s, total: 25min 57s\n",
            "Wall time: 25min 55s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare Ya variants**\n",
        "```\n",
        "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
        "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
        "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
        "\n",
        "ppx1 = perplexity(lm1, dummy_lines)\n",
        "ppx3 = perplexity(lm3, dummy_lines)\n",
        "ppx10 = perplexity(lm10, dummy_lines)\n",
        "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
        "\n",
        "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
        "\n",
        "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n",
        "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
        "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
        "    \" Make sure you use min_logprob right\"\n",
        "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))\n",
        "```\n",
        "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only.\n",
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "for n in (1, 2, 3):\n",
        "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n",
        "```"
      ],
      "metadata": {
        "id": "GCetAlfQEU1I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84GlE138q4SD"
      },
      "source": [
        "## LM Smoothing\n",
        "\n",
        "The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n",
        "\n",
        "To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
        "\n",
        "If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n",
        "\n",
        "Here's an example code we've implemented for you:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LaplaceLanguageModel"
      ],
      "metadata": {
        "id": "KsJXEeN1xa4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqwkoHZGq4SE"
      },
      "outputs": [],
      "source": [
        "class LaplaceLanguageModel(NGramLanguageModel): \n",
        "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        for prefix in counts:\n",
        "            token_counts = counts[prefix]\n",
        "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
        "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
        "                                          for token in token_counts}\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
        "    \n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        if next_token in token_probs:\n",
        "            return token_probs[next_token]\n",
        "        else:\n",
        "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
        "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)"
      ],
      "metadata": {
        "id": "Ugvblvrex1pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XivNZrgjq4SE"
      },
      "outputs": [],
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
        "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW961f01q4SE"
      },
      "outputs": [],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4nUJlOQq4SE"
      },
      "outputs": [],
      "source": [
        "# optional: try to sample tokens from such a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqkBBZ6nq4SE"
      },
      "source": [
        "### Kneser-Ney smoothing - not realized\n",
        "Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n",
        "\n",
        "\n",
        "Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n",
        "\n",
        "It can be computed recurrently, for n>1:\n",
        "\n",
        "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
        "\n",
        "where\n",
        "- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n",
        "- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n",
        "- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n",
        "- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n",
        "\n",
        "See lecture slides or wiki for more detailed formulae.\n",
        "\n",
        "__Your task__ is to\n",
        "- implement KneserNeyLanguageModel\n",
        "- test it on 1-3 gram language models\n",
        "- find optimal (within reason) smoothing delta for 3-gram language model with Kneser-Ney smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ke3beLfq4SF"
      },
      "outputs": [],
      "source": [
        "class KneserNeyLanguageModel(NGramLanguageModel): \n",
        "    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        <YOUR CODE>\n",
        "        \n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        < YOUR CODE >\n",
        "        \n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        <YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujqyjEgqq4SF"
      },
      "outputs": [],
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n",
        "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBmVrSkVq4SI"
      },
      "outputs": [],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = KneserNeyLanguageModel(train_lines, n=n, smoothing=<...>)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "LM_Statistical_NGram_v01_Ya",
      "provenance": [],
      "collapsed_sections": [
        "a8IJshmRM4qD"
      ],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0eff9e60a9a44e4a98d735f67008ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fad58d15324245b6b75e3be6d5663b44",
              "IPY_MODEL_13bd46e99c184a6aa0f920c212b8c929",
              "IPY_MODEL_f77df3d8edcc48fd9c714b7d8e9a616b"
            ],
            "layout": "IPY_MODEL_94b47f17600b47468a442bc15b112187"
          }
        },
        "fad58d15324245b6b75e3be6d5663b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e61313beae404ff0ac82be94d00ccf1f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_46af1e18e32c44c589454b5e48c54306",
            "value": "Downloading: 100%"
          }
        },
        "13bd46e99c184a6aa0f920c212b8c929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bebc247a61c4b66a9b3dc81713ab7ab",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30a44d126db547e5b71c13c7e8cad0d2",
            "value": 665
          }
        },
        "f77df3d8edcc48fd9c714b7d8e9a616b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fccf7a3c5b54ed587ae7c6417e71b8c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e5e017d346f3492ab622a774d4d79a5c",
            "value": " 665/665 [00:00&lt;00:00, 7.35kB/s]"
          }
        },
        "94b47f17600b47468a442bc15b112187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e61313beae404ff0ac82be94d00ccf1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46af1e18e32c44c589454b5e48c54306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bebc247a61c4b66a9b3dc81713ab7ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a44d126db547e5b71c13c7e8cad0d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fccf7a3c5b54ed587ae7c6417e71b8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5e017d346f3492ab622a774d4d79a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7053e0107374f87b8bb0e0b2545868a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a06dfdfc02042f5a4b4f09687417f2a",
              "IPY_MODEL_2e6d41f27c674309b2fa863526e9528c",
              "IPY_MODEL_0c03acea8e034ea0a2aeabef27860ace"
            ],
            "layout": "IPY_MODEL_471a18eaae7d4c368ea42a8004b6977d"
          }
        },
        "9a06dfdfc02042f5a4b4f09687417f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4717a3b9c6847c0b99a714ae8e5b951",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b538fe48c2af4d29bd60e4fb434f32ca",
            "value": "Downloading: 100%"
          }
        },
        "2e6d41f27c674309b2fa863526e9528c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1f1e7e3b25346b19d102abadf1e7695",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_808b409732834d29a66af944f79b284d",
            "value": 548118077
          }
        },
        "0c03acea8e034ea0a2aeabef27860ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373a2d36fdd347a794da7381246cff91",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2cb66700b43742bab1e629865d1189c7",
            "value": " 523M/523M [00:17&lt;00:00, 49.7MB/s]"
          }
        },
        "471a18eaae7d4c368ea42a8004b6977d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4717a3b9c6847c0b99a714ae8e5b951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b538fe48c2af4d29bd60e4fb434f32ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1f1e7e3b25346b19d102abadf1e7695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808b409732834d29a66af944f79b284d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "373a2d36fdd347a794da7381246cff91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cb66700b43742bab1e629865d1189c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58eda14fb3cb442e93232ffb89adfd76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5cdda5a9abe84ef8b659ef08eea4497c",
              "IPY_MODEL_cf2dfe9550b0427faee8bda834849f42",
              "IPY_MODEL_4fa62771e0354b40a595256a4f389bef"
            ],
            "layout": "IPY_MODEL_0002d0274b374e3b83161f4a026cb916"
          }
        },
        "5cdda5a9abe84ef8b659ef08eea4497c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa4ed503c22546418dd90d158ec2d174",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_336e39bdbf9646159cf5a44f0aab0c65",
            "value": "Downloading: 100%"
          }
        },
        "cf2dfe9550b0427faee8bda834849f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3802eec884c64d6d928f631b8bb19506",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2863bb5b39fb4a2ea9f7db25d5f6c115",
            "value": 1042301
          }
        },
        "4fa62771e0354b40a595256a4f389bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dab50edaeea94b36964711a3bed7face",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_310ef09f51c34327898ab258c0832d5b",
            "value": " 0.99M/0.99M [00:00&lt;00:00, 1.84MB/s]"
          }
        },
        "0002d0274b374e3b83161f4a026cb916": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa4ed503c22546418dd90d158ec2d174": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "336e39bdbf9646159cf5a44f0aab0c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3802eec884c64d6d928f631b8bb19506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2863bb5b39fb4a2ea9f7db25d5f6c115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dab50edaeea94b36964711a3bed7face": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "310ef09f51c34327898ab258c0832d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e7e1f90b0eb4dd8b6b0ad8d7ff07d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b69d1a3091a4a6d849d8ee1fcfa2393",
              "IPY_MODEL_9721f56b62dd40a4b86914912ece4666",
              "IPY_MODEL_4f9bfa793e764ebdbcbfdfd94383a5c7"
            ],
            "layout": "IPY_MODEL_fee6d7e306c743b6a448060ef2d41c05"
          }
        },
        "1b69d1a3091a4a6d849d8ee1fcfa2393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_456ac64f5c3547679286dcf4989e973a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_12393c12f5064027945d581563dd2e63",
            "value": "Downloading: 100%"
          }
        },
        "9721f56b62dd40a4b86914912ece4666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71ffff68ba9e44d3a94f92585af13c41",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c997c157ca53437f8ceac26524fa7272",
            "value": 456318
          }
        },
        "4f9bfa793e764ebdbcbfdfd94383a5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9573f858498241c19f4f42e9e8317a89",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_759b0cd987164000b1cf35e9d80bf65d",
            "value": " 446k/446k [00:00&lt;00:00, 1.31MB/s]"
          }
        },
        "fee6d7e306c743b6a448060ef2d41c05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "456ac64f5c3547679286dcf4989e973a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12393c12f5064027945d581563dd2e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71ffff68ba9e44d3a94f92585af13c41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c997c157ca53437f8ceac26524fa7272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9573f858498241c19f4f42e9e8317a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "759b0cd987164000b1cf35e9d80bf65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdb252068e2e439f877ad5e0b95f012a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a75a40a34d94366aca19e13f7450291",
              "IPY_MODEL_cd17f321ce164ac8bdecb86a27b0afd0",
              "IPY_MODEL_af886b9596bd4b9bad5b98801e73d195"
            ],
            "layout": "IPY_MODEL_ad27ea45229d458a85bbafe0937a750a"
          }
        },
        "2a75a40a34d94366aca19e13f7450291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebf29c5f551d4b4690b6c0f2095bc364",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_819a6c9274d544339eb379f3f490dd1f",
            "value": "Downloading: 100%"
          }
        },
        "cd17f321ce164ac8bdecb86a27b0afd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebe06cd313d440a8a348e8469a31fa97",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f579a6c1c8c4a17811142323b3552c5",
            "value": 1355256
          }
        },
        "af886b9596bd4b9bad5b98801e73d195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87035a9f802a4ad9afdd367ee85768e7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1f1202298f894bb7a7b3fc226e4e82d9",
            "value": " 1.29M/1.29M [00:00&lt;00:00, 2.06MB/s]"
          }
        },
        "ad27ea45229d458a85bbafe0937a750a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebf29c5f551d4b4690b6c0f2095bc364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "819a6c9274d544339eb379f3f490dd1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebe06cd313d440a8a348e8469a31fa97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f579a6c1c8c4a17811142323b3552c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87035a9f802a4ad9afdd367ee85768e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f1202298f894bb7a7b3fc226e4e82d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}